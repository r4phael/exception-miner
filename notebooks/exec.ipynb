{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Geração do dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import io\n",
    "from typing import List, Any\n",
    "import astunparse\n",
    "import token\n",
    "import tokenize\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "from tree_sitter.binding import Node\n",
    "from numpy.random import default_rng\n",
    "\n",
    "rng = default_rng()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('<INDENT>', '<DEDENT>', '<NEWLINE>')"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "INDENT_STR = f\"<{token.tok_name[token.INDENT]}>\"\n",
    "DEDENT_STR = f\"<{token.tok_name[token.DEDENT]}>\"\n",
    "NEWLINE_STR = f\"<{token.tok_name[token.NEWLINE]}>\"\n",
    "\n",
    "INDENT_STR, DEDENT_STR, NEWLINE_STR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TryDatasetGenerator:\n",
    "\n",
    "    def __init__(self, func_defs: List[Node], stats: TBLDStats) -> None:\n",
    "        self.func_defs = func_defs\n",
    "        self.stats = stats\n",
    "        self.reset()\n",
    "\n",
    "    def reset(self):\n",
    "        self.indentation_counter = 0\n",
    "        self.lines = []\n",
    "        self.labels = []\n",
    "        self.start_function_def = False\n",
    "        self.try_reached = False\n",
    "\n",
    "        self.current_lineno = None\n",
    "        self.token_buffer = []\n",
    "\n",
    "        self.stats.num_max_tokens = max(\n",
    "            self.stats.num_max_tokens, self.stats.function_tokens_acc\n",
    "        )\n",
    "        self.stats.tokens_count += self.stats.function_tokens_acc\n",
    "        self.stats.function_tokens_acc = 0\n",
    "\n",
    "    def generate(self):\n",
    "        generated = []\n",
    "\n",
    "        pbar = tqdm(self.func_defs)\n",
    "        for func_def in pbar:\n",
    "            pbar.set_description(\n",
    "                f\"Function: {func_def.child_by_field_name('name').text[-40:].ljust(40)}\")  # type: ignore\n",
    "            try:\n",
    "                tokenized_function_def = self.tokenize_function_def(func_def)\n",
    "\n",
    "                if tokenized_function_def is not None:\n",
    "                    self.stats.functions_count += 1\n",
    "                    self.stats.increment_try_stats(count_try(func_def))\n",
    "                    num_statements = statement_couter(func_def)\n",
    "                    self.stats.statements_count += num_statements\n",
    "                    self.stats.num_max_statement = max(\n",
    "                        self.stats.num_max_statement, num_statements\n",
    "                    )\n",
    "                    generated.append(tokenized_function_def)\n",
    "            except SyntaxError as e:\n",
    "                print(\n",
    "                    f\"###### SyntaxError Error!!! in ast.FunctionDef {func_def}.\\n{str(e)}\")\n",
    "                continue\n",
    "            except tokenize.TokenError as e:\n",
    "                print(\n",
    "                    f\"###### TokenError Error!!! in ast.FunctionDef {func_def}.\\n{str(e)}\")\n",
    "                continue\n",
    "            except ValueError as e:\n",
    "                print(\n",
    "                    f\"###### ValueError Error!!! in ast.FunctionDef {func_def}.\\n{str(e)}\")\n",
    "                continue\n",
    "            except MemoryError as e:\n",
    "                print(\n",
    "                    f\"###### MemoryError Error!!! in ast.FunctionDef {func_def}.\\n{str(e)}\")\n",
    "                print(func_def.sexp())\n",
    "                print(astunparse.unparse(func_def))\n",
    "                continue\n",
    "\n",
    "        return pd.DataFrame(generated)\n",
    "\n",
    "    def clear_line_buffer(self):\n",
    "        if len(self.token_buffer) == 0:\n",
    "            return\n",
    "\n",
    "        if self.try_reached:\n",
    "            indentation = \" \".join([INDENT_STR for _ in range(\n",
    "                self.indentation_counter - 1)])\n",
    "            self.stats.function_tokens_acc += self.indentation_counter - 1\n",
    "        else:\n",
    "            indentation = \" \".join(\n",
    "                [INDENT_STR for _ in range(self.indentation_counter)])\n",
    "            self.stats.function_tokens_acc += self.indentation_counter\n",
    "\n",
    "        if self.indentation_counter != 0:\n",
    "            indentation += ' '\n",
    "\n",
    "        tokenized_line = indentation + \" \".join(self.token_buffer)\n",
    "\n",
    "        self.stats.function_tokens_acc += len(self.token_buffer)\n",
    "        self.stats.unique_tokens.update(self.token_buffer)\n",
    "\n",
    "        self.token_buffer = []\n",
    "\n",
    "        self.lines.append(tokenized_line)\n",
    "        self.labels.append(1 if self.try_reached else 0)\n",
    "\n",
    "    def end_of_generation(self):\n",
    "        res = {\n",
    "            \"hasCatch\": max(self.labels),\n",
    "            \"lines\": self.lines,\n",
    "            \"labels\": self.labels,\n",
    "        }\n",
    "\n",
    "        self.reset()\n",
    "\n",
    "        return res\n",
    "\n",
    "    def handle_indentation_and_newline(self, token_info: tokenize.TokenInfo):\n",
    "        if token_info.type == token.STRING:\n",
    "            self.token_buffer.append(token_info.string[0])\n",
    "            self.token_buffer.append(\n",
    "                \"\".join(token_info.string[1:-1].splitlines()).strip()\n",
    "            )\n",
    "            self.token_buffer.append(token_info.string[-1])\n",
    "            return True\n",
    "        return self.handle_indentation(token_info) or self.handle_new_line(token_info)\n",
    "\n",
    "    def handle_new_line(self, token_info: tokenize.TokenInfo):\n",
    "        if token_info.type == token.NEWLINE:\n",
    "            self.token_buffer.append(NEWLINE_STR)\n",
    "            return True\n",
    "        return False\n",
    "\n",
    "    def handle_indentation(self, token_info: tokenize.TokenInfo):\n",
    "        if token_info.type == token.INDENT:\n",
    "            self.indentation_counter += 1\n",
    "            return True\n",
    "\n",
    "        if token_info.type == token.DEDENT:\n",
    "            self.indentation_counter -= 1\n",
    "            assert self.indentation_counter >= 0\n",
    "            return True\n",
    "        return False\n",
    "\n",
    "    def tokenize_function_def(self, node: Node):\n",
    "        assert node is not None\n",
    "        if not isinstance(node.text, bytes):\n",
    "            raise TreeSitterNodeException(\"node.text is not bytes\")\n",
    "\n",
    "        try:\n",
    "            try_slice = mpu.get_try_slices(node)\n",
    "        except TryNotFoundException:\n",
    "            try_slice = None\n",
    "\n",
    "        for token_info in tokenize.generate_tokens(io.StringIO(node.text.decode('utf-8')).readline):\n",
    "            if token_info.start[0] != self.current_lineno:\n",
    "                self.clear_line_buffer()\n",
    "                self.current_lineno = token_info.start[0]\n",
    "\n",
    "            if try_slice is not None:\n",
    "                self.try_reached = token_info.start[0] >= try_slice.try_block_start\n",
    "                if token_info.start[0] == try_slice.try_block_start:  # ignore try\n",
    "                    self.handle_indentation(token_info)\n",
    "                    continue\n",
    "\n",
    "                if len(try_slice.handlers) != 0 and token_info.start[0] >= try_slice.handlers[0][0]:\n",
    "                    return self.end_of_generation()\n",
    "\n",
    "            if token_info.type in [token.COMMENT, token.NL]:\n",
    "                continue\n",
    "\n",
    "            if token_info.type == token.ENDMARKER:\n",
    "                return self.end_of_generation()\n",
    "\n",
    "            if not self.handle_indentation_and_newline(token_info):\n",
    "                self.token_buffer.append(token_info.string)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
